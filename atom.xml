<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Zach Stednick]]></title>
  <link href="http://stedy.github.io/atom.xml" rel="self"/>
  <link href="http://stedy.github.io/"/>
  <updated>2021-10-21T22:29:33-07:00</updated>
  <id>http://stedy.github.io/</id>
  <author>
    <name><![CDATA[Zach Stednick]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Cluster analysis of world flags]]></title>
    <link href="http://stedy.github.io/blog/2021/10/21/cluster-analysis-of-world-flags/"/>
    <updated>2021-10-21T22:25:28-07:00</updated>
    <id>http://stedy.github.io/blog/2021/10/21/cluster-analysis-of-world-flags</id>
    <content type="html"><![CDATA[<p>After spending time looking at world flags, I have started to notice similarities in flags from different countries. Sometimes the similarity may be due to historical relationships, such as the <a href="https://en.wikipedia.org/wiki/United_States">United States of America</a> and <a href="https://en.wikipedia.org/wiki/Liberia">Liberia</a> or the <a href="https://en.wikipedia.org/wiki/United_Kingdom">United Kingdom</a> and <a href="https://en.wikipedia.org/wiki/Australia">Australia</a>. Other times it may be purely coincidental such as <a href="https://en.wikipedia.org/wiki/Romania">Romania</a> and <a href="https://en.wikipedia.org/wiki/Chad">Chad</a>.</p>

<table>
<thead>
<tr>
<th style="text-align:center;"> <img src="http://zachstednick.com/romania_chad.png"> </th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center;"> Flag of Romania on left, Flag of Chad on right </td>
</tr>
</tbody>
</table>


<p> I was interested in determining if additional relationships between flags could be discovered via mathematical analysis of the properties of each flag. To begin my analysis, I needed to find a standardized set of flag image files to make up my dataset. I started by using flags from Wikipedia but found these were too variable in size and image quality. I later ended up purchasing a set of flag image files from <a href="countryflags.com">CountryFlags</a>. I used the Python library <a href="https://github.com/obskyr/colorgram.py">colorgram</a> to scan every pixel of every flag image file and then determine each pixelâ€™s color using the <a href="https://en.wikipedia.org/wiki/RGB_color_model">RGB color model</a>. There are many distinct shades of every color so I grouped all shades into groups of red, yellow, blue, green, orange, white and black. For example, the blue of the Argentina flag is quite different from the blue of the Sweden flag, however for the purposes of this project I called them both blue.</p>

<table>
<thead>
<tr>
<th style="text-align:center;"> <img src="http://zachstednick.com/arg_swe.png"> </th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center;"> Flag of Argentina on left, Flag of Sweden on right </td>
</tr>
</tbody>
</table>


<p>In addition to determining the colors of each flag I was also interested in what percentage of each flag was a specific color. By this method, the flag of France would be represented as 33% red, 33% white and 33% blue. Some country flags have smaller details with distinct colors which made classification trickier. To simplify analysis I limited my dataset to colors that appeared on at least 5% of the overall flag. For example, the flag of Belize has many small details that had to be omitted while retaining the dominant red, blue, and white color pattern.</p>

<p><img src="http://zachstednick.com/belize_flag.png"></p>

<p>After collecting the data, I implemented a <a href="https://en.wikipedia.org/wiki/Cluster_analysis">clustering algorithm</a>, specifically <a href="https://en.wikipedia.org/wiki/K-means_clustering">K-means clustering</a> which is a mathematical method for grouping observations into clusters. Each cluster contains individual observations with similar values and the smaller the cluster the more similar all of the values are in that particular cluster. For an initial demonstration, I clustered the eleven country flags of South America into three clusters.</p>

<p><img src="http://zachstednick.com/sa_cluster.png"></p>

<p>From this figure, three clusters of interest are distinctly observed with the smallest cluster consisting of the flags of Colombia, Ecuador, and Venezuela. We can see the other two clusters which have grouped the remaining flags of South America into two separate and larger clusters. From a historical perspective, this makes sense as most countries in South America were colonized by Spain or Brazil and became independent at approximately the same time.</p>

<p>For the analysis of all the flags of the world, I again set the number of clusters to 3 in order to simplify the visual output:</p>

<p><img src="http://zachstednick.com/all_flags_cluster.png"></p>

<p>Unsurprisingly, there is quite a bit of overlap between the three clusters and it is difficult to ascertain how these clusters are distinct from each other. Most countries use good design patterns (For example the <a href="https://nava.clubexpress.com/content.aspx?page_id=22&amp;club_id=622278&amp;module_id=475721">NAVA guide</a>), similar colors, and similar color proportions for their flags. However, there are some interesting observations and unexpected pairings that reveal themselves after studying the figure. For example, Lesotho and Uzbekistan have similar design patterns as well as similar color profiles. Another interesting pairng that does not share similar design patterns is Honduras and Greece. Also interesting to note how similar many of these flags are, with the two notable outliers being Ukraine and Niue.</p>

<p><img src="http://zachstednick.com/niue_flag.png">
|:&ndash;:|
| Flag of <a href="https://en.wikipedia.org/wiki/Niue">Niue</a> |</p>

<p>Cluster analysis is by no means definitive, and is more of an exploratory analysis. Regardless, I found this method a rewarding way to reconsider the design
patterns of the many flags of the world.</p>

<p>As always, analysis code can be found on <a href="https://github.com/stedy/flag_clustering">GitHub</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Pair Programming With a 13 Month Old]]></title>
    <link href="http://stedy.github.io/blog/2021/04/14/pair-programming-with-a-13-month-old/"/>
    <updated>2021-04-14T11:02:42-07:00</updated>
    <id>http://stedy.github.io/blog/2021/04/14/pair-programming-with-a-13-month-old</id>
    <content type="html"><![CDATA[<p><a href="https://en.wikipedia.org/wiki/Pair_programming">Pair Programming</a> is a
software development method that uses two programmers on one
workstation thereby writing code as a team. I did a slight variation of this on a recent
project with a thirteen month old and had pretty good success.</p>

<table>
<thead>
<tr>
<th style="text-align:center;"> <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/a/af/Pair_programming_1.jpg/1024px-Pair_programming_1.jpg" alt="Pair Programming" /> </th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center;"> Lisamarie Babik, CC BY 2.0 <a href="https://creativecommons.org/licenses/by/2.0">https://creativecommons.org/licenses/by/2.0</a>, via Wikimedia Commons </td>
</tr>
</tbody>
</table>


<p>I used to really like this site <a href="http://graphtv.kevinformatics.com/">GraphTV</a> and was sad when it closed. I later found <a href="http://www.omdbapi.com/">OMDb API</a> which I
 used to recreate GraphTV as a command line program on <a href="https://github.com/stedy/binge-trendy">GitHub</a>. But who wants to use a command line program to look
things up? I knew it could be much better hosted on its own website
which meant improving my JavaScript and a ideally use a JS charting
library (I ultimately went with <a href="https://www.chartjs.org/">Chart.js</a>). In a past life I wrote a lot of R code and JS often feels
incredibly foreign to me for reasons I cannot quite articulate.</p>

<p>These days I spend most of my time with a 13 month old and have pretty limited amounts of time
here and there for writing code. I do find that when working on a
programming problem that it can be incredibly easy for me
to run into a roadblock, hop on <a href="https://stackoverflow.com/tags/javascript">SO</a> and then find myself further out in the
weeds and really frustrated.</p>

<p>With this project, I took a different approach. Any time I ran into a roadblock I did a quick search on SO, read a few answers and then just closed my laptop and walked away. Granted this was not the fastest way to solve the problem but it did lead to many minor epiphanies in otherwise quotidian parenting events:</p>

<ul>
<li>Playing with some books on the floor and thinking &ldquo;The
<code>innerHTML()</code>
<a href="https://developer.mozilla.org/en-US/docs/Web/API/Element/innerHTML">element</a> is not flushing everything out, maybe there is a
dedicated function in Chartjs&rdquo;.</li>
<li>Trying to get the 13 month old to eat more and thinking &ldquo;Why am I using this unwieldy CSV file when I could simplify my code so much
with a JSON object&rdquo;.</li>
<li>Changing a diaper and thinking &ldquo;Well, I have tried everything else, did Bootstrap change something between v4.5 and <a href="https://blog.getbootstrap.com/2020/06/16/bootstrap-5-alpha/">5.0</a>?&rdquo;</li>
</ul>


<p>I finally finished my project and launched
<a href="http://bingetrendy.com/">bingetrendy</a>!
I am not nor will ever be a shredding programmer and I am okay with
that. I also realize that not everyone might have their own 13 month old to help distract them.
However, I strongly feel that walking away from my code for extended periods of time
was beneficial for me and may be beneficial for you as well. Now if you
will excuse me, the 13 month old is asleep and I want to figure out what
episodes of <a href="https://www.imdb.com/title/tt0472954/episodes?season=6">It&rsquo;s Always Sunny in Philadelphia Season
6</a> I should
watch.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Have any cities had a team in the championship game for the four major professional sports?]]></title>
    <link href="http://stedy.github.io/blog/2021/02/04/have-any-cities-had-a-team-in-the-championship-game-for-the-four-major-professional-sports/"/>
    <updated>2021-02-04T15:04:06-08:00</updated>
    <id>http://stedy.github.io/blog/2021/02/04/have-any-cities-had-a-team-in-the-championship-game-for-the-four-major-professional-sports</id>
    <content type="html"><![CDATA[<p>The <a href="https://en.wikipedia.org/wiki/2020_Tampa_Bay_Buccaneers_season">Tampa Bay
Buccaneers</a>
are set to play in (and
<a href="https://en.wikipedia.org/wiki/Super_Bowl_LV">host</a>!) the Super Bowl
becoming the third professional sports team from the City of Tampa to go to a championship
game within a year. Tampa does not currently have an NBA team so they
cannot be represented in all four championship games of the major
professional sports leagues. However, Miami does have an NBA team and
also played in the <a href="https://en.wikipedia.org/wiki/2019%E2%80%9320_Miami_Heat_season">2020 NBA
finals</a>
so a pretty good year for professional sports in the State of Florida. Naturally,
this made me wonder if any state has ever had a team in all four
championship games in a calendar year. In 1980 Pennysylvania
accomplished exactly this:</p>

<table>
<thead>
<tr>
<th> League </th>
<th> Team </th>
<th> Outcome </th>
</tr>
</thead>
<tbody>
<tr>
<td> NFL </td>
<td> Pittsburgh Steelers </td>
<td> <a href="https://en.wikipedia.org/wiki/Super_Bowl_XIV">Defeated LA Rams 31-19</a> </td>
</tr>
<tr>
<td> MLB </td>
<td> Philadelphia Phillies  &nbsp; </td>
<td> <a href="https://en.wikipedia.org/wiki/1980_World_Series">Defeated KC Royals 4-2</a> </td>
</tr>
<tr>
<td> NBA </td>
<td> Philadelphia 76ers </td>
<td> <a href="https://en.wikipedia.org/wiki/1980_NBA_Finals">Lost to LA Lakers 4-2</a> </td>
</tr>
<tr>
<td> NHL </td>
<td> Philadelphia Flyers </td>
<td> <a href="https://en.wikipedia.org/wiki/1980_Stanley_Cup_Finals">Lost to NY Islanders 4-2</a> </td>
</tr>
</tbody>
</table>


<p>1980 must have been a great year to live in the Keystone State!</p>

<p>Obviously not every state has a team in all four of the major sports
leagues but when did each state last have a team play in a championship game?
Here are all states with a championship game drought lasting longer than
ten years:</p>

<table>
<thead>
<tr>
<th style="text-align:center;"> Year </th>
<th style="text-align:center;"> State </th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center;"> 1991 </td>
<td style="text-align:center;">Minnesota &nbsp;</td>
</tr>
<tr>
<td style="text-align:center;"> 1992 </td>
<td style="text-align:center;">Oregon</td>
</tr>
<tr>
<td style="text-align:center;"> 1998 </td>
<td style="text-align:center;">Utah</td>
</tr>
<tr>
<td style="text-align:center;"> 2009 </td>
<td style="text-align:center;">Arizona</td>
</tr>
<tr>
<td style="text-align:center;"> 2010 </td>
<td style="text-align:center;">Indiana</td>
</tr>
<tr>
<td style="text-align:center;"> 2010 </td>
<td style="text-align:center;">Louisiana</td>
</tr>
<tr>
<td style="text-align:center;"> 2011 </td>
<td style="text-align:center;">Wisconsin</td>
</tr>
</tbody>
</table>


<p>I wrote all my analysis code and put it in a repo
<a href="https://github.com/stedy/blog-supplemental">here</a>. Finally all of this
talk of 1980s sports made me think of one of the greatest music videos
ever:</p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/kd9TlGDZGkI" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The Great Influenza review]]></title>
    <link href="http://stedy.github.io/blog/2020/08/26/the-great-influenza-review/"/>
    <updated>2020-08-26T15:22:34-07:00</updated>
    <id>http://stedy.github.io/blog/2020/08/26/the-great-influenza-review</id>
    <content type="html"><![CDATA[<p><em>NB:</em> I initally wrote this with the goal of submitting to
<a href="https://slatestarcodex.com/">SlateStarCodex</a> however things are on hold
with that site so I thought I would just post here instead.</p>

<p>As I get older, I find that I am less willing to tolerate bad books. In
the past, out of a strange sense of guilt, I would force myself to keep
reading books even though I did not enjoy them. Nowadays, if I am
starting to get bored with a book I will stop reading as early as
possible and move on to other books. <a href="https://en.wikipedia.org/wiki/The_Great_Influenza">The Great
Influenza</a> by John M.
Berry challenged this mantra immensely.</p>

<p>This was a book I had heard quite a bit about over the past few years
and had long been interested in reading it. I have a background in
public health, attended Johns Hopkins University and I wanted to learn
more about influenza outbreaks - all of which made me excited to finally
sit down and read this book.</p>

<p>This book starts off by making two compelling arguments - the first is
that the US medical education system was incredibly weak in 1918 and
most medical doctors had no real expertise or training upon graduation
and the second argument is that the outbreak began at an army camp in Kansas.
Both of these arguments are methodically presented with extensive
background by the author. About halfway through the book, as the
influenza epidemic continues to grow larger, the author abandons both of
these arguments and shifts to more of a focus on how different
communities and the US Government responded to the outbreak. The author
argues that the Johns Hopkins School of Medicine was intented to improve
medical education in the US and was based on the medical education system in German Universities. Many of the
early founders of the Johns Hopkins School of Medicine are extensively
profiled, only to completely disappear from the book entirely over the
course of the book. This is not uncommon, many
individuals are introduced, given a thorough background and then never
mentioned again.</p>

<p>In the Afterword, the author says he had hoped this book would take only
1-2 years to complete and it ended up taking him seven years to
complete. This is evident as there are many examples of the author
describing events and people in excessive detail only for the event or
person never to never be mentioned again. For example &ldquo;Cincinnati&rsquo;s public health agencies had examined 7,058
influenza victims since the epidemic had ended and found that 5,264
needed some medical assistance; 643 of them had heart problems, and an
extraordinary number of prominent citizens who had influenza had died
suddenly early in 1919.&rdquo; (p. 392) This is the first time in the book
that Cincinnati is mentioned in the book and it is unclear what exactly
these statistical counts add to the narrative as Cincinnati is never
mentioned again.</p>

<p>This book is sorely in need of an editor to prune some of the
superfluous text. For example:</p>

<p>&ldquo;It also seemed - although this was not scientifically established -
that those who went to bed the earliest, stayed there the longest, and
had the best care survived at the highest rates. Those findings meant of
course that the poor died in larger numbers than the rich.&rdquo; (p. 408)</p>

<p>Some basic run on sentences:</p>

<p>&ldquo;Ten days, two weeks, sometimes even longer than two weeks after the
initial attack by the virus, after victims had felt better, after
recovery had seemed to begin, victims were suddenly getting seriously
ill again. And they were dying.&rdquo; (p. 317)</p>

<p>Reading page after page of this is exhausting.</p>

<p>This is not to say the book is all bad. There are some interesting
parallels between today and the 1918 outbreak that are worth exploring.
For example in 1918, President Wilson did not mention the
outbreak at all in public and very rarely if at all in private with his
staff. The US Surgeon General at the time, Rupert Blue, was extremely slow
to even ask for influenza infection counts and instead focused on
extolling patriotism by the virtues of Liberty Loans. In Phoenix and
Philadelphia, &ldquo;Citizens Committees&rdquo; were created by private citizen
groups who took it upon themselves to act to enforce quarantine and sanitation ordinances.</p>

<p>Finally, I was cautiously optimistic about the Afterword,
written ten years after publication in 2018, and I hoped it would be a
breath of fresh air. Sadly it is not. The author rants in the Afterword by claiming that wearing face masks is useless and that the only thing
that might get society through a similar epidemic is belief in our
elected leaders.</p>

<p>In many ways, reading this book is like performing a science experiment
in the sense that there is an interesting story present but to get to it one must sift through a great deal of noise. The book as a whole was moderately
interesting but far, far too long. For additional background I would
also suggest reading the excellent <a href="https://en.wikipedia.org/wiki/Spanish_flu">Wikipedia page about the 1918
Flu</a> instead.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The first book of the year is always a biography]]></title>
    <link href="http://stedy.github.io/blog/2020/01/02/the-first-book-of-the-year-is-always-a-biography/"/>
    <updated>2020-01-02T23:47:35-08:00</updated>
    <id>http://stedy.github.io/blog/2020/01/02/the-first-book-of-the-year-is-always-a-biography</id>
    <content type="html"><![CDATA[<p>For the past few years I have tried to start each New Year off by
selecting a biography as the first book I read that particular year.
I have no specific methodology as to why I choose the biographies I do,
often
it is from something tangential to the topic of the biography or just
because I am interested in the topic in general. Here are some of my
recent picks
as well as my pick for 2020:</p>

<p><strong>2016 - <a href="https://www.librarything.com/work/15743242">Elon Musk: Tesla, SpaceX, and the Quest for a Fantastic
Future</a></strong></p>

<p>I picked up this one largely because I was interested in Elon Musk and
curious to learn more about him. This book was illuminating in regards
to how much self-confidence Musk has in himself and how much he was
willing to double down on himself and how frequently that was
all he needed to move on to more stable ground.</p>

<p><strong>2017 - <a href="https://www.librarything.com/work/8586497">Spread Spectrum</a></strong></p>

<p>I was interested in reading more about <a href="https://en.wikipedia.org/wiki/Hedy_Lamarr">Hedy
Lamarr</a> and this book seemed
like a great place to start. It was self-published and quite bizarre at
times but it did explain technical details quite well and was well-paced
which more than made up for the author&rsquo;s unconventional asides.</p>

<p><strong>2018 - <a href="https://www.librarything.com/work/1653761">The Man Who Fed the World: Nobel Peace Prize Laureate Norman
Borlaug and His Battle to End World
Hunger</a></strong></p>

<p>I was
really interested in learning more about
<a href="https://en.wikipedia.org/wiki/Norman_Borlaug">Borlaug</a> and his
contributions to agronomy but this particular biography was sloppy and
rife with colloquialisms. Borlaug was a fascinating character who likely
saved billions of people from starvation and demands a better biography
than this one.</p>

<p><strong>2019 - <a href="https://www.librarything.com/work/22229559">I Am Dynamite!: A Life of
Nietzsche</a></strong></p>

<p>I did not initially set out to read this book as my first book of the
year. Unfortunately, the book I had
planned on reading, <a href="https://www.librarything.com/work/18814688">In the Great Green Room: The Brilliant and Bold Life
of Margaret Wise</a>, was so
bad
that I bailed on it and read the Nietzsche biography instead. Prior to
reading, I was aware of
Nietzsche if only because of the famous &ldquo;God is dead&rdquo; quote and
something
vaguely related to Nazism. I was fascinated to learn more about him and
how
his writings were completely reappropriated after his death. This was
one of the better books I read all year.</p>

<p>Finally, in 2020 the first book I am reading is <strong><a href="https://www.librarything.com/work/14139984">Empress Dowager Cixi:
The Concubine Who Launched Modern
China</a></strong>. Each year I try
to alternate between male and female biographies and after bailing on
the biography of Margaret Wise last year, I felt it was time to read a
female biography to start 2020. I am also quite interested in China and while
I realize this book might be quite revisionist am curious to learn more
about Cixi and her role in developing modern day China.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Impact of Amazon Echo on babies named Alexa]]></title>
    <link href="http://stedy.github.io/blog/2019/05/23/impact-of-amazon-echo-on-babies-named-alexa/"/>
    <updated>2019-05-23T15:10:53-07:00</updated>
    <id>http://stedy.github.io/blog/2019/05/23/impact-of-amazon-echo-on-babies-named-alexa</id>
    <content type="html"><![CDATA[<p>A few years ago, there was an article in the Seattle Times about <a href="https://www.seattletimes.com/life/for-4-seattle-women-called-alexa-its-fun-frustrating-to-share-name-with-amazon-device/">girls
named Alexa post-introduction of the Amazon
Echo</a>.
I was chatting with a friend of mine about this and we wondered if the
introduction of the Amazon Echo has lead to a reduction in girls named
Alexa. According to Wikipedia, the Amazon Echo was first introduced on
<a href="https://en.wikipedia.org/wiki/Amazon_Echo">November 6, 2014</a> and Hadley
Wickham was kind enough to organize an R package of baby names as
recorded by the <a href="https://github.com/hadley/babynames">United States
SSA</a>. Using this data shows a
steep decline in the number of female babies named Alexa which may be
due to a variety of factors:</p>

<p><img src="http://zachstednick.com/alexa_females.png"></p>

<p>In the process of making this first plot I realized that there are boys
in the SSA data named Alexa as well, lets see what the data for boys looks like:</p>

<p><img src="http://zachstednick.com/alexa_females.png"></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[What is the most remote airport from the city it serves in the United States?]]></title>
    <link href="http://stedy.github.io/blog/2019/05/04/what-is-the-most-remote-airport-from-the-city-it-serves-in-the-united-states/"/>
    <updated>2019-05-04T20:51:52-07:00</updated>
    <id>http://stedy.github.io/blog/2019/05/04/what-is-the-most-remote-airport-from-the-city-it-serves-in-the-united-states</id>
    <content type="html"><![CDATA[<p>Recently, there was a question on
<a href="https://travel.stackexchange.com/questions/137558/what-is-the-most-remote-airport-from-the-center-of-the-city-it-supposedly-serves">travel.SE</a> about locating the airport furthest
from the city it is supposed to serve. There are some interesing answers
including the winner from Paris where the airport is approximately 147
km away from the Paris metro area. I started to wonder about this
question for different airports
within the US and I stumbled onto this <a href="https://en.wikipedia.org/wiki/List_of_airports_in_the_United_States">Wikipedia page</a>
which served as a good baseline for a quick analysis.</p>

<p>I used the <a href="https://cloud.google.com/maps-platform/places/">Google Maps location
API</a> to calculate
latitude and longitude
for the center
of the town or city and location of the airport. I then used the Google
Maps directions API to calculate the driving distance between the
airport and the center of the town it serves. This lead to some
interesting edge cases. For example, Peach Springs, Arizona is on this
list and the airport is about 113 miles away:</p>

<p><img src="http://zachstednick.com/peach_springs_drive.png"></p>

<p><a href="https://en.wikipedia.org/wiki/Peach_Springs,_Arizona">Peach Springs</a> is a Census Designated Place and largely serves the
Hualapai tribe. Should it have been included on this list?</p>

<p>I also noticed on this Wikipedia page that there are many airports with
split locations (<a href="https://en.wikipedia.org/wiki/Seattle%E2%80%93Tacoma_International_Airport">Sea/Tac
Airport</a>
for example)
or a single airport serving multiple locations such as <a href="https://en.wikipedia.org/wiki/Harrisburg_International_Airport">Harrisburg
International
Airport</a>
serving
<a href="https://en.wikipedia.org/wiki/Harrisburg,_Pennsylvania">Harrisburg</a>/<a href="https://en.wikipedia.org/wiki/Middletown,_Dauphin_County,_Pennsylvania">Middletown,
PA</a>).
For these cases I just used
the first city mentioned for this analysis. The Wikipedia article also
lists
enplanements as recorded by the FAA in 2015 which provides a useful
metric for comparison. First I looked
at distance to airport versus number of passengers:</p>

<p><img src="http://zachstednick.com/airport_all.png"></p>

<p>Not too surprising to observe that many of the airports have very few
emboardings each year and are reasonably close to the center of town.</p>

<p>Then I looked at only airports that had over one million enplanements
which narrowed my list of airports down to 27:</p>

<p><img src="http://zachstednick.com/airport_1mil.png"></p>

<p>To me it is interesting to note that San Diego, Boston and to some
extent Dulles are all close to the city center with relatively few
emboardings as compared to this subset. Also, I grew up in Fort Collins,
CO and have many fond memories as a child of driving across the plains
with Denver International Airport seeming so, so far away.</p>

<p>As always, full code available on
<a href="https://github.com/stedy/blog-supplemental">GitHub</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[changes in voter turnout between the 2014 and 2018 US elections]]></title>
    <link href="http://stedy.github.io/blog/2018/12/03/changes-in-voter-turnout-between-the-2014-and-2018-us-elections/"/>
    <updated>2018-12-03T22:08:34-08:00</updated>
    <id>http://stedy.github.io/blog/2018/12/03/changes-in-voter-turnout-between-the-2014-and-2018-us-elections</id>
    <content type="html"><![CDATA[<p>As I watched the livestream of the 2018 US midterm election results, I
was absolutely stunned at the significant increase in voter turnout over
the 2014 US midterm election. Now that almost all of the 2018 election
results have been certified by their repective Secretaries of State, I
wanted to take a look at how this increase in voter turnout manifested
on a state by state basis.</p>

<p>To make things as simple as I could, I primarily used the data from two
New York Times elections pages: the <a href="https://www.nytimes.com/elections/2014/results/house">2014 results
page</a> and the
<a href="https://www.nytimes.com/interactive/2018/11/06/us/elections/results-house-elections.html">2018 results
page</a>.</p>

<p>I realize this data may not be complete as some of the voter counts are
not fully reported for all precincts on these pages. However, the total
vote count from the 2014 data is 72,031,124 while the total vote count
from 2018 is 106,385,810 which I felt was accurate enough for the
purposes of this analysis.</p>

<p>In both elections, I primarily focused on the House of Representatives
because that was the only office for universally up for election. As I
embarked on this project I soon realized that a direct comparison would
not be possible after Pennslyvania <a href="https://en.wikipedia.org/wiki/Pennsylvania's_congressional_districts">re-drew its congressional maps in
early
2018</a>
and Florida did so as well in a <a href="https://en.wikipedia.org/wiki/Florida%27s_congressional_districts">2016
redistricting</a>.</p>

<p>This first map simply shows congressional districts where the voter
turnout increased. The congressional districts colored grey have either
a decline in voter turnout, or where the candidate ran uncontested in
either 2014 or 2018 (or both) and therefore do not have a difference in
percentage to measure.</p>

<p><img src="http://zachstednick.com/US_house_diff_map_pos_only.png"></p>

<p>Explorable version
<a href="http://zachstednick.com/US_house_diff_map_pos_only.html">here</a></p>

<p>The second choropleth map shows states that increased in voter turnout
as blue, states that decreased in voter turnout as red while those
colored grey had at least one uncontested election.</p>

<p><img src="http://zachstednick.com/house_diff_map_pos_neg.png">
Explorable version of this map
<a href="http://zachstednick.com/house_diff_map_pos_neg.html">here</a></p>

<p>It is interesting to note that only seven US Congressional districts had
decreases in voter turnout from 2014 to 2018. Table of districts with
decreased voter turnout:</p>

<table>
<thead>
<tr>
<th> District </th>
<th> 2014 votes </th>
<th> 2018 votes </th>
<th style="text-align:right;"> Difference</th>
</tr>
</thead>
<tbody>
<tr>
<td> IL-09 </td>
<td> 203946 </td>
<td> 91476 </td>
<td style="text-align:right;"> -55.14</td>
</tr>
<tr>
<td> CO-01 </td>
<td> 266021 </td>
<td> 256542 </td>
<td style="text-align:right;"> -3.56</td>
</tr>
<tr>
<td> PA-02 </td>
<td> 202635 </td>
<td> 197495 </td>
<td style="text-align:right;"> -2.53</td>
</tr>
<tr>
<td> AK-00 </td>
<td> 242844 </td>
<td> 238131 </td>
<td style="text-align:right;"> -1.94</td>
</tr>
<tr>
<td> IL-07 </td>
<td> 171502 </td>
<td> 170290 </td>
<td style="text-align:right;"> -0.70</td>
</tr>
<tr>
<td> AR-04 </td>
<td> 205066 </td>
<td> 204113 </td>
<td style="text-align:right;"> -0.46</td>
</tr>
<tr>
<td> KY-05 </td>
<td> 218697 </td>
<td> 218324 </td>
<td style="text-align:right;"> -0.17</td>
</tr>
</tbody>
</table>


<p>The district with the highest increase in turnout? That would be
<a href="https://ballotpedia.org/California's_34th_Congressional_District">CA-34</a></p>

<p>Finally I grouped all the votes together by state to make the following
choropleth of US House votes on a state level:</p>

<p><img src="http://zachstednick.com/state_vote_diff_map.png">
Explorable version of this map
<a href="http://zachstednick.com/state_vote_diff_map.html">here</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A visual comparison of votes for two carbon tax initiatives]]></title>
    <link href="http://stedy.github.io/blog/2018/11/10/a-visual-comparison-of-votes-for-two-carbon-tax-initiatives/"/>
    <updated>2018-11-10T21:52:00-08:00</updated>
    <id>http://stedy.github.io/blog/2018/11/10/a-visual-comparison-of-votes-for-two-carbon-tax-initiatives</id>
    <content type="html"><![CDATA[<p>Washington State voters were presented two different
carbon tax initiatives in the General Elections of 2016 and 2018. A full comparison of both
proposals is
<a href="http://carbonwa.org/1631-compare-recent-carbon-pricing-proposals-washington-state/">here</a>.
While neither passed, I was curious how the Yes vote looked for both
Initiatives.</p>

<p>Map of percentage of Yes votes for Initiative 732 (General Election
2016), hover mouse cursor over county for exact Yes percentage.</p>

<iframe src="http://zachstednick.com/map_732.html" height=500 width=520></iframe>


<p>Map of percentage of Yes votes for Initiative 1631 (General Election
2018), hover mouse cursor over county for exact Yes percentage.</p>

<iframe src="http://zachstednick.com/map_1631.html" height=500 width=520></iframe>


<p>Overall net change in Yes votes from Initiative 732 to Initiative 1631 as a percentage of the whole by county. The more red counties are where Initiative 732 was favored more while the more blue counties are where Initiative 1631 was favored more. Hover mouse cursor over county for net difference between Yes votes for both initiatives.</p>

<iframe src="http://zachstednick.com/diff_map.html" height=500 width=620></iframe>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A quick look at the Seattle Mariners 2018 attendance]]></title>
    <link href="http://stedy.github.io/blog/2018/10/01/a-quick-look-at-the-seattle-mariners-2018-attendance/"/>
    <updated>2018-10-01T21:33:20-07:00</updated>
    <id>http://stedy.github.io/blog/2018/10/01/a-quick-look-at-the-seattle-mariners-2018-attendance</id>
    <content type="html"><![CDATA[<p>On September 17, 2018, the King County council <a href="https://mkcclegisearch.kingcounty.gov/LegislationDetail.aspx?ID=3585391&amp;GUID=4C4DA409-D946-4189-AF23-59682F4FEF44&amp;Options=Advanced&amp;Search=">voted
5-4</a>
to allow for a
new funding agreement between King County and sports stadiums such as
<a href="https://en.wikipedia.org/wiki/Safeco_Field">Safeco Field</a>. There have
already
been challenges
to the legislation and it may appear as a
<a href="https://komonews.com/news/local/residents-could-get-to-vote-whether-safeco-field-gets-its-135-million-in-county-funding">petition</a>
on the ballot soon.</p>

<p>The 2018 season was unusually successful for the Mariners, and while
they are again sitting out the Playoffs this season I did wonder what
attendance looked like this year.</p>

<p><img src="http://zachstednick.com/Mariners_Attendance_2018.png"></p>

<p>I added two lines to this plot, one for maximum attendance (currently
47,715 according to
<a href="https://en.wikipedia.org/wiki/Safeco_Field#Seating_capacity">Wikipedia</a>)
and one for mean attendance which was 28,389 this year. This means that
we have a
stadium that is on average about 60% full for any game for a team that
competed for a
Playoff spot until the very end of the
<a href="https://www.seattletimes.com/sports/mariners/robinson-cano-breaks-out-bat-as-mariners-score-season-high-13-runs-to-bash-rangers/">season</a>.
Is that really the best use of this money?</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sentiment analysis of Candidate Statements by Senate candidates]]></title>
    <link href="http://stedy.github.io/blog/2018/08/08/sentiment-analysis-of-candidate-statements-by-senate-candidates/"/>
    <updated>2018-08-08T16:00:05-07:00</updated>
    <id>http://stedy.github.io/blog/2018/08/08/sentiment-analysis-of-candidate-statements-by-senate-candidates</id>
    <content type="html"><![CDATA[<p>The <a href="https://www.sos.wa.gov/elections/calendar.aspx">2018 Washington State
Primary</a> was held on
August 7, 2018. As a
registered voter in Washington State I am mailed a <a href="https://www.sos.wa.gov/elections/research/2018-voters-pamphlet.aspx">Voter&rsquo;s Information
Pamphlet</a>
which lists the candidates and a Candidate
Statement (provided by the candidate) for each office. The Candidate
Statement is where the candidate is allowed to write anything they want
as long as its under 300 words. I was curious if there was any
relationship between the sentiment of a candidate&rsquo;s Statement and the
number of votes
that candidate received.</p>

<p>I conducted my analysis using sentiment analysis which groups words
together based on
pre-defined lists of words that are members of that group. There are
many word groups to choose from such as &ldquo;joy&rdquo; and &ldquo;trust&rdquo; but for this
analysis I just looked at &ldquo;positive&rdquo; and &ldquo;negative&rdquo; words (as classified
by <a href="http://www.saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm">NRC</a>).</p>

<p>Although there were many different offices up for election in this Primary,
the U.S. Senate race had 29 candidates which made for a very rich
dataset. Washington State uses a <a href="https://en.wikipedia.org/wiki/Nonpartisan_blanket_primary">Top-Two
Primary</a>
which allows for easy comparison across political parties.</p>

<p>There are many factors deliberately ignored by this analysis such as
<a href="https://en.wikipedia.org/wiki/Cook_Partisan_Voting_Index">PVI</a>,
incumbency, fundraising, political party and candidate issues. However I thought
text mining could
be an interesting way to analyze these candidates in a slightly
different manner.</p>

<p>First I just plotted the number of positive words in the Candidate
Statement for each candidate:</p>

<p><img src="http://zachstednick.com/WA_candidate_positive.png"></p>

<p>Then I repeated with the number of negative words in the Candidate
Statement for each candidate:</p>

<p><img src="http://zachstednick.com/WA_candidate_negative.png"></p>

<p>Next I looked at the number of positive words in the Candidate Statement
by candidate versus the number of votes that candidate received. Because
of the strength of the Democratic incumbent candidate <a href="https://ballotpedia.org/Maria_Cantwell">Maria
Cantwell</a> and the Republican
establishment candidate <a href="https://ballotpedia.org/Susan_Hutchison">Susan
Hutchison</a> I had to
<a href="https://en.wikipedia.org/wiki/Logarithm">log-transform</a> the vote counts because these two candidates got so much of
the total vote.</p>

<p><img src="http://zachstednick.com/WA_candidate_positive_vote_count.png"></p>

<p>Then I repeated this analysis by looking at the negative word count for
each candidate versus the total log-transformed vote count:</p>

<p><img src="http://zachstednick.com/WA_candidate_negative_vote_count.png"></p>

<p>While I don&rsquo;t think this will lead to any novel political insights, I
do think its an interesting way to look at candidates. Full code
including individual candidate statements available
<a href="https://github.com/stedy/candidate_sentiment_analysis">here</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Should Seattle convert public golf courses into land for housing?]]></title>
    <link href="http://stedy.github.io/blog/2018/07/18/should-seattle-convert-public-golf-courses-into-land-for-housing/"/>
    <updated>2018-07-18T13:19:56-07:00</updated>
    <id>http://stedy.github.io/blog/2018/07/18/should-seattle-convert-public-golf-courses-into-land-for-housing</id>
    <content type="html"><![CDATA[<p>Like many other metropolitan areas, Seattle is currently dealing with a
serious housing shortage. Recently, there have been some good articles
(<a href="https://seattletransitblog.com/2018/06/11/west-seattle-can-tunnel-housing/">here</a>
and
<a href="https://fancybeans.com/2018/01/27/what-is-a-housing-emergency-really/">here</a>)
about converting public golf
courses into housing. It is an interesting concept and certainly should
be discussed, however one aspect I feel is being overlooked in this
debate
is how popular are these golf courses anyways? According to Bloomberg
News, there is <a href="https://www.bloomberg.com/gadfly/articles/2017-05-25/off-for-a-round-of-golf-this-weekend-didn-t-think-so">declining interest in golf
nationwide</a>,
is this happening in
Seattle?</p>

<p>The City of Seattle is somewhat
aware of this issue and is at least discussing
<a href="https://www.seattle.gov/Documents/Departments/ParksAndRecreation/BriefingPapers/Golf%20Park%20Board%20Briefing%20Paper%20Final%202-6-18.pdf">options</a>
for the golf courses. I filed a <a href="http://www.seattle.gov/public-records">Public Records
Request</a> act
with the City of Seattle and they sent me data on the rounds of golf
played for
the past three years.</p>

<p>Unfortunately the city only provided the count data for number of
rounds of golf played per year so I cannot look at more granular trends.
However, I can plot the counts on an annual basis.</p>

<p><img src="http://zachstednick.com/Seattle_golf_counts.png"></p>

<p>It will be interesting to see if this debate both on converting public
golf
courses to housing goes anywhere both in Seattle and other major cities.
I don&rsquo;t have a horse in this
particular race, I live near the <a href="http://premiergc.com/-jefferson-park-golf-course">Jefferson
Park course</a> and I
really appreciate the large
expanse of green it provides however I am acutely aware of the need for
more housing within Seattle city limits.</p>

<p>Interested in taking a look? I put the full data I got from the city
<a href="https://github.com/stedy/city_of_seattle_golf_counts">here</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A visual ranking of Seattle public elementary schools]]></title>
    <link href="http://stedy.github.io/blog/2018/05/27/a-visual-ranking-of-seattle-public-elementary-schools/"/>
    <updated>2018-05-27T23:08:01-07:00</updated>
    <id>http://stedy.github.io/blog/2018/05/27/a-visual-ranking-of-seattle-public-elementary-schools</id>
    <content type="html"><![CDATA[<p>One of the things people consistently tell me when they are
considering buying or renting a home in a new location is
that they want to move somewhere with &ldquo;good schools.&rdquo; This always makes
me wonder how we quantify what schools are
considered &ldquo;good&rdquo;? To start you might look for information on school
reputations or performance using local school data fact sheets from
realtors or apartment managers or, more likely, by searching ranking and
review sites such as
<a href="https://www.niche.com/k12/thurgood-marshall-elementary-school-seattle-wa/">Niche</a>.
This approach is generally fine but it assumes that you are only
interested in a specific neighborhood which may be difficult to achieve
right now in most major cities across the US and especially in
<a href="https://www.seattletimes.com/business/real-estate/new-home-price-records-777000-in-seattle-950000-on-the-eastside/">Seattle</a>.</p>

<p>What if instead we looked at all the neighborhood school ratings in a
city simultaneously? This would
allow the reader to spot trends and make visual comparisons as well
as possibly identify overperforming schools in unexpected areas.
Fortunately, Seattle Public Schools (SPS) provides quite a lot of
<a href="https://www.seattleschools.org/cms/One.aspx?portalId=627&amp;pageId=6369011">data</a>
about their schools which makes this easy to visualize.</p>

<h1>Setup</h1>

<ul>
<li><p>For this analysis I just the SPS data for the 2016-17 school
year.</p></li>
<li><p>I focused solely on elementary school data for the 2016-17
school year. I used the SPS district boundary map for all public elementary
schools in the City of Seattle and ignored any magnet or alternative
elementary schools.</p></li>
</ul>


<h1>Rankings</h1>

<p>Initially I focused on three questions:</p>

<ul>
<li>What school has the best student/teacher ratio?</li>
<li>What school reports the best attendance?</li>
<li>What schools are best for reading and math?</li>
</ul>


<p>I took the SPS data for student/teacher ratio, attendance rate, and reading and math
proficiency scores for each school and calculated their rank
within the city to make this table. Click on the category name to sort
by that category.</p>

<p><link rel="stylesheet" type="text/css"
src="http://cdn.datatables.net/1.10.16/css/jquery.dataTables.min.css" /></p>

<script src="https://code.jquery.com/jquery-3.3.1.min.js"
        integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8="
        crossorigin="anonymous">
</script>




<script type="text/javascript"
src="http://cdn.datatables.net/1.10.16/js/jquery.dataTables.min.js"
></script>


<script>
$(document).ready( function () {
      $('#myTable').DataTable({paging: false, info: false});
} );

</script>








<table border=1 id="myTable">
  <thead>
<tr> <th> School Name </th> <th> Attendance Rank </th> <th>
Student/Teacher Ratio Rank </th> <th> Grade 3 Math Rank </th> <th> Grade
3 Reading Rank </th>  </tr>
</thead>
<tbody>
  <tr> <td> Adams </td> <td align="right"> 12 </td> <td align="right">
53 </td> <td align="right"> 29 </td> <td align="right"> 21 </td> </tr>
  <tr> <td> Alki </td> <td align="right"> 20 </td> <td align="right"> 35
</td> <td align="right"> 12 </td> <td align="right"> 15 </td> </tr>
  <tr> <td> Arbor Heights </td> <td align="right"> 21 </td> <td
align="right"> 31 </td> <td align="right"> 45 </td> <td align="right">
33 </td> </tr>
  <tr> <td> Gatzert </td> <td align="right"> 42 </td> <td align="right">
4 </td> <td align="right"> 49 </td> <td align="right"> 56 </td> </tr>
  <tr> <td> Beacon Hill Int&#8217;l </td> <td align="right"> 1 </td> <td
align="right"> 32.50 </td> <td align="right"> 30 </td> <td
align="right"> 37 </td> </tr>
  <tr> <td> B.F. Day </td> <td align="right"> 16 </td> <td
align="right"> 43 </td> <td align="right"> 25 </td> <td align="right">
23 </td> </tr>
  <tr> <td>  Broadview-Thomson K-8 </td> <td align="right"> 49 </td> <td
align="right"> 2 </td> <td align="right"> 46 </td> <td align="right"> 45
</td> </tr>
  <tr> <td> Bryant </td> <td align="right"> 6 </td> <td align="right">
55 </td> <td align="right"> 3 </td> <td align="right"> 4 </td> </tr>
  <tr> <td> Cascadia </td> <td align="right"> 2 </td> <td align="right">
59 </td> <td align="right"> 1 </td> <td align="right"> 1 </td> </tr>
  <tr>  <td> Catharine Blaine K-8 </td> <td align="right"> 38 </td> <td
align="right"> 15 </td> <td align="right"> 6 </td> <td align="right"> 13
</td> </tr>
  <tr>  <td> Concord Int&#8217;l </td> <td align="right"> 44 </td> <td
align="right"> 22 </td> <td align="right"> 59 </td> <td align="right">
60 </td> </tr>
  <tr>  <td> Bagley </td> <td align="right"> 10 </td> <td align="right">
20 </td> <td align="right"> 24 </td> <td align="right"> 19 </td> </tr>
  <tr>  <td> Dearborn Park Int&#8217;l </td> <td align="right"> 61 </td> <td
align="right"> 61 </td> <td align="right"> 61 </td> <td align="right">
61 </td> </tr>
  <tr>  <td> Dunlap </td> <td align="right"> 43 </td> <td align="right">
5 </td> <td align="right"> 56 </td> <td align="right"> 55 </td> </tr>
  <tr>  <td> Emerson </td> <td align="right"> 58 </td> <td
align="right"> 14 </td> <td align="right"> 43 </td> <td align="right">
53 </td> </tr>
  <tr>  <td> Fairmount Park </td> <td align="right"> 22 </td> <td
align="right"> 47 </td> <td align="right"> 2 </td> <td align="right"> 3
</td> </tr>
  <tr>  <td> Coe </td> <td align="right"> 9 </td> <td align="right"> 48
</td> <td align="right"> 10 </td> <td align="right"> 9 </td> </tr>
  <tr>  <td> Gatewood </td> <td align="right"> 35 </td> <td
align="right"> 26 </td> <td align="right"> 33 </td> <td align="right">
40 </td> </tr>
  <tr>  <td> Genesee Hill </td> <td align="right"> 24 </td> <td
align="right"> 50 </td> <td align="right"> 16 </td> <td align="right">
20 </td> </tr>
  <tr>  <td> Graham Hill </td> <td align="right"> 39 </td> <td
align="right"> 10 </td> <td align="right"> 55 </td> <td align="right">
49 </td> </tr>
  <tr>  <td> Green Lake </td> <td align="right"> 31 </td> <td
align="right"> 37 </td> <td align="right"> 26 </td> <td align="right">
28 </td> </tr>
  <tr>  <td> Greenwood </td> <td align="right"> 15 </td> <td
align="right"> 54 </td> <td align="right"> 20 </td> <td align="right">
17 </td> </tr>
  <tr>  <td> Hawthorne </td> <td align="right"> 46 </td> <td
align="right"> 16 </td> <td align="right"> 48 </td> <td align="right">
41 </td> </tr>
  <tr>  <td> Highland Park </td> <td align="right"> 48 </td> <td
align="right"> 6 </td> <td align="right"> 57 </td> <td align="right"> 50
</td> </tr>
  <tr>  <td> Hay </td> <td align="right"> 8 </td> <td align="right"> 38
</td> <td align="right"> 19 </td> <td align="right"> 10 </td> </tr>
  <tr>  <td> John Muir </td> <td align="right"> 23 </td> <td
align="right"> 25 </td> <td align="right"> 52 </td> <td align="right">
48 </td> </tr>
  <tr>  <td> John Rogers </td> <td align="right"> 33 </td> <td
align="right"> 24 </td> <td align="right"> 34 </td> <td align="right">
34 </td> </tr>
  <tr>  <td> Kimball </td> <td align="right"> 30 </td> <td
align="right"> 32.50 </td> <td align="right"> 40 </td> <td
align="right"> 36 </td> </tr>
  <tr>  <td> Lafayette </td> <td align="right"> 29 </td> <td
align="right"> 44 </td> <td align="right"> 27 </td> <td align="right">
25 </td> </tr>
  <tr>  <td> Laurelhurst </td> <td align="right"> 34 </td> <td
align="right"> 49 </td> <td align="right"> 15 </td> <td align="right">
24 </td> </tr>
  <tr>  <td> Lawton </td> <td align="right"> 26 </td> <td align="right">
42 </td> <td align="right"> 4 </td> <td align="right"> 6 </td> </tr>
  <tr>  <td> Leschi </td> <td align="right"> 53 </td> <td align="right">
36 </td> <td align="right"> 37 </td> <td align="right"> 44 </td> </tr>
  <tr>  <td> Lowell </td> <td align="right"> 60 </td> <td align="right">
3 </td> <td align="right"> 58 </td> <td align="right"> 54 </td> </tr>
  <tr>  <td> Loyal Heights </td> <td align="right"> 17 </td> <td
align="right"> 58 </td> <td align="right"> 5 </td> <td align="right"> 7
</td> </tr>
  <tr>  <td> Madrona </td> <td align="right"> 52 </td> <td
align="right"> 1 </td> <td align="right"> 47 </td> <td align="right"> 43
</td> </tr>
  <tr>  <td> Maple </td> <td align="right"> 19 </td> <td align="right">
28 </td> <td align="right"> 31 </td> <td align="right"> 32 </td> </tr>
  <tr>  <td> MLK Jr. </td> <td align="right"> 45 </td> <td
align="right"> 8 </td> <td align="right"> 54 </td> <td align="right"> 58
</td> </tr>
  <tr>  <td> McDonald International </td> <td align="right"> 7 </td> <td
align="right"> 30 </td> <td align="right"> 23 </td> <td align="right"> 2
</td> </tr>
  <tr>  <td> McGilvra </td> <td align="right"> 25 </td> <td
align="right"> 40 </td> <td align="right"> 14 </td> <td align="right">
11 </td> </tr>
  <tr>  <td> Montlake </td> <td align="right"> 37 </td> <td
align="right"> 51 </td> <td align="right"> 9 </td> <td align="right"> 16
</td> </tr>
  <tr>  <td> North Beach </td> <td align="right"> 14 </td> <td
align="right"> 46 </td> <td align="right"> 13 </td> <td align="right">
12 </td> </tr>
  <tr>  <td> Northgate </td> <td align="right"> 51 </td> <td
align="right"> 11 </td> <td align="right"> 50 </td> <td align="right">
51 </td> </tr>
  <tr>  <td> Olympic Hills </td> <td align="right"> 41 </td> <td
align="right"> 34 </td> <td align="right"> 21 </td> <td align="right">
30 </td> </tr>
  <tr>  <td> Olympic View </td> <td align="right"> 27 </td> <td
align="right"> 56 </td> <td align="right"> 32 </td> <td align="right">
29 </td> </tr>
  <tr>  <td> Queen Anne </td> <td align="right"> 28 </td> <td
align="right"> 57 </td> <td align="right"> 28 </td> <td align="right">
22 </td> </tr>
  <tr>  <td> Rainier View </td> <td align="right"> 55 </td> <td
align="right"> 21 </td> <td align="right"> 18 </td> <td align="right">
26 </td> </tr>
  <tr>  <td> Roxhill </td> <td align="right"> 56 </td> <td
align="right"> 13 </td> <td align="right"> 53 </td> <td align="right">
57 </td> </tr>
  <tr>  <td> Sacajawea </td> <td align="right"> 36 </td> <td
align="right"> 7 </td> <td align="right"> 42 </td> <td align="right"> 42
</td> </tr>
  <tr>  <td> Sand Point </td> <td align="right"> 47 </td> <td
align="right"> 27 </td> <td align="right"> 39 </td> <td align="right">
35 </td> </tr>
  <tr>  <td> Sanislo </td> <td align="right"> 57 </td> <td
align="right"> 12 </td> <td align="right"> 60 </td> <td align="right">
59 </td> </tr>
  <tr>  <td> Stevens </td> <td align="right"> 32 </td> <td
align="right"> 29 </td> <td align="right"> 35 </td> <td align="right">
31 </td> </tr>
  <tr>  <td> Thornton Creek </td> <td align="right"> 18 </td> <td
align="right"> 17 </td> <td align="right"> 41 </td> <td align="right">
38 </td> </tr>
  <tr>  <td> Thurgood Marshall </td> <td align="right"> 5 </td> <td
align="right"> 39 </td> <td align="right"> 8 </td> <td align="right"> 18
</td> </tr>
  <tr>  <td> Van Asselt </td> <td align="right"> 59 </td> <td
align="right"> 9 </td> <td align="right"> 51 </td> <td align="right"> 52
</td> </tr>
  <tr>  <td> Viewlands </td> <td align="right"> 40 </td> <td
align="right"> 19 </td> <td align="right"> 44 </td> <td align="right">
47 </td> </tr>
  <tr>  <td> View Ridge </td> <td align="right"> 3 </td> <td
align="right"> 52 </td> <td align="right"> 11 </td> <td align="right"> 5
</td> </tr>
  <tr>  <td> Wedgwood </td> <td align="right"> 11 </td> <td
align="right"> 41 </td> <td align="right"> 7 </td> <td align="right"> 14
</td> </tr>
  <tr>  <td> West Seattle Elem </td> <td align="right"> 54 </td> <td
align="right"> 23 </td> <td align="right"> 38 </td> <td align="right">
46 </td> </tr>
  <tr>  <td> West Woodland </td> <td align="right"> 13 </td> <td
align="right"> 45 </td> <td align="right"> 17 </td> <td align="right"> 8
</td> </tr>
  <tr>  <td> Whittier </td> <td align="right"> 4 </td> <td
align="right"> 60 </td> <td align="right"> 22 </td> <td align="right">
27 </td> </tr>
  <tr>  <td> Wing Luke </td> <td align="right"> 50 </td> <td
align="right"> 18 </td> <td align="right"> 36 </td> <td align="right">
39 </td> </tr>
  <tr>  <td> Schmitz Park </td> <td align="right"> 62 </td> <td
align="right"> 62 </td> <td align="right"> 62 </td> <td align="right">
62 </td> </tr>
</tbody>
</table>


<p>What jumps out at me most is that no particular school consistently
out-performs the others, which can make it challenging to decide what
to prioritize when choosing a school.</p>

<h1>Student/Teacher ratio</h1>

<p>Each school reports the number of enrolled students and the number of
teachers which I simply used to calculate a ratio.</p>

<p><em>Click on an attendance area for the exact percentage.</em></p>

<iframe src="http://zachstednick.com/student_teacher_ratio_map.html"
height=850 width=520></iframe>


<h1>Attendance</h1>

<p>I was initially interested in student attendance data, but the elementary
school with the
lowest daily attendance was Lowell Elementary with an attendance rate of
89%. Every other school reported an attendance rate at or
above 95% which did not make for a very interesting map. I later learned
that Washington State has a compulsory
attendance <a href="http://apps.leg.wa.gov/RCW/default.aspx?cite=28A.225">law</a>
which likely affected these numbers.</p>

<h1>Reading proficiency</h1>

<p>I was interested in looking at district-wide third grade reading achievement scores district-wide
for 3rd
graders as measured by the <a href="http://www.k12.wa.us/Assessment/StateTesting/default.aspx">Washington State proficiency
test</a>. I
chose third grade because that is the first year a Washington State standardized test is
administered for reading.</p>

<p><em>Click on an attendance area for the exact percentage.</em></p>

<iframe src="http://zachstednick.com/grade3_reading_map.html" height=850
width=520></iframe>


<h1>Math proficiency</h1>

<p>Similarly, I looked at the district-wide third grade
math achievement scores as measured by the <a href="http://www.k12.wa.us/Assessment/StateTesting/default.aspx">Washington State proficiency
test</a></p>

<p><em>Click on an attendance area for the exact percentage.</em></p>

<iframe src="http://zachstednick.com/grade3_math_map.html" height=850
width=520></iframe>


<h1>Family engagement</h1>

<p>SPS provides a parent survey with a variety of questions evaluating
parent enthusiasm and approval of Seattle schools. These survey results are not published, so I looked at how many families
completed these surveys for the 2016-17 school year.</p>

<p><em>Click on an attendance area for the exact percentage.</em></p>

<iframe src="http://zachstednick.com/family_engagement_map.html"
height=850 width=520></iframe>


<p>Even the school with the most responses reported that only 49.1% of families responded to the
survey which to me means that most families are satisfied with
their school but neither especially excited or disappointed by their school experiences.</p>

<p><strong>tl;dr</strong> Choosing a school is hard but ultimately it comes down to how
satisfied the parents or guardians are with the school. Schools report
on a wide array of metrics about student performance,
but performance is often an issue of secondary importance when
compared to parents&#8217; overall perception of the school quality.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Visualizing flight data for the 2017 Seattle Mariners]]></title>
    <link href="http://stedy.github.io/blog/2018/05/26/visualizing-flight-data-for-the-2017-seattle-mariners/"/>
    <updated>2018-05-26T17:11:21-07:00</updated>
    <id>http://stedy.github.io/blog/2018/05/26/visualizing-flight-data-for-the-2017-seattle-mariners</id>
    <content type="html"><![CDATA[<p>Remember this map that Facebook created of friend connections back in
2011?</p>

<p><img
src="https://advox.globalvoices.org/wp-content/uploads/2015/05/5261568726_d51149d62c_b-800x398.jpg"
height="430" width="670" ></p>

<p>I thought it was pretty cool back then and I still
think its pretty cool. I wanted to make a similar map but was not sure
where to start. I could have done a similar visualization however I
recently quit
Facebook so I can no longer export all my friend&rsquo;s data to use for
making maps. My next thought was visualizing travel routes such as flight information. I am trying to
reduce my carbon footprint which meant I only flew five times in 2017
and have flown exactly zero times so far in 2018. Then I thought, you
know who does
fly alot? The Seattle Mariners.</p>

<p>First step was to collect all the Mariners game data, fortunately
Baseball Reference has all that data in an easily accessible <a href="https://www.baseball-reference.com/teams/SEA/2017-schedule-scores.shtml">HTML
table</a>.</p>

<p>Next step was to geolocate all the stadiums which can be a bit tedious.
Fortunately GitHub user
<a href="https://github.com/the55">the55</a> created a nice JSON file of all the
stadiums and put it as a
<a href="https://gist.github.com/the55/2155142">gist</a>. I was able to use an R
library called
<a href="https://cran.r-project.org/web/packages/geosphere/">geosphere</a> for
using the <a href="https://en.wikipedia.org/wiki/Haversine_formula">Haversine
formula</a> to calculate
the distance between two stadiums.</p>

<p>My initial attempt here:</p>

<p><img src="http://zachstednick.com/mariners_base.svg"></p>

<p>In order to make the image look similar to the Facebook connection map,
I ended up using this <a href="https://flowingdata.com/2011/05/11/how-to-map-connections-with-great-circles/">Flowing Data
post</a>
quite a bit to figure out how to
add the lines and change the background color:</p>

<p><img src="http://zachstednick.com/mariners_black_bg.svg"></p>

<p>Finally because there were so many trips from Seattle to American League
West opponents that I ended up adding a bit of <a href="https://en.wikipedia.org/wiki/Jitter">noise or
jitter</a> to the stadium locations
to make the flight paths not perfectly overlap each other.</p>

<p><img src="http://zachstednick.com/mariners_map_with_noise.svg"></p>

<p>Looking back at this 2017 reminded me the Mariners finished 78-84 in
2017, here&rsquo;s hoping to a better season in 2018!</p>

<p>If interested, I put all the code for this analysis
<a href="https://github.com/stedy/blog-supplemental/blob/master/analysis/mariners_2017_travel_map.R">here</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Further analysis of the 2017-18 WA State Legislature]]></title>
    <link href="http://stedy.github.io/blog/2018/04/05/further-analysis-of-the-2017-18-wa-state-legislature/"/>
    <updated>2018-04-05T23:11:26-07:00</updated>
    <id>http://stedy.github.io/blog/2018/04/05/further-analysis-of-the-2017-18-wa-state-legislature</id>
    <content type="html"><![CDATA[<p>This is my second post looking at the data from the 2017-18 Washington
State Legislative Session. the first part of this blog can be
read
<a href="http://zachstednick.name/blog/2018/04/05/visualizing-the-2017-18-wa-state-legislature/">here</a></p>

<p>After some time looking at different bills that did pass, I started to
wonder if a bill
was more likely to pass if it had more sponsors. First I took the 647
bills passed by the Legislation and signed into law by Governor and
looked up how many co-sponsors each bill had:</p>

<p><img src="http://zachstednick.com/all_sponsor_counts.png"></p>

<p>Then I I took every bill that was introduced but did not become
law and counted up the sponsors for these:</p>

<p><img src="http://zachstednick.com/all_sponsor_counts_not_passed.png"></p>

<p>So it appears that the number of sponsors is not particulary predictive
for a bill becoming law. The three bills introduced in the Senate with
the highest number of Sponsors were:</p>

<table>
<thead>
<tr>
<th>Bill </th>
<th> Sponsor count </th>
<th style="text-align:right;"> Summary </th>
</tr>
</thead>
<tbody>
<tr>
<td> <a href="http://apps2.leg.wa.gov/billsummary?BillNumber=5598&amp;Year=2017">5598</a> </td>
<td> 40 </td>
<td style="text-align:right;"> Granting relatives, including but not limited to grandparents, the right to seek visitation with a child through the courts.</td>
</tr>
<tr>
<td> <a href="http://apps2.leg.wa.gov/billsummary?BillNumber=6037&amp;Year=2017">6037</a> </td>
<td> 28 </td>
<td style="text-align:right;"> Concerning the uniform parentage act.</td>
</tr>
<tr>
<td> <a href="http://apps2.leg.wa.gov/billsummary?BillNumber=5375&amp;Year=2017">5375</a> </td>
<td> 27 </td>
<td style="text-align:right;"> Renaming the cancer research endowment authority to the Andy Hill cancer research endowment.</td>
</tr>
</tbody>
</table>


<p>And in the House:</p>

<table>
<thead>
<tr>
<th>Bill </th>
<th> Sponsor count </th>
<th style="text-align:right;"> Summary </th>
</tr>
</thead>
<tbody>
<tr>
<td> <a href="http://apps2.leg.wa.gov/billsummary?BillNumber=2282&amp;Year=2017">2282</a> </td>
<td> 52 </td>
<td style="text-align:right;"> Protecting an open internet in Washington state.</td>
</tr>
<tr>
<td> <a href="http://apps2.leg.wa.gov/billsummary?BillNumber=1714&amp;Year=2017">1714</a> </td>
<td> 45 </td>
<td style="text-align:right;"> Concerning nursing staffing practices at hospitals.</td>
</tr>
<tr>
<td> <a href="http://apps2.leg.wa.gov/billsummary?BillNumber=1400&amp;Year=2017">1400</a> </td>
<td> 42 </td>
<td style="text-align:right;"> Creating Washington state aviation special license plates.</td>
</tr>
</tbody>
</table>


<p>In November 2017, Manka Dhingra won a special election and the
Washington State Senate
flipped from Republican held to Democrat held. Initially I wanted to
focus on the number of bills passed by a Republican held Senate versus a
Democrat held Senate but there were too many extraneous variables such
as passing a budget and a shorter session in 2018. Instead, I decided to
focus on the number of Yea votes by bill</p>

<p><img src ="http://zachstednick.com/percentage_yea_votes.png"></p>

<p>Many of the bills passed were with almost overwhelming support, which is
refreshing to see that there is quite a bit of bipartisanship in
Washington State in 2018.</p>

<p>As always, analysis code on
<a href="https://github.com/stedy/blog-supplemental">GitHub</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Visualizing the 2017-18 WA State Legislature]]></title>
    <link href="http://stedy.github.io/blog/2018/04/05/visualizing-the-2017-18-wa-state-legislature/"/>
    <updated>2018-04-05T11:25:23-07:00</updated>
    <id>http://stedy.github.io/blog/2018/04/05/visualizing-the-2017-18-wa-state-legislature</id>
    <content type="html"><![CDATA[<p>In his <a href="https://www.governor.wa.gov/news-media/news-media/speeches/2018-state-state">2018 State of the State
speech</a>,
Washington State Governor Jay Inslee
made a passioned appeal for a carbon tax and proposed one in Washington
State Senate bill
<a href="http://apps2.leg.wa.gov/billsummary?BillNumber=6203&amp;Year=2017">6203</a>.
Because of this, I paid more attention to the
activities of the Washington State Legislature than I ever had before
and I found it fascinating.</p>

<p>First off, lets start with the website for the state Legislature. Here
is a screenshot of the
Washington State Legislature page for SB 6203 which is the bill I was
most interested in:</p>

<p><img src="http://zachstednick.com/WA_6203.png"></p>

<p>The website is very resource dense and well worth time exploring when
the Legislature is in session. Every piece of proposed legislation
shows the same amount of information and allows you to easily find and
contact your legislators about a particular bill if interested. The site
also has livestreams of <a href="http://apps2.leg.wa.gov/billsummary?BillNumber=6203&amp;Year=2017#videoSection">committee
hearings</a>
and displays <a href="https://app.leg.wa.gov/far/Senate/Calendar">vote
counts</a> on bills in almost
real time as the votes are tallied on both the
Senate and the House floor.</p>

<p>Is Washington State unique in this regard? Of course not, here is a
screen shot for an interesting bill in Legislature for the State of
California.</p>

<p><img src="http://zachstednick.com/CA_827.png"></p>

<p>Finally here is a screenshot of a House bill on the United States
Congress website</p>

<p><img src="http://zachstednick.com/HR_5031.png"></p>

<p>Does ease of use of the website increase participation in the civic
process at the state level? That is a difficult question to answer but
personally I am glad I get to use the Washington State one instead of
the California State Legislature webpage.</p>

<p>The 2017-18 Washington State Legislative Session ended on <a href="http://leg.wa.gov/legislature/Documents/2018CutoffCalendar.pdf">March 8,
2018</a>
and
Governor Inslee then had 21 days to sign bills into law or veto them.</p>

<p>The conclusion of the 2017-18 Session made me wonder what happened to
those bills that were introduced and how many of them actually became
law.
In addition to a great website, the Washington State
Legislature also has an excellent set of <a href="http://wslwebservices.leg.wa.gov/">Web
Services</a> that allow for
programmatically
capturing metrics and data about activities in the state
legislation. One way to easily visualize this is with a <a href="https://en.wikipedia.org/wiki/Sankey_diagram">Sankey
Diagram</a> (no relation to
this <a href="https://www.youtube.com/watch?v=6Iec080MWCs">Sankey</a> though).</p>

<iframe src="http://zachstednick.com/WA_leg_sankey.html" marginwidth="0" marginheight="0" style="height:600px; width:1060px;" scrolling="no"></iframe>


<p>Here is a smaller image of the diagram with a larger version
<a href="https://zachstednick.com/WA_leg_sankey.html">here</a></p>

<p>Code to generate this figure available on my <a href="https://github.com/stedy/blog-supplemental/">GitHub repo</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Has the Pac-12 Network decreased UW home football game attendance UPDATED]]></title>
    <link href="http://stedy.github.io/blog/2017/11/30/has-the-pac-12-network-decreased-uw-home-football-game-attendance-updated/"/>
    <updated>2017-11-30T13:10:16-08:00</updated>
    <id>http://stedy.github.io/blog/2017/11/30/has-the-pac-12-network-decreased-uw-home-football-game-attendance-updated</id>
    <content type="html"><![CDATA[<p>Following up on my earlier
<a href="http://zachstednick.name/blog/2016/09/02/has-the-pac-12-network-decreased-uw-home-football-game-attendance/">post</a>,
how much has the Pac-12 Network affected game attendance? I updated my previous data set to include the past two seasons so as to include 2008-2017 data. I relied on home game
attendance as reported by Wikipedia and also used Wikipedia to determine what TV network broadcast each home game. In an ideal world I would be able to make better comparisons using the <a href="http://www.nielsen.com/us/en/solutions/measurement/television.html">Nielsen rating</a>
for each game however my guess is that data does not come as cheap or as
easily as data from Wikipedia. For the purposes of this analysis I am
neglecting various other factors in this anaysis
such as time at kickoff, game day temperature, opponent, ranking of UW,
ranking of opponent, etc&hellip; the list goes on and on. My main intention
was
to simply show home game attendance versus TV network for all games:</p>

<p><img src="http://zachstednick.com/UW_football_attendance_by_TV_2008-17.png"></p>

<p>And attendance for Pac-12 only opponents versus TV network:</p>

<p><img src="http://zachstednick.com/UW_football_attendance_by_TV_2008-17_Pac12_only.png"></p>

<p>Based on the available data it appears that attendance during home games
has
been influenced and possibly decreased by the Pac-12 Network but it is
difficult to say for sure while ignoring so many external factors. With
a significant budget deficit still a major
<a href="https://www.seattletimes.com/seattle-news/education/uw-regents-assail-ex-athletic-director-over-growing-deficit/">issue</a>,
one can only hope that losses from game day ticket sales are made up for
with Pac-12 Network advertising revenue.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[States with multiple football teams in the AP Top 25]]></title>
    <link href="http://stedy.github.io/blog/2017/10/07/states-with-multiple-football-teams-in-the-ap-top-25/"/>
    <updated>2017-10-07T22:24:34-07:00</updated>
    <id>http://stedy.github.io/blog/2017/10/07/states-with-multiple-football-teams-in-the-ap-top-25</id>
    <content type="html"><![CDATA[<p>With <a href="http://www.espn.com/college-football/game?gameId=400935292">WSU beating
Oregon</a> and
<a href="http://www.espn.com/college-football/game?gameId=400935293">UW beating UC
Berkeley</a>,
the State of Washington is poised to have two football teams in the top
ten of NCAA Division I football rankings. Naturally this got me
thinking, how often does this happen and how many states have had this
same achievement?</p>

<p>To answer this I used the weekly results of <a href="https://en.wikipedia.org/wiki/AP_Poll">Associated Press
poll</a> which started in 1936 and
thanks to our good friends at <a href="https://donate.wikimedia.org">Wikipedia</a>,
I was able to get AP Poll results for every week.</p>

<p>I found that 25 states had at least one week where two teams from that
state were in the AP Poll. However, the more I thought about it the more
I realized this was slightly biased because some states might only have
one team (i.e. Wyoming) while other states might have two Division I
teams that are never both great at the same time (i.e. Montana). I
tightened down my restrictions a bit and only looked at the top 10 teams
from each AP Poll.</p>

<p>Surprisingly, of the 25 states with at least two teams in the AP top 25
Poll,
21 of those states had a week with at least two teams from that state in the AP top 10.
I made a summary table with the most recent year each state achieved
this distinction listed:</p>

<table>
<thead>
<tr>
<th>state </th>
<th style="text-align:right;"> year </th>
</tr>
</thead>
<tbody>
<tr>
<td>Louisiana</td>
<td style="text-align:right;"><a href="https://en.wikipedia.org/wiki/1936_NCAA_football_rankings">1936</a></td>
</tr>
<tr>
<td>Maryland</td>
<td style="text-align:right;"><a href="https://en.wikipedia.org/wiki/1955_NCAA_football_rankings">1955</a></td>
</tr>
<tr>
<td>North Carolina</td>
<td style="text-align:right;"><a href="https://en.wikipedia.org/wiki/1957_NCAA_University_Division_football_rankings">1957</a></td>
</tr>
<tr>
<td>New York</td>
<td style="text-align:right;"><a href="https://en.wikipedia.org/wiki/1958_NCAA_University_Division_football_rankings">1958</a></td>
</tr>
<tr>
<td>Illinois</td>
<td style="text-align:right;"><a href="https://en.wikipedia.org/wiki/1963_NCAA_University_Division_football_rankings">1963</a></td>
</tr>
<tr>
<td>Indiana</td>
<td style="text-align:right;"><a href="https://en.wikipedia.org/wiki/1979_NCAA_Division_I-A_football_rankings">1979</a></td>
</tr>
<tr>
<td>Pennsylvania</td>
<td style="text-align:right;"><a href="https://en.wikipedia.org/wiki/1982_NCAA_Division_I-A_football_rankings">1982</a></td>
</tr>
<tr>
<td>Colorado</td>
<td style="text-align:right;"><a href="https://en.wikipedia.org/wiki/1994_NCAA_Division_I-A_football_rankings">1994</a></td>
</tr>
<tr>
<td>Kansas</td>
<td style="text-align:right;"><a href="https://en.wikipedia.org/wiki/1995_NCAA_Division_I-A_football_rankings">1995</a></td>
</tr>
<tr>
<td>Washington</td>
<td style="text-align:right;"><a href="https://en.wikipedia.org/wiki/1997_NCAA_Division_I-A_football_rankings">1997</a></td>
</tr>
<tr>
<td>Ohio</td>
<td style="text-align:right;"><a href="https://en.wikipedia.org/wiki/2009_NCAA_Division_I_FBS_football_rankings">2009</a></td>
</tr>
<tr>
<td>Oregon</td>
<td style="text-align:right;"><a href="https://en.wikipedia.org/wiki/2012_NCAA_Division_I_FBS_football_rankings">2012</a></td>
</tr>
<tr>
<td>Florida</td>
<td style="text-align:right;"><a href="https://en.wikipedia.org/wiki/2013_NCAA_Division_I_FBS_football_rankings">2013</a></td>
</tr>
<tr>
<td>South Carolina</td>
<td style="text-align:right;"><a href="https://en.wikipedia.org/wiki/2013_NCAA_Division_I_FBS_football_rankings">2013</a></td>
</tr>
<tr>
<td>Georgia</td>
<td style="text-align:right;"><a href="https://en.wikipedia.org/wiki/2014_NCAA_Division_I_FBS_football_rankings">2014</a></td>
</tr>
<tr>
<td>Mississippi</td>
<td style="text-align:right;"><a href="https://en.wikipedia.org/wiki/2014_NCAA_Division_I_FBS_football_rankings">2014</a></td>
</tr>
<tr>
<td>California</td>
<td style="text-align:right;"><a href="https://en.wikipedia.org/wiki/2015_NCAA_Division_I_FBS_football_rankings">2015</a></td>
</tr>
<tr>
<td>Alabama</td>
<td style="text-align:right;"><a href="https://en.wikipedia.org/wiki/2016_NCAA_Division_I_FBS_football_rankings">2016</a></td>
</tr>
<tr>
<td>Michigan</td>
<td style="text-align:right;"><a href="https://en.wikipedia.org/wiki/2016_NCAA_Division_I_FBS_football_rankings">2016</a></td>
</tr>
<tr>
<td>Texas</td>
<td style="text-align:right;"><a href="https://en.wikipedia.org/wiki/2016_NCAA_Division_I_FBS_football_rankings">2016</a></td>
</tr>
<tr>
<td>Oklahoma</td>
<td style="text-align:right;"><a href="https://en.wikipedia.org/wiki/2017_NCAA_Division_I_FBS_football_rankings">2017</a></td>
</tr>
</tbody>
</table>


<p>Then, I thought what if there were ever a week when a state had 3 teams
in the AP top 10. Sure enough, four states have achieved this:</p>

<table>
<thead>
<tr>
<th>state </th>
<th style="text-align:right;"> year </th>
</tr>
</thead>
<tbody>
<tr>
<td>California</td>
<td style="text-align:right;"><a href="https://en.wikipedia.org/wiki/1952_NCAA_football_rankings">1952</a></td>
</tr>
<tr>
<td>Indiana</td>
<td style="text-align:right;"><a href="https://en.wikipedia.org/wiki/1967_NCAA_University_Division_football_rankings">1967</a></td>
</tr>
<tr>
<td>Florida</td>
<td style="text-align:right;"><a href="https://en.wikipedia.org/wiki/2005_NCAA_Division_I-A_football_rankings">2005</a></td>
</tr>
<tr>
<td>Texas</td>
<td style="text-align:right;"><a href="https://en.wikipedia.org/wiki/2015_NCAA_Division_I_FBS_football_rankings">2015</a></td>
</tr>
</tbody>
</table>


<p>As always, all of my code for this is on
<a href="https://github.com/stedy/blog-supplemental/">GitHub</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Further exploration of IMDb TV show rating data]]></title>
    <link href="http://stedy.github.io/blog/2017/09/30/further-exploration-of-imdb-tv-show-rating-data/"/>
    <updated>2017-09-30T13:57:18-07:00</updated>
    <id>http://stedy.github.io/blog/2017/09/30/further-exploration-of-imdb-tv-show-rating-data</id>
    <content type="html"><![CDATA[<p>I wanted to revist my previous
<a href="http://zachstednick.name/blog/2017/08/09/smarter-binge-watching-with-linear-regression/">post</a>
continuing to look at using linear
regression for determining the best
episodes of a TV show to watch. I started to think about how to look at
this data for multiple TV shows. Performing a linear regression on show
rating by episode number within a season quickly allows us to determine
the maximum and minimum residual for all the show episodes. I took this
a step further and
calculated which episode of the show it was. For example, here are all
the episodes with residual value for that particular show <a href="http://www.imdb.com/title/tt4635276/">Master of
None</a>:</p>

<table>
<thead>
<tr>
<th>Season</th>
<th style="text-align:center;">Episode</th>
<th style="text-align:center;">Name</th>
<th style="text-align:center;">Residual</th>
<th style="text-align:center;">count</th>
<th style="text-align:right;">appearance</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td style="text-align:center;">1</td>
<td style="text-align:center;">               Plan B</td>
<td style="text-align:center;">-0.28</td>
<td style="text-align:center;">1</td>
<td style="text-align:right;">0.05</td>
</tr>
<tr>
<td>1</td>
<td style="text-align:center;">2</td>
<td style="text-align:center;">              Parents</td>
<td style="text-align:center;">0.21</td>
<td style="text-align:center;">2</td>
<td style="text-align:right;">0.1</td>
</tr>
<tr>
<td>1</td>
<td style="text-align:center;">3</td>
<td style="text-align:center;">           Hot Ticket</td>
<td style="text-align:center;">0.01</td>
<td style="text-align:center;">3</td>
<td style="text-align:right;">0.15</td>
</tr>
<tr>
<td>1</td>
<td style="text-align:center;">4</td>
<td style="text-align:center;">        Indians on TV</td>
<td style="text-align:center;">0.21</td>
<td style="text-align:center;">4</td>
<td style="text-align:right;">0.2</td>
</tr>
<tr>
<td>1</td>
<td style="text-align:center;">5</td>
<td style="text-align:center;">        The Other Man</td>
<td style="text-align:center;">-0.09</td>
<td style="text-align:center;">5</td>
<td style="text-align:right;">0.25</td>
</tr>
<tr>
<td>1</td>
<td style="text-align:center;">6</td>
<td style="text-align:center;">            Nashville</td>
<td style="text-align:center;">0.31</td>
<td style="text-align:center;">6</td>
<td style="text-align:right;">0.3</td>
</tr>
<tr>
<td>1</td>
<td style="text-align:center;">7</td>
<td style="text-align:center;"> Ladies and Gentlemen</td>
<td style="text-align:center;">-0.39</td>
<td style="text-align:center;">7</td>
<td style="text-align:right;">0.35</td>
</tr>
<tr>
<td>1</td>
<td style="text-align:center;">8</td>
<td style="text-align:center;">           Old People</td>
<td style="text-align:center;">-0.09</td>
<td style="text-align:center;">8</td>
<td style="text-align:right;">0.4</td>
</tr>
<tr>
<td>1</td>
<td style="text-align:center;">9</td>
<td style="text-align:center;">             Mornings</td>
<td style="text-align:center;">0.11</td>
<td style="text-align:center;">9</td>
<td style="text-align:right;">0.45</td>
</tr>
<tr>
<td>1</td>
<td style="text-align:center;">10</td>
<td style="text-align:center;">               Finale</td>
<td style="text-align:center;">0.01</td>
<td style="text-align:center;">10</td>
<td style="text-align:right;">0.5</td>
</tr>
<tr>
<td>2</td>
<td style="text-align:center;">1</td>
<td style="text-align:center;">            The Thief</td>
<td style="text-align:center;">0.44</td>
<td style="text-align:center;">11</td>
<td style="text-align:right;">0.55</td>
</tr>
<tr>
<td>2</td>
<td style="text-align:center;">2</td>
<td style="text-align:center;">             Le Nozze</td>
<td style="text-align:center;">-0.36</td>
<td style="text-align:center;">12</td>
<td style="text-align:right;">0.6</td>
</tr>
<tr>
<td>2</td>
<td style="text-align:center;">3</td>
<td style="text-align:center;">             Religion</td>
<td style="text-align:center;">-0.27</td>
<td style="text-align:center;">13</td>
<td style="text-align:right;">0.65</td>
</tr>
<tr>
<td>2</td>
<td style="text-align:center;">4</td>
<td style="text-align:center;">           First Date</td>
<td style="text-align:center;">0.13</td>
<td style="text-align:center;">14</td>
<td style="text-align:right;">0.7</td>
</tr>
<tr>
<td>2</td>
<td style="text-align:center;">5</td>
<td style="text-align:center;">     The Dinner Party</td>
<td style="text-align:center;">0.02</td>
<td style="text-align:center;">15</td>
<td style="text-align:right;">0.75</td>
</tr>
<tr>
<td>2</td>
<td style="text-align:center;">6</td>
<td style="text-align:center;"> New York, I Love You</td>
<td style="text-align:center;">0.42</td>
<td style="text-align:center;">16</td>
<td style="text-align:right;">0.8</td>
</tr>
<tr>
<td>2</td>
<td style="text-align:center;">7</td>
<td style="text-align:center;">              Door #3</td>
<td style="text-align:center;">-0.89</td>
<td style="text-align:center;">17</td>
<td style="text-align:right;">0.85</td>
</tr>
<tr>
<td>2</td>
<td style="text-align:center;">8</td>
<td style="text-align:center;">         Thanksgiving</td>
<td style="text-align:center;">0.31</td>
<td style="text-align:center;">18</td>
<td style="text-align:right;">0.9</td>
</tr>
<tr>
<td>2</td>
<td style="text-align:center;">9</td>
<td style="text-align:center;">         Amarsi Un Po</td>
<td style="text-align:center;">0.30</td>
<td style="text-align:center;">19</td>
<td style="text-align:right;">0.95</td>
</tr>
<tr>
<td>2</td>
<td style="text-align:center;">10</td>
<td style="text-align:center;">          Buona Notte</td>
<td style="text-align:center;">-0.10</td>
<td style="text-align:center;">20</td>
<td style="text-align:right;">1</td>
</tr>
</tbody>
</table>


<p>We can see that the episode with the highest residual is S2E1 &ldquo;The
Thief&rdquo; and the episode with the lowest residual is S2E7 &ldquo;Door #3&rdquo;. For
every TV show I took all the episodes and calculated their order as a percent of
the total number of episodes - for example the pilot episode would be 0.0 and the
series
finale would be 1.0 to generate an index. I then took the
maximum and minimum residual values for each show and plotted them
against that episode. For example here is a plot of just Master of None:</p>

<iframe src="http://zachstednick.com/mon_min_max.html" marginwidth="0" marginheight="0" style="height:500px; width:800px;" scrolling="no"></iframe>


<p>To obtain data on as many shows as I could I used this <a href="http://www.imdb.com/search/title?at=0&amp;num_votes=5000,&amp;sort=user_rating,de">IMDb
list</a>
of shows with over 5000 votes and selected the first 1200 shows as a
dataset. I then reused the <a href="http://www.omdbapi.com/">OMDb API</a> as I did before. I then calculated the same values as I did for Master of None
above and plotted them in a similar manner (use the mouseover for more
information on each point):</p>

<iframe src="http://zachstednick.com/imdb_min_max.html" marginwidth="0" marginheight="0" style="height:500px; width:800px;" scrolling="no"></iframe>


<p>Two things immediately jump out at me:</p>

<ol>
<li><p>The density of points right around the zero line shows that linear
regression is a pretty good metric to use for this type of analysis and
that most people rate the show generally in line with the overall trend
for that particular season.</p></li>
<li><p>There seems to be a tendancy for people to really love or really hate
the series finale of TV shows and this shows up by the sheer number of
points at 1. Possibly this is people expressing their overall view of
the show as a whole or maybe people really were really happy or unhappy
with the series finale.</p></li>
</ol>


<p>I put some of the main code I used in a <a href="https://github.com/stedy/blog-supplemental">GitHub
repository</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Smarter binge watching with linear regression]]></title>
    <link href="http://stedy.github.io/blog/2017/08/09/smarter-binge-watching-with-linear-regression/"/>
    <updated>2017-08-09T21:32:39-07:00</updated>
    <id>http://stedy.github.io/blog/2017/08/09/smarter-binge-watching-with-linear-regression</id>
    <content type="html"><![CDATA[<p>I am not much of a binge watcher but I do enjoy quality TV shows which
is why I think <a href="http://graphtv.kevinformatics.com/">GraphTV</a> is so
great. GraphTV plots the IMDb user ratings for every episode
and then performs a
<a href="https://en.wikipedia.org/wiki/Linear_regression">linear regression</a> of
the episode rating by the episode number to create a trend line which
helps you see if the show gets better or worse over the course of the
season.</p>

<p>This is nice but it can get difficult to use GraphTV for shows like
<a href="http://graphtv.kevinformatics.com/tt0088526">Golden Girls</a> and
downright impossible for shows like <a href="http://graphtv.kevinformatics.com/tt0096697">The
Simpsons</a>.</p>

<p>To solve this I created a GitHub repo
<a href="https://github.com/stedy/binge-trendy">binge-trendy</a>. Because the trend
line is fit to the IMDb user rating data, we are
interested in which episodes do IMDb users think are better than the
regression model predicts which translates to any deviation from the
trend line. Since I am only interested in episodes that are
rated higher than the regression model would have predicted,
I only look at episodes with a positive residual.</p>

<p>For example, Golden Girls season 4</p>

<table>
<thead>
<tr>
<th>   Season</th>
<th style="text-align:center;"> Episode </th>
<th style="text-align:right;">                                Name</th>
</tr>
</thead>
<tbody>
<tr>
<td>       4 </td>
<td style="text-align:center;">     1 </td>
<td style="text-align:right;">               Yes, We Have No Havanas</td>
</tr>
<tr>
<td>       4 </td>
<td style="text-align:center;">     2 </td>
<td style="text-align:right;">The Days and Nights of Sophia Petrillo</td>
</tr>
<tr>
<td>       4 </td>
<td style="text-align:center;">     6 </td>
<td style="text-align:right;">              Sophia&rsquo;s Wedding: Part 1</td>
</tr>
<tr>
<td>       4 </td>
<td style="text-align:center;">     9 </td>
<td style="text-align:right;">                       Scared Straight</td>
</tr>
<tr>
<td>      4  </td>
<td style="text-align:center;">   11  </td>
<td style="text-align:right;">                          The Auction</td>
</tr>
<tr>
<td>      4  </td>
<td style="text-align:center;">   14  </td>
<td style="text-align:right;">                       Love Me Tender</td>
</tr>
<tr>
<td>      4  </td>
<td style="text-align:center;">   15  </td>
<td style="text-align:right;">                      Valentine&rsquo;s Day</td>
</tr>
<tr>
<td>      4  </td>
<td style="text-align:center;">   19  </td>
<td style="text-align:right;">              Till Death Do We Volley</td>
</tr>
<tr>
<td>      4  </td>
<td style="text-align:center;">   20  </td>
<td style="text-align:right;">                         High Anxiety</td>
</tr>
<tr>
<td>      4  </td>
<td style="text-align:center;">   22  </td>
<td style="text-align:right;">                      Sophia&rsquo;s Choice</td>
</tr>
<tr>
<td>      4  </td>
<td style="text-align:center;">   23  </td>
<td style="text-align:right;">                      Rites of Spring</td>
</tr>
<tr>
<td>      4  </td>
<td style="text-align:center;">   24  </td>
<td style="text-align:right;">                     Foreign Exchange</td>
</tr>
</tbody>
</table>


<p>I realize the code is not great,
<a href="https://pypi.python.org/pypi/pylint">pylint</a> currently gives it a 6.05 but if there is
one thing I have learned in software:</p>

<blockquote class="twitter-tweet" data-lang="en"><p lang="en"
dir="ltr">The only way to write good code is to write tons of shitty
code first. Feeling shame about bad code stops you from getting to good
code</p>&mdash; Hadley Wickham (@hadleywickham) <a
href="https://twitter.com/hadleywickham/status/589068687669243905">April
17, 2015</a></blockquote>


<script async src="//platform.twitter.com/widgets.js"
charset="utf-8"></script>

]]></content>
  </entry>
  
</feed>
