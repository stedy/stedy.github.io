<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Zach Stednick]]></title>
  <link href="http://stedy.github.io/atom.xml" rel="self"/>
  <link href="http://stedy.github.io/"/>
  <updated>2018-04-05T23:28:00-07:00</updated>
  <id>http://stedy.github.io/</id>
  <author>
    <name><![CDATA[Zach Stednick]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Further analysis of the 2017-18 WA State Legislature]]></title>
    <link href="http://stedy.github.io/blog/2018/04/05/further-analysis-of-the-2017-18-wa-state-legislature/"/>
    <updated>2018-04-05T23:11:26-07:00</updated>
    <id>http://stedy.github.io/blog/2018/04/05/further-analysis-of-the-2017-18-wa-state-legislature</id>
    <content type="html"><![CDATA[<p>This is my second post looking at the data from the 2017-18 Washington
State Legislative Session. the first part of this blog can be
read
<a href="http://zachstednick.name/blog/2018/04/05/visualizing-the-2017-18-wa-state-legislature/">here</a></p>

<p>After some time looking at different bills that did pass, I started to
wonder if a bill
was more likely to pass if it had more sponsors. First I took the 647
bills passed by the Legislation and signed into law by Governor and
looked up how many co-sponsors each bill had:</p>

<p><img src="http://zachstednick.com/all_sponsor_counts.png"></p>

<p>Then I I took every bill that was introduced but did not become
law and counted up the sponsors for these:</p>

<p><img src="http://zachstednick.com/all_sponsor_counts_not_passed.png"></p>

<p>So it appears that the number of sponsors is not particulary predictive
for a bill becoming law. The three bills introduced in the Senate with
the highest number of Sponsors were:</p>

<p>|Bill | Number of Sponsors | Summary |
| &mdash;- | &mdash;&mdash;&mdash;&mdash;&mdash;&mdash; | &mdash;&mdash;: |
| <a href="http://apps2.leg.wa.gov/billsummary?BillNumber=5598&amp;Year=2017">5598</a> | 40 | Granting
relatives, including but not limited to grandparents, the right to seek
visitation with a child through the courts.
| <a href="http://apps2.leg.wa.gov/billsummary?BillNumber=6037&amp;Year=2017">6037</a> | 28 | Concerning
the uniform parentage act.
| <a href="http://apps2.leg.wa.gov/billsummary?BillNumber=5375&amp;Year=2017">5375</a> | 27 | Renaming
the cancer research endowment authority to the Andy Hill cancer research
endowment.</p>

<p>And in the House:</p>

<p>|Bill | Number of Sponsors | Summary |
| &mdash;- | &mdash;&mdash;&mdash;&mdash;&mdash;&mdash; | &mdash;&mdash;: |
| <a href="http://apps2.leg.wa.gov/billsummary?BillNumber=2282&amp;Year=2017">2282</a> | 52 | Protecting
an open internet in Washington state.
| <a href="http://apps2.leg.wa.gov/billsummary?BillNumber=1714&amp;Year=2017">1714</a> | 45 | Concerning
nursing staffing practices at hospitals.
| <a href="http://apps2.leg.wa.gov/billsummary?BillNumber=1400&amp;Year=2017">1400</a> | 42 | Creating
Washington state aviation special license plates.</p>

<p>In November 2017, Manka Dhingra won a special election and the
Washington State Senate
flipped from Republican held to Democrat held. Initially I wanted to
focus on the number of bills passed by a Republican held Senate versus a
Democrat held Senate but there were too many extraneous variables such
as passing a budget and a shorter session in 2018. Instead, I decided to
focus on the number of Yea votes by bill</p>

<p><img src ="http://zachstednick.com/percentage_yea_votes.png"></p>

<p>Many of the bills passed were with almost overwhelming support, which is
refreshing to see that there is quite a bit of bipartisanship in
Washington State in 2018.</p>

<p>As always, analysis code on
<a href="https://github.com/stedy/blog-supplemental">GitHub</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Visualizing the 2017-18 WA State Legislature]]></title>
    <link href="http://stedy.github.io/blog/2018/04/05/visualizing-the-2017-18-wa-state-legislature/"/>
    <updated>2018-04-05T11:25:23-07:00</updated>
    <id>http://stedy.github.io/blog/2018/04/05/visualizing-the-2017-18-wa-state-legislature</id>
    <content type="html"><![CDATA[<p>In his <a href="https://www.governor.wa.gov/news-media/news-media/speeches/2018-state-state">2018 State of the State
speech</a>,
Washington State Governor Jay Inslee
made a passioned appeal for a carbon tax and proposed one in Washington
State Senate bill
<a href="http://apps2.leg.wa.gov/billsummary?BillNumber=6203&amp;Year=2017">6203</a>.
Because of this, I paid more attention to the
activities of the Washington State Legislature than I ever had before
and I found it fascinating.</p>

<p>First off, lets start with the website for the state Legislature. Here
is a screenshot of the
Washington State Legislature page for SB 6203 which is the bill I was
most interested in:</p>

<p><img src="http://zachstednick.com/WA_6203.png"></p>

<p>The website is very resource dense and well worth time exploring when
the Legislature is in session. Every piece of proposed legislation
shows the same amount of information and allows you to easily find and
contact your legislators about a particular bill if interested. The site
also has livestreams of <a href="http://apps2.leg.wa.gov/billsummary?BillNumber=6203&amp;Year=2017#videoSection">committee
hearings</a>
and displays <a href="https://app.leg.wa.gov/far/Senate/Calendar">vote
counts</a> on bills in almost
real time as the votes are tallied on both the
Senate and the House floor.</p>

<p>Is Washington State unique in this regard? Of course not, here is a
screen shot for an interesting bill in Legislature for the State of
California.</p>

<p><img src="http://zachstednick.com/CA_827.png"></p>

<p>Finally here is a screenshot of a House bill on the United States
Congress website</p>

<p><img src="http://zachstednick.com/HR_5031.png"></p>

<p>Does ease of use of the website increase participation in the civic
process at the state level? That is a difficult question to answer but
personally I am glad I get to use the Washington State one instead of
the California State Legislature webpage.</p>

<p>The 2017-18 Washington State Legislative Session ended on <a href="http://leg.wa.gov/legislature/Documents/2018CutoffCalendar.pdf">March 8,
2018</a>
and
Governor Inslee then had 21 days to sign bills into law or veto them.</p>

<p>The conclusion of the 2017-18 Session made me wonder what happened to
those bills that were introduced and how many of them actually became
law.
In addition to a great website, the Washington State
Legislature also has an excellent set of <a href="http://wslwebservices.leg.wa.gov/">Web
Services</a> that allow for
programmatically
capturing metrics and data about activities in the state
legislation. One way to easily visualize this is with a <a href="https://en.wikipedia.org/wiki/Sankey_diagram">Sankey
Diagram</a> (no relation to
this <a href="https://www.youtube.com/watch?v=6Iec080MWCs">Sankey</a> though).</p>

<iframe src="http://zachstednick.com/WA_leg_sankey.html" marginwidth="0" marginheight="0" style="height:600px; width:1060px;" scrolling="no"></iframe>


<p>Here is a smaller image of the diagram with a larger version
<a href="https://zachstednick.com/WA_leg_sankey.html">here</a></p>

<p>Code to generate this figure available on my <a href="https://github.com/stedy/blog-supplemental/">GitHub repo</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Has the Pac-12 Network decreased UW home football game attendance UPDATED]]></title>
    <link href="http://stedy.github.io/blog/2017/11/30/has-the-pac-12-network-decreased-uw-home-football-game-attendance-updated/"/>
    <updated>2017-11-30T13:10:16-08:00</updated>
    <id>http://stedy.github.io/blog/2017/11/30/has-the-pac-12-network-decreased-uw-home-football-game-attendance-updated</id>
    <content type="html"><![CDATA[<p>Following up on my earlier
<a href="http://zachstednick.name/blog/2016/09/02/has-the-pac-12-network-decreased-uw-home-football-game-attendance/">post</a>,
how much has the Pac-12 Network affected game attendance? I updated my previous data set to include the past two seasons so as to include 2008-2017 data. I relied on home game
attendance as reported by Wikipedia and also used Wikipedia to determine what TV network broadcast each home game. In an ideal world I would be able to make better comparisons using the <a href="http://www.nielsen.com/us/en/solutions/measurement/television.html">Nielsen rating</a>
for each game however my guess is that data does not come as cheap or as
easily as data from Wikipedia. For the purposes of this analysis I am
neglecting various other factors in this anaysis
such as time at kickoff, game day temperature, opponent, ranking of UW,
ranking of opponent, etc&hellip; the list goes on and on. My main intention
was
to simply show home game attendance versus TV network for all games:</p>

<p><img src="http://zachstednick.com/UW_football_attendance_by_TV_2008-17.png"></p>

<p>And attendance for Pac-12 only opponents versus TV network:</p>

<p><img src="http://zachstednick.com/UW_football_attendance_by_TV_2008-17_Pac12_only.png"></p>

<p>Based on the available data it appears that attendance during home games
has
been influenced and possibly decreased by the Pac-12 Network but it is
difficult to say for sure while ignoring so many external factors. With
a significant budget deficit still a major
<a href="https://www.seattletimes.com/seattle-news/education/uw-regents-assail-ex-athletic-director-over-growing-deficit/">issue</a>,
one can only hope that losses from game day ticket sales are made up for
with Pac-12 Network advertising revenue.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[States with multiple football teams in the AP Top 25]]></title>
    <link href="http://stedy.github.io/blog/2017/10/07/states-with-multiple-football-teams-in-the-ap-top-25/"/>
    <updated>2017-10-07T22:24:34-07:00</updated>
    <id>http://stedy.github.io/blog/2017/10/07/states-with-multiple-football-teams-in-the-ap-top-25</id>
    <content type="html"><![CDATA[<p>With <a href="http://www.espn.com/college-football/game?gameId=400935292">WSU beating
Oregon</a> and
<a href="http://www.espn.com/college-football/game?gameId=400935293">UW beating UC
Berkeley</a>,
the State of Washington is poised to have two football teams in the top
ten of NCAA Division I football rankings. Naturally this got me
thinking, how often does this happen and how many states have had this
same achievement?</p>

<p>To answer this I used the weekly results of <a href="https://en.wikipedia.org/wiki/AP_Poll">Associated Press
poll</a> which started in 1936 and
thanks to our good friends at <a href="https://donate.wikimedia.org">Wikipedia</a>,
I was able to get AP Poll results for every week.</p>

<p>I found that 25 states had at least one week where two teams from that
state were in the AP Poll. However, the more I thought about it the more
I realized this was slightly biased because some states might only have
one team (i.e. Wyoming) while other states might have two Division I
teams that are never both great at the same time (i.e. Montana). I
tightened down my restrictions a bit and only looked at the top 10 teams
from each AP Poll.</p>

<p>Surprisingly, of the 25 states with at least two teams in the AP top 25
Poll,
21 of those states had a week with at least two teams from that state in the AP top 10.
I made a summary table with the most recent year each state achieved
this distinction listed:</p>

<table>
<thead>
<tr>
<th>state </th>
<th style="text-align:right;"> year </th>
</tr>
</thead>
<tbody>
<tr>
<td>Louisiana</td>
<td style="text-align:right;"><a href="https://en.wikipedia.org/wiki/1936_NCAA_football_rankings">1936</a></td>
</tr>
<tr>
<td>Maryland</td>
<td style="text-align:right;"><a href="https://en.wikipedia.org/wiki/1955_NCAA_football_rankings">1955</a></td>
</tr>
<tr>
<td>North Carolina</td>
<td style="text-align:right;"><a href="https://en.wikipedia.org/wiki/1957_NCAA_University_Division_football_rankings">1957</a></td>
</tr>
<tr>
<td>New York</td>
<td style="text-align:right;"><a href="https://en.wikipedia.org/wiki/1958_NCAA_University_Division_football_rankings">1958</a></td>
</tr>
<tr>
<td>Illinois</td>
<td style="text-align:right;"><a href="https://en.wikipedia.org/wiki/1963_NCAA_University_Division_football_rankings">1963</a></td>
</tr>
<tr>
<td>Indiana</td>
<td style="text-align:right;"><a href="https://en.wikipedia.org/wiki/1979_NCAA_Division_I-A_football_rankings">1979</a></td>
</tr>
<tr>
<td>Pennsylvania</td>
<td style="text-align:right;"><a href="https://en.wikipedia.org/wiki/1982_NCAA_Division_I-A_football_rankings">1982</a></td>
</tr>
<tr>
<td>Colorado</td>
<td style="text-align:right;"><a href="https://en.wikipedia.org/wiki/1994_NCAA_Division_I-A_football_rankings">1994</a></td>
</tr>
<tr>
<td>Kansas</td>
<td style="text-align:right;"><a href="https://en.wikipedia.org/wiki/1995_NCAA_Division_I-A_football_rankings">1995</a></td>
</tr>
<tr>
<td>Washington</td>
<td style="text-align:right;"><a href="https://en.wikipedia.org/wiki/1997_NCAA_Division_I-A_football_rankings">1997</a></td>
</tr>
<tr>
<td>Ohio</td>
<td style="text-align:right;"><a href="https://en.wikipedia.org/wiki/2009_NCAA_Division_I_FBS_football_rankings">2009</a></td>
</tr>
<tr>
<td>Oregon</td>
<td style="text-align:right;"><a href="https://en.wikipedia.org/wiki/2012_NCAA_Division_I_FBS_football_rankings">2012</a></td>
</tr>
<tr>
<td>Florida</td>
<td style="text-align:right;"><a href="https://en.wikipedia.org/wiki/2013_NCAA_Division_I_FBS_football_rankings">2013</a></td>
</tr>
<tr>
<td>South Carolina</td>
<td style="text-align:right;"><a href="https://en.wikipedia.org/wiki/2013_NCAA_Division_I_FBS_football_rankings">2013</a></td>
</tr>
<tr>
<td>Georgia</td>
<td style="text-align:right;"><a href="https://en.wikipedia.org/wiki/2014_NCAA_Division_I_FBS_football_rankings">2014</a></td>
</tr>
<tr>
<td>Mississippi</td>
<td style="text-align:right;"><a href="https://en.wikipedia.org/wiki/2014_NCAA_Division_I_FBS_football_rankings">2014</a></td>
</tr>
<tr>
<td>California</td>
<td style="text-align:right;"><a href="https://en.wikipedia.org/wiki/2015_NCAA_Division_I_FBS_football_rankings">2015</a></td>
</tr>
<tr>
<td>Alabama</td>
<td style="text-align:right;"><a href="https://en.wikipedia.org/wiki/2016_NCAA_Division_I_FBS_football_rankings">2016</a></td>
</tr>
<tr>
<td>Michigan</td>
<td style="text-align:right;"><a href="https://en.wikipedia.org/wiki/2016_NCAA_Division_I_FBS_football_rankings">2016</a></td>
</tr>
<tr>
<td>Texas</td>
<td style="text-align:right;"><a href="https://en.wikipedia.org/wiki/2016_NCAA_Division_I_FBS_football_rankings">2016</a></td>
</tr>
<tr>
<td>Oklahoma</td>
<td style="text-align:right;"><a href="https://en.wikipedia.org/wiki/2017_NCAA_Division_I_FBS_football_rankings">2017</a></td>
</tr>
</tbody>
</table>


<p>Then, I thought what if there were ever a week when a state had 3 teams
in the AP top 10. Sure enough, four states have achieved this:</p>

<table>
<thead>
<tr>
<th>state </th>
<th style="text-align:right;"> year </th>
</tr>
</thead>
<tbody>
<tr>
<td>California</td>
<td style="text-align:right;"><a href="https://en.wikipedia.org/wiki/1952_NCAA_football_rankings">1952</a></td>
</tr>
<tr>
<td>Indiana</td>
<td style="text-align:right;"><a href="https://en.wikipedia.org/wiki/1967_NCAA_University_Division_football_rankings">1967</a></td>
</tr>
<tr>
<td>Florida</td>
<td style="text-align:right;"><a href="https://en.wikipedia.org/wiki/2005_NCAA_Division_I-A_football_rankings">2005</a></td>
</tr>
<tr>
<td>Texas</td>
<td style="text-align:right;"><a href="https://en.wikipedia.org/wiki/2015_NCAA_Division_I_FBS_football_rankings">2015</a></td>
</tr>
</tbody>
</table>


<p>As always, all of my code for this is on
<a href="https://github.com/stedy/blog-supplemental/">GitHub</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Further exploration of IMDb TV show rating data]]></title>
    <link href="http://stedy.github.io/blog/2017/09/30/further-exploration-of-imdb-tv-show-rating-data/"/>
    <updated>2017-09-30T13:57:18-07:00</updated>
    <id>http://stedy.github.io/blog/2017/09/30/further-exploration-of-imdb-tv-show-rating-data</id>
    <content type="html"><![CDATA[<p>I wanted to revist my previous
<a href="http://zachstednick.name/blog/2017/08/09/smarter-binge-watching-with-linear-regression/">post</a>
continuing to look at using linear
regression for determining the best
episodes of a TV show to watch. I started to think about how to look at
this data for multiple TV shows. Performing a linear regression on show
rating by episode number within a season quickly allows us to determine
the maximum and minimum residual for all the show episodes. I took this
a step further and
calculated which episode of the show it was. For example, here are all
the episodes with residual value for that particular show <a href="http://www.imdb.com/title/tt4635276/">Master of
None</a>:</p>

<table>
<thead>
<tr>
<th>Season</th>
<th style="text-align:center;">Episode</th>
<th style="text-align:center;">Name</th>
<th style="text-align:center;">Residual</th>
<th style="text-align:center;">count</th>
<th style="text-align:right;">appearance</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td style="text-align:center;">1</td>
<td style="text-align:center;">               Plan B</td>
<td style="text-align:center;">-0.28</td>
<td style="text-align:center;">1</td>
<td style="text-align:right;">0.05</td>
</tr>
<tr>
<td>1</td>
<td style="text-align:center;">2</td>
<td style="text-align:center;">              Parents</td>
<td style="text-align:center;">0.21</td>
<td style="text-align:center;">2</td>
<td style="text-align:right;">0.1</td>
</tr>
<tr>
<td>1</td>
<td style="text-align:center;">3</td>
<td style="text-align:center;">           Hot Ticket</td>
<td style="text-align:center;">0.01</td>
<td style="text-align:center;">3</td>
<td style="text-align:right;">0.15</td>
</tr>
<tr>
<td>1</td>
<td style="text-align:center;">4</td>
<td style="text-align:center;">        Indians on TV</td>
<td style="text-align:center;">0.21</td>
<td style="text-align:center;">4</td>
<td style="text-align:right;">0.2</td>
</tr>
<tr>
<td>1</td>
<td style="text-align:center;">5</td>
<td style="text-align:center;">        The Other Man</td>
<td style="text-align:center;">-0.09</td>
<td style="text-align:center;">5</td>
<td style="text-align:right;">0.25</td>
</tr>
<tr>
<td>1</td>
<td style="text-align:center;">6</td>
<td style="text-align:center;">            Nashville</td>
<td style="text-align:center;">0.31</td>
<td style="text-align:center;">6</td>
<td style="text-align:right;">0.3</td>
</tr>
<tr>
<td>1</td>
<td style="text-align:center;">7</td>
<td style="text-align:center;"> Ladies and Gentlemen</td>
<td style="text-align:center;">-0.39</td>
<td style="text-align:center;">7</td>
<td style="text-align:right;">0.35</td>
</tr>
<tr>
<td>1</td>
<td style="text-align:center;">8</td>
<td style="text-align:center;">           Old People</td>
<td style="text-align:center;">-0.09</td>
<td style="text-align:center;">8</td>
<td style="text-align:right;">0.4</td>
</tr>
<tr>
<td>1</td>
<td style="text-align:center;">9</td>
<td style="text-align:center;">             Mornings</td>
<td style="text-align:center;">0.11</td>
<td style="text-align:center;">9</td>
<td style="text-align:right;">0.45</td>
</tr>
<tr>
<td>1</td>
<td style="text-align:center;">10</td>
<td style="text-align:center;">               Finale</td>
<td style="text-align:center;">0.01</td>
<td style="text-align:center;">10</td>
<td style="text-align:right;">0.5</td>
</tr>
<tr>
<td>2</td>
<td style="text-align:center;">1</td>
<td style="text-align:center;">            The Thief</td>
<td style="text-align:center;">0.44</td>
<td style="text-align:center;">11</td>
<td style="text-align:right;">0.55</td>
</tr>
<tr>
<td>2</td>
<td style="text-align:center;">2</td>
<td style="text-align:center;">             Le Nozze</td>
<td style="text-align:center;">-0.36</td>
<td style="text-align:center;">12</td>
<td style="text-align:right;">0.6</td>
</tr>
<tr>
<td>2</td>
<td style="text-align:center;">3</td>
<td style="text-align:center;">             Religion</td>
<td style="text-align:center;">-0.27</td>
<td style="text-align:center;">13</td>
<td style="text-align:right;">0.65</td>
</tr>
<tr>
<td>2</td>
<td style="text-align:center;">4</td>
<td style="text-align:center;">           First Date</td>
<td style="text-align:center;">0.13</td>
<td style="text-align:center;">14</td>
<td style="text-align:right;">0.7</td>
</tr>
<tr>
<td>2</td>
<td style="text-align:center;">5</td>
<td style="text-align:center;">     The Dinner Party</td>
<td style="text-align:center;">0.02</td>
<td style="text-align:center;">15</td>
<td style="text-align:right;">0.75</td>
</tr>
<tr>
<td>2</td>
<td style="text-align:center;">6</td>
<td style="text-align:center;"> New York, I Love You</td>
<td style="text-align:center;">0.42</td>
<td style="text-align:center;">16</td>
<td style="text-align:right;">0.8</td>
</tr>
<tr>
<td>2</td>
<td style="text-align:center;">7</td>
<td style="text-align:center;">              Door #3</td>
<td style="text-align:center;">-0.89</td>
<td style="text-align:center;">17</td>
<td style="text-align:right;">0.85</td>
</tr>
<tr>
<td>2</td>
<td style="text-align:center;">8</td>
<td style="text-align:center;">         Thanksgiving</td>
<td style="text-align:center;">0.31</td>
<td style="text-align:center;">18</td>
<td style="text-align:right;">0.9</td>
</tr>
<tr>
<td>2</td>
<td style="text-align:center;">9</td>
<td style="text-align:center;">         Amarsi Un Po</td>
<td style="text-align:center;">0.30</td>
<td style="text-align:center;">19</td>
<td style="text-align:right;">0.95</td>
</tr>
<tr>
<td>2</td>
<td style="text-align:center;">10</td>
<td style="text-align:center;">          Buona Notte</td>
<td style="text-align:center;">-0.10</td>
<td style="text-align:center;">20</td>
<td style="text-align:right;">1</td>
</tr>
</tbody>
</table>


<p>We can see that the episode with the highest residual is S2E1 &ldquo;The
Thief&rdquo; and the episode with the lowest residual is S2E7 &ldquo;Door #3&rdquo;. For
every TV show I took all the episodes and calculated their order as a percent of
the total number of episodes - for example the pilot episode would be 0.0 and the
series
finale would be 1.0 to generate an index. I then took the
maximum and minimum residual values for each show and plotted them
against that episode. For example here is a plot of just Master of None:</p>

<iframe src="http://zachstednick.com/mon_min_max.html" marginwidth="0" marginheight="0" style="height:500px; width:800px;" scrolling="no"></iframe>


<p>To obtain data on as many shows as I could I used this <a href="http://www.imdb.com/search/title?at=0&amp;num_votes=5000,&amp;sort=user_rating,de">IMDb
list</a>
of shows with over 5000 votes and selected the first 1200 shows as a
dataset. I then reused the <a href="http://www.omdbapi.com/">OMDb API</a> as I did before. I then calculated the same values as I did for Master of None
above and plotted them in a similar manner (use the mouseover for more
information on each point):</p>

<iframe src="http://zachstednick.com/imdb_min_max.html" marginwidth="0" marginheight="0" style="height:500px; width:800px;" scrolling="no"></iframe>


<p>Two things immediately jump out at me:</p>

<ol>
<li><p>The density of points right around the zero line shows that linear
regression is a pretty good metric to use for this type of analysis and
that most people rate the show generally in line with the overall trend
for that particular season.</p></li>
<li><p>There seems to be a tendancy for people to really love or really hate
the series finale of TV shows and this shows up by the sheer number of
points at 1. Possibly this is people expressing their overall view of
the show as a whole or maybe people really were really happy or unhappy
with the series finale.</p></li>
</ol>


<p>I put some of the main code I used in a <a href="https://github.com/stedy/blog-supplemental">GitHub
repository</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Smarter binge watching with linear regression]]></title>
    <link href="http://stedy.github.io/blog/2017/08/09/smarter-binge-watching-with-linear-regression/"/>
    <updated>2017-08-09T21:32:39-07:00</updated>
    <id>http://stedy.github.io/blog/2017/08/09/smarter-binge-watching-with-linear-regression</id>
    <content type="html"><![CDATA[<p>I am not much of a binge watcher but I do enjoy quality TV shows which
is why I think <a href="http://graphtv.kevinformatics.com/">GraphTV</a> is so
great. GraphTV plots the IMDb user ratings for every episode
and then performs a
<a href="https://en.wikipedia.org/wiki/Linear_regression">linear regression</a> of
the episode rating by the episode number to create a trend line which
helps you see if the show gets better or worse over the course of the
season.</p>

<p>This is nice but it can get difficult to use GraphTV for shows like
<a href="http://graphtv.kevinformatics.com/tt0088526">Golden Girls</a> and
downright impossible for shows like <a href="http://graphtv.kevinformatics.com/tt0096697">The
Simpsons</a>.</p>

<p>To solve this I created a GitHub repo
<a href="https://github.com/stedy/binge-trendy">binge-trendy</a>. Because the trend
line is fit to the IMDb user rating data, we are
interested in which episodes do IMDb users think are better than the
regression model predicts which translates to any deviation from the
trend line. Since I am only interested in episodes that are
rated higher than the regression model would have predicted,
I only look at episodes with a positive residual.</p>

<p>For example, Golden Girls season 4</p>

<table>
<thead>
<tr>
<th>   Season</th>
<th style="text-align:center;"> Episode </th>
<th style="text-align:right;">                                Name</th>
</tr>
</thead>
<tbody>
<tr>
<td>       4 </td>
<td style="text-align:center;">     1 </td>
<td style="text-align:right;">               Yes, We Have No Havanas</td>
</tr>
<tr>
<td>       4 </td>
<td style="text-align:center;">     2 </td>
<td style="text-align:right;">The Days and Nights of Sophia Petrillo</td>
</tr>
<tr>
<td>       4 </td>
<td style="text-align:center;">     6 </td>
<td style="text-align:right;">              Sophia&rsquo;s Wedding: Part 1</td>
</tr>
<tr>
<td>       4 </td>
<td style="text-align:center;">     9 </td>
<td style="text-align:right;">                       Scared Straight</td>
</tr>
<tr>
<td>      4  </td>
<td style="text-align:center;">   11  </td>
<td style="text-align:right;">                          The Auction</td>
</tr>
<tr>
<td>      4  </td>
<td style="text-align:center;">   14  </td>
<td style="text-align:right;">                       Love Me Tender</td>
</tr>
<tr>
<td>      4  </td>
<td style="text-align:center;">   15  </td>
<td style="text-align:right;">                      Valentine&rsquo;s Day</td>
</tr>
<tr>
<td>      4  </td>
<td style="text-align:center;">   19  </td>
<td style="text-align:right;">              Till Death Do We Volley</td>
</tr>
<tr>
<td>      4  </td>
<td style="text-align:center;">   20  </td>
<td style="text-align:right;">                         High Anxiety</td>
</tr>
<tr>
<td>      4  </td>
<td style="text-align:center;">   22  </td>
<td style="text-align:right;">                      Sophia&rsquo;s Choice</td>
</tr>
<tr>
<td>      4  </td>
<td style="text-align:center;">   23  </td>
<td style="text-align:right;">                      Rites of Spring</td>
</tr>
<tr>
<td>      4  </td>
<td style="text-align:center;">   24  </td>
<td style="text-align:right;">                     Foreign Exchange</td>
</tr>
</tbody>
</table>


<p>I realize the code is not great,
<a href="https://pypi.python.org/pypi/pylint">pylint</a> currently gives it a 6.05 but if there is
one thing I have learned in software:</p>

<blockquote class="twitter-tweet" data-lang="en"><p lang="en"
dir="ltr">The only way to write good code is to write tons of shitty
code first. Feeling shame about bad code stops you from getting to good
code</p>&mdash; Hadley Wickham (@hadleywickham) <a
href="https://twitter.com/hadleywickham/status/589068687669243905">April
17, 2015</a></blockquote>


<script async src="//platform.twitter.com/widgets.js"
charset="utf-8"></script>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Standing up for net neutrality]]></title>
    <link href="http://stedy.github.io/blog/2017/07/05/standing-up-for-net-neutrality/"/>
    <updated>2017-07-05T21:26:12-07:00</updated>
    <id>http://stedy.github.io/blog/2017/07/05/standing-up-for-net-neutrality</id>
    <content type="html"><![CDATA[<p>Currently there are many political issues that demand
attention however in my opinion there are none that would affect more
people than the possible destruction of net neutrality.</p>

<p><a href="https://en.wikipedia.org/wiki/Net_neutrality">Net neutrality</a> is simply the principle that all data on the internet
should be treated the same. It does not matter if you are visiting Fox
News or Mother Jones - the data and content from both of these
websites (as well as from every other website) should be treated as
equal and that data should be served equally by all Internet Service Providers. Losing net neutrality could
lead to an internet that favors
one of these two sites based on which site is willing to pay more. I
chose these sites because they are such polar opposites but at the same
time we live in a country that allows for such opposites to
have equal protection of freedom of speech. I may disagree with the content of a particular
website but
I do not think it should be served any differently than the website of a
site I do agree with. Destruction of net neutrality will lead to greater
influence wielded by larger corporations and could
stifle smaller websites and startups.</p>

<p>Fortunately, there is still time to act. On July 12, various online
communities and users will come together to stand tall and sound the
alarm about the FCC&rsquo;s attack on net neutrality. Join us
<a href="https://www.battleforthenet.com/july12">here</a>!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Pronto post-mortem]]></title>
    <link href="http://stedy.github.io/blog/2017/03/26/pronto-post-mortem/"/>
    <updated>2017-03-26T17:21:03-07:00</updated>
    <id>http://stedy.github.io/blog/2017/03/26/pronto-post-mortem</id>
    <content type="html"><![CDATA[<p>Pronto bike share ends this Friday March 31st and I will miss it for
sure. I <a href="http://zachstednick.name/blog/2016/02/09/thoughts-on-pronto/">wrote about why Pronto mattered to me</a> and I
even rode a Pronto bike in the 25
mile <a href="http://www.obliteride.org/">Obliteride</a> last year:</p>

<p><img src="http://zachstednick.com/IMG_2260.JPG"></p>

<p>In October 2015, there was a Pronto sponsored
<a href="https://www.prontocycleshare.com/datachallenge">contest</a> to visualize
bikeshare ride data. I created an
<a href="http://prontostories.com/">entry</a> which although did not win, was a
nice introduction for me to learn mapping with
<a href="d3js.org">D3</a>. As I was
checking out the Pronto site one last time today I noticed that they had
updated their publically available dataset to include 2016
ride data as well as the 2015 ride data.</p>

<p><img src="http://zachstednick.com/pronto_historical_ridership.png"></p>

<p>To me it seems that Pronto had a hard time expanding and encouraging
repeat riders. Unfortunately we do not have the membership data but if
we can assume that people who did not ride much in 2015 did not renew
their membership in 2016 then it looks pretty clear that Pronto was hurting
more than people thought. Also, it looked like they had good success in
2015 with getting people to buy day passes especially during peak tourist
season in the summer and were able to replicate that success in 2016. I
feel there is a need for a dedicated
bike share in Seattle however this iteration of bike share does not
appear to be the solution we need.</p>

<p>I went back to the Pronto site to fetch all my data because of an idea I
worked on then abandoned last summer. The idea was for a website that
was basically <a href="https://www.strava.com/">Strava</a> for Pronto
whereby you compared your ride time data to everyone else&rsquo;s and mapped out how
fast you were compared to them. Pronto did not make it easy to download
all your trip data so I ended up having to write a webscraper to get out
my own data (Hint, hint Pronto 2.0!) which I put at this <a href="https://github.com/stedy/pronto-scraper">GitHub
repository</a>. I never was able
to get my project past the personal level and my ultimate goal was to
simply make a map of the route and add
in plots. Here is an example of all trips from Fairview Ave N &amp; Ward St
to Westlake Ave &amp; 6th Ave:</p>

<iframe src="http://zachstednick.com/strava_for_pronto_demo.html" marginwidth="0" marginheight="0" style="height:700px; width:1260px;" scrolling="no"></iframe>


<p>I&rsquo;m sad that I can&rsquo;t work on this project anymore but maybe with Pronto
2.0 I will be able to revist this idea.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Have there been more upsets in the NCAA Tournament recently?]]></title>
    <link href="http://stedy.github.io/blog/2017/03/07/have-there-been-more-upsets-in-the-ncaa-tournament-recently/"/>
    <updated>2017-03-07T13:19:58-08:00</updated>
    <id>http://stedy.github.io/blog/2017/03/07/have-there-been-more-upsets-in-the-ncaa-tournament-recently</id>
    <content type="html"><![CDATA[<p>I have been following the <a href="https://en.wikipedia.org/wiki/NCAA_Division_I_Men%27s_Basketball_Tournament">NCAA Men&rsquo;s Basketball
Tournament</a>
for as long
as I can remember and with Selection Sunday coming up, I wondered if
there
have been more or less upsets in recent tournaments. To look at this
visually I used a hypothetical perfect bracket as a reference (i.e. #1
seed beats #16 seed, #2 seed beats #15 seed all the way to #1 seed beating a #2 seed). I
took the sum of all the winning seed numbers at each round in the
Regional Tournament and used that as the denominator
for comparison with the other Regional Tournaments for that particular
year.</p>

<p>I went back in time as far as I could but the <a href="https://en.wikipedia.org/wiki/2007_NCAA_Division_I_Men's_Basketball_Tournament">2007
Tournament</a>
finally harmonized the names of the Regional tournaments with the names
East, West, South, and
Midwest which made for easier comparison across years.</p>

<p><img src="http://zachstednick.com/NCAA_by_region.png"></p>

<p>Clearly there have been quite a lot of upsets in the past ten years
especially within the Midwest Region.</p>

<p>I then went back and looked at all games back to 1985 when
the Tournament first expanded to 64 teams. For this I did not have all
the Regional Tournament information so I just looked at all the games
(except the Final Four).</p>

<p><img src="http://zachstednick.com/NCAA_all_by_year.png"></p>

<p>The aggregate data is pretty volatile year over year as well with a low
in
<a href="https://en.wikipedia.org/wiki/2007_NCAA_Division_I_Men's_Basketball_Tournament">2007</a>. If anything, this shows we should be in for another great year of NCAA tournament basketball complete with some (hopefully many) exciting upsets.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A forgotten cron job leads to interesting results]]></title>
    <link href="http://stedy.github.io/blog/2017/03/02/a-forgotten-cron-job-leads-to-interesting-results/"/>
    <updated>2017-03-02T15:15:33-08:00</updated>
    <id>http://stedy.github.io/blog/2017/03/02/a-forgotten-cron-job-leads-to-interesting-results</id>
    <content type="html"><![CDATA[<p>On January 1, 2016 I set up a <a href="https://en.wikipedia.org/wiki/Cron">cron</a> job to perform a daily count of the number of Twitter followers of the two main Gubernatorial candidates in Washington State: <a href="https://twitter.com/GovInslee">Jay Inslee</a> and <a href="https://twitter.com/BillBryantWA">Bill Bryant</a>. I was not attempting to predict the election or do anything with the data, I just wanted to count followers until Election Day 2016 and hopefully plot some interesting results. I checked on Election Day and the trend lines remained pretty much the same as they did at the start of the year so I abandonded my idea. Today I was cleaning out my crontab file and I found that the cron job was still running. I added a solid line for the 2016 Election Day and dashed line for the 2017 Presidential Inauguration.</p>

<p><img src="http://zachstednick.com/Inslee_vs_Bryant.png"></p>

<p>To me, the follower count data after the inauguration is the most interesting but it is just count data and I am not sure how much you can really read into it. If anything, forgetting the cron job was a pleasant surprise that reminded me of this guy who took a screenshot of the New York Times frontpage everyday for 5 years <a href="https://www.youtube.com/watch?v=sCKGOiauJCE">YouTube</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[First book of the year]]></title>
    <link href="http://stedy.github.io/blog/2017/01/01/first-book-of-the-year/"/>
    <updated>2017-01-01T15:10:38-08:00</updated>
    <id>http://stedy.github.io/blog/2017/01/01/first-book-of-the-year</id>
    <content type="html"><![CDATA[<p>Last year I started off the year by making Ashlee Vance&rsquo;s
<a href="https://www.librarything.com/work/15743242/book/124695914">biography</a>
of Elon Musk the first book I read all year.
I wanted to start 2016 off better than 2015 and thought this book might
help my thinking. The story is on Musk is quite interesting if anything
to simply show how much he believes in himself even when the odds seem
stacked against him and the money in the bank grows lower. I tried to
use Musk&rsquo;s story to improve my own self-confidence and I felt like the most concrete way I
was able to do so was to reduce the amount I took on and instead focus
on doing a better job of what I had in front of me.</p>

<p>I will be repeating this little project in 2017 by starting the year off
by reading <a href="https://www.librarything.com/work/8586497">Spread Spectrum:Hedy Lamarr and
the mobile phone</a> by Rob
Walters. I know very little about <a href="https://en.wikipedia.org/wiki/Spread_spectrum">spread spectrum
technology</a> and <a href="https://en.wikipedia.org/wiki/Hedy_Lamarr">Hedy
Lamarr</a> had a very
interesting life and is greatly underappreciated in
modern society. Hopefully this book will prove to be as motivational
over the course of the year in a similar manner as Musk&rsquo;s biography was.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Election 2016]]></title>
    <link href="http://stedy.github.io/blog/2016/12/08/election-2016/"/>
    <updated>2016-12-08T14:30:57-08:00</updated>
    <id>http://stedy.github.io/blog/2016/12/08/election-2016</id>
    <content type="html"><![CDATA[<p>It has now been a month since the 2016 US Presidential election and I am
still stunned by the outcome but am ready to move on.</p>

<p>The major issues I focused on while voting at the Presidential level
were a better climate policy
and more equal treatment for minorities and other marginalized
populations. When I stop and think about why these were the major issues
for me, I realize that I am pretty fortunate. I have a great job,
generally feel safe, and am optimistic overall about the future and the
economy.</p>

<p>The biggest realization for me was that although I care deeply about
these issues on a national level, I need to be more involved at the
community level.</p>

<p>After thinking about it, there are three ways I want to get more
politically involved:</p>

<ol>
<li><p>Increase the amount of money I donate to specific organizations on a
recurring basis.</p></li>
<li><p>Get more involved with organizations that focus on climate advocacy
and immigrant populations. I did do some volunteer work with
<a href="https://carbonwa.org">CarbonWA</a> and I want to get more involved with
them as well as with an organization that focuses on immigrants such as
<a href="http://www.rewa.org/">ReWA</a></p></li>
<li><p>Write more letters to elected officials about the issues I am most
passionate about. I have helped make a github
<a href="https://github.com/openseattle/seattle-boundaries">repo</a> of all the
boundaries of my hometown and I have never used this for any reason
other than looking up addresses. At least I know where to look to figure
out the various districts I live in.</p></li>
</ol>


<p>Will these actions by me make a difference on the national level? Not
likely but hard to say. What they will do for sure is to make an impact
at the local level and will help me to improve the community around me.
If these issues are important
enough for me to write this post about, then it goes to show that they
are important enough for me to get more involved with.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Has the Pac-12 Network decreased UW home football game attendance?]]></title>
    <link href="http://stedy.github.io/blog/2016/09/02/has-the-pac-12-network-decreased-uw-home-football-game-attendance/"/>
    <updated>2016-09-02T12:52:46-07:00</updated>
    <id>http://stedy.github.io/blog/2016/09/02/has-the-pac-12-network-decreased-uw-home-football-game-attendance</id>
    <content type="html"><![CDATA[<p>The University of Washington Husky football team is taking on Rutgers
this Saturday with kickoff at 11 AM PST. This is awfully early to start a game, especially a game that occurs during Labor Day weekend. The game is being aired on the <a href="https://en.wikipedia.org/wiki/Pac-12_Network">Pac-12 Network</a> which is about to enter its fifth year of operation. This made me wonder, with the presence of the Pac-12 Network, has attendance decreased at home UW football games?</p>

<p>Fortunately, Wikipedia lists game attendance which allows for a quick
overview of UW home games stratified by network:</p>

<p><img src="http://zachstednick.com/UW_football_attendance_by_TV.png"></p>

<p>The purple dots are UW home games shown on the Pac-12 network, not
entirely convincing but at first glance they don&rsquo;t look too great for
the network. I then looked at only home Pac-10/Pac-12 games and looked at attendance
by season:</p>

<p><img src="http://zachstednick.com/UW_football_total_attendance.png"></p>

<p>That high point in this figure is UW versus Oregon in 2013 while that particularly low point in 2015 was versus Arizona on Halloween
which happened to fall on a Thursday in 2015. Why some executive at FS1 thought it would be a good idea to schedule a game then is beyond me.</p>

<p>In 2012, UW played its home games at <a href="https://en.wikipedia.org/wiki/CenturyLink_Field">CenturyLink
Field</a>, while <a href="https://en.wikipedia.org/wiki/Husky_Stadium">Husky
Stadium</a> was renovated. For the 2013 season the UW football team returned to play at a smaller Husky
Stadium, did either of these factors impact attendance?</p>

<p><img src="http://zachstednick.com/UW_football_percentage_capacity.png"></p>

<p>Not really, there is a minimal difference in stadium size which is
reflected in this identical-looking figure.</p>

<p>What Pac-12 opponents were the biggest draws on average?</p>

<table>
<thead>
<tr>
<th> Opponent </th>
<th style="text-align:right;"> Average Attendance </th>
</tr>
</thead>
<tbody>
<tr>
<td>          Oregon </td>
<td style="text-align:right;"> 69584</td>
</tr>
<tr>
<td>Washington State </td>
<td style="text-align:right;"> 68862</td>
</tr>
<tr>
<td>        Colorado </td>
<td style="text-align:right;"> 64373</td>
</tr>
<tr>
<td>             USC </td>
<td style="text-align:right;"> 64046</td>
</tr>
<tr>
<td>    Oregon State </td>
<td style="text-align:right;"> 63777</td>
</tr>
<tr>
<td>        Stanford </td>
<td style="text-align:right;"> 63360</td>
</tr>
<tr>
<td>            UCLA </td>
<td style="text-align:right;"> 62544</td>
</tr>
<tr>
<td>      California </td>
<td style="text-align:right;"> 62541</td>
</tr>
<tr>
<td>         Arizona </td>
<td style="text-align:right;"> 60756</td>
</tr>
</tbody>
</table>


<p>Obviously there are a lot of factors that can&rsquo;t be captured by game
attendance alone but with a significant budget deficit largely blamed on
<a href="http://www.seattletimes.com/sports/uw-huskies/uw-athletic-department-projects-budget-deficit-of-14-million-for-2016-fiscal-year/">reduced
attendance</a>, it seems like it might be time for the UW to analyze if the Pac-12 Network has really been worth the investments to date.</p>

<p>Full code for scraping Wikipedia on
<a href="https://github.com/stedy/blog-supplemental">GitHub</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[This American Life stats]]></title>
    <link href="http://stedy.github.io/blog/2016/08/16/this-american-life-stats/"/>
    <updated>2016-08-16T22:19:24-07:00</updated>
    <id>http://stedy.github.io/blog/2016/08/16/this-american-life-stats</id>
    <content type="html"><![CDATA[<p>Lately I have been listening to episodes of <a href="http://www.thisamericanlife.org/">This American
Life</a> faster than they are making them
which means I have been going back to the archive for past unheard shows. Their website has a nice <a href="http://www.thisamericanlife.org/user/login">user section</a> where you can log in and mark episodes you have heard and your favorites. The archives are arranged by year which naturally got me thinking about the number episodes I have listened to by year. A search of GitHub revealed many libraries for downloading episodes of the podcast but none that were interested in user statistics so I decided to write my own library. I am still very much beginner level with technologies such as passing cookies and CSRF requests which is why I ultimately ended up using <a href="https://splinter.readthedocs.io/en/latest/">Splinter</a> which just lets you automate browser actions. I used that to login and navigate the TAL archives by year. I then used <a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/">BeautifulSoup</a> to parse the HTML. Finally, I just wanted to visualize the results so I used Mike Bostock&rsquo;s <a href="https://bl.ocks.org/mbostock/3885304">D3 Bar Chart example</a>.</p>

<iframe src="http://zachstednick.com/TAL_stats.html" marginwidth="0" marginheight="0" style="height:500px; width:1060px;" scrolling="no"></iframe>


<p>Pretty basic but it gets the job done, full code here on
<a href="https://github.com/stedy/TAL_stats">GitHub</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Slopeplots of African GDP with ggplot2]]></title>
    <link href="http://stedy.github.io/blog/2016/06/11/slopeplots-of-african-gdp-with-ggplot2/"/>
    <updated>2016-06-11T20:03:34-07:00</updated>
    <id>http://stedy.github.io/blog/2016/06/11/slopeplots-of-african-gdp-with-ggplot2</id>
    <content type="html"><![CDATA[<p>I finally got around to reading <a href="https://www.librarything.com/work/13493349">Poor
Numbers</a> by <a href="http://mortenjerven.com/">Morton
Jerven</a> and found it really interesting. Basically Jerven argues that
academic literature has either &ldquo;neglected the issue of data quality and
therefore accepted the data at face value or dismissed the data as
unreliable and therefore irrelevant&rdquo; and this causes many issues with
the more data-driven approach to international aid in recent years.</p>

<p>The key table in this book was (in my opinion) a largely inscrutable table that showed
African economies ranked by per capita GDP with data from three
different sources of national income data: the <a href="http://data.worldbank.org/data-catalog/world-development-indicators">World Development
Indicators</a>,
<a href="http://www.ggdc.net/maddison/maddison-project/home.htm">Angus
Maddison</a>, and
<a href="http://www.rug.nl/research/ggdc/data/pwt/">Penn World Tables</a>. The
differences in the rankings is hard to parse in a table but  would
theoretically lend
themselves well to a slopegraph originally proposed by Edward Tufte in
<a href="https://www.librarything.com/work/7983">The Visual Display of Quantitative
Information</a></p>

<p><img src="http://charliepark.org/images/slopegraphs/slopegraph.gif"></p>

<p>Although not a true slopeplot, I was able to use a combination of
<code>geom_line</code> from ggplot2 and the
<a href="https://cran.r-project.org/web/packages/directlabels/">directlabels
package</a> to
generate the following plot (which I will admit is a bit of a hack):</p>

<p><img src="http://zachstednick.com/AfricanEconomiesbyGDP.png"></p>

<p>I was mainly interested in observing the variation in the top ten or so
countries which this plot handles well. The remaining 35 or so countries
are difficult to tell apart mostly due to very large differences in GDP.
A log transformed plot shows that
there is generally more consistency within the different rating agencies
but some variation between them.</p>

<p><img src="http://zachstednick.com/AfricanEconomiesbyGDP_logscaled.png"></p>

<p>Slopegraphs are an effective and efficient way to visualize this type of data which
is odd because I feel like they are rarely used and only barely
mentioned in Tufte&rsquo;s works. Hopefully more people being exposed to them
will result in further usage.</p>

<p>Data from Table 1.1 from Poor Numbers and full code available at this
<a href="https://gist.github.com/stedy/59fe24c6b5ab02a870d2584e4ce89ef9">gist</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Changes in NPS visits over time with D3]]></title>
    <link href="http://stedy.github.io/blog/2016/06/05/changes-in-nps-visits-over-time-with-d3/"/>
    <updated>2016-06-05T11:24:26-07:00</updated>
    <id>http://stedy.github.io/blog/2016/06/05/changes-in-nps-visits-over-time-with-d3</id>
    <content type="html"><![CDATA[<p>This year is the 100th anniversary of the National Park Service and I
was curious about how park attendance had changed recently. Andrew
Flowers of FiveThirtyEight had a nice
<a href="http://fivethirtyeight.com/features/the-national-parks-have-never-been-more-popular/">overview</a>
using official <a href="https://irma.nps.gov/Stats/Reports/National">NPS data</a>.
The article was interesting but contrary to most of FiveThirtyEight&rsquo;s
pieces there was a stunning lack of interactivity in what I felt was the
main figure of the piece</p>

<p><img src="http://i2.wp.com/espnfivethirtyeight.files.wordpress.com/2016/05/flowers-national-parks-4.png?quality=90&strip=all&w=575&ssl=1"></p>

<p>I remade the figure using that same data but just focusong on 2006 until
2015 and basically the popular parks stay popular and vice versa:</p>

<iframe src="http://zachstednick.com/rankNP.html" marginwidth="0" marginheight="0" style="height:500px; width:1060px;" scrolling="no"></iframe>


<p>Basically, Great Smoky Mountains NP has dominated attendance since its
creation but what other parks have recently become popular? I used
<a href="https://d3js.org/">D3</a> to make a simple line plot that allowed for
interactively exploring park attendance based on year over year change
from the mean attendance over the last ten years:</p>

<iframe src="http://zachstednick.com/deltaNP.html" marginwidth="0" marginheight="0" style="height:500px; width:1060px;" scrolling="no"></iframe>


<p>Clearly a spike in recent years - maybe due to lower gas prices or
increased popularity of social media?</p>

<p>Also, I was curious about what the trend looked like for the National
Monuments.</p>

<iframe src="http://zachstednick.com/deltaNM.html" marginwidth="0" marginheight="0" style="height:500px; width:1060px;" scrolling="no"></iframe>


<p>The most immediate trend that jumps out is the effect of the
<a href="https://en.wikipedia.org/wiki/Statue_of_Liberty#Closures_and_reopening_.282001.E2.80.93present.29">closing</a> of the <a href="https://www.nps.gov/stli/index.htm">Statue of Liberty</a> in 2011 which indirectly caused a slowdown in visits to <a href="https://www.nps.gov/cacl/index.htm">Castle Clinton</a>.</p>

<p>I aim to visit some new parks and monuments this summer and hopefully
looking at the data like this will help me avoid the crowds.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A National Parks tour with feather]]></title>
    <link href="http://stedy.github.io/blog/2016/04/16/a-national-parks-tour-with-feather/"/>
    <updated>2016-04-16T12:35:08-07:00</updated>
    <id>http://stedy.github.io/blog/2016/04/16/a-national-parks-tour-with-feather</id>
    <content type="html"><![CDATA[<p>This year is the 100th Anniversary of the National Park system and the
National Park Service is kicking things off with <a href="https://www.nps.gov/findapark/national-park-week.htm">National Park
Week</a> where every
National Park will be open for free. With all the savings, my next
thought was what would be the best way to visit all of them in a single
trip?</p>

<p>To calculate this, I used a variant of the <a href="https://en.wikipedia.org/wiki/Concorde_TSP_Solver">Concorde
algorithm</a> which is
an algorithm for solving the <a href="https://en.wikipedia.org/wiki/Traveling_salesman_problem">Traveling Salesman
Problem</a> or
what is the shortest route I can use to visit every National Park?</p>

<p>This also gave me an excellent opportunity to use
<a href="https://blog.rstudio.org/2016/03/29/feather/">Feather</a> a way of writing
dataframes to disk for interchanging between R and Python. I wanted to
use R largely due to Michael Hahsler&rsquo;s <a href="https://github.com/mhahsler/TSP">TSP
library</a> and I wanted to use python
because of the ease of use of the Google Maps API client. Finally, I
wanted to make a static map to show the route and I decided return to R
and use the <a href="https://github.com/dkahle/ggmap">ggmap library</a>.</p>

<p><img src="http://zachstednick.com/NP_tour.png"></p>

<p>I realize there are many ways to call R from Python and vice versa but I
wanted to try feather. As a first attempt, I was
pretty impressed with feather&rsquo;s ease of use. I did not have too large of
a dataset so I was unable to comment on the speed but simple reading and
writing in R and Python is made to feel very simple. The one issue I did run into was
more of a user issue and that it was challenging to rapidly flip back and forth
between the two languages as I iterated this code. The 0-index of Python versus
the 1-index of R is all handled by feather which is nice not to have to think
about.</p>

<p>As a whole, I highly recommend checking out feather and as for me, well
its time to hit the road and start visiting some National Parks and
according to Google Maps I only have 14832.8 miles to go.</p>

<p>My code for this lives
<a href="https://github.com/stedy/blog-supplemental">here</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Super Bowl Sunday at Chuck's]]></title>
    <link href="http://stedy.github.io/blog/2016/02/13/super-bowl-sunday-at-chucks/"/>
    <updated>2016-02-13T20:58:15-08:00</updated>
    <id>http://stedy.github.io/blog/2016/02/13/super-bowl-sunday-at-chucks</id>
    <content type="html"><![CDATA[<p>In the same vein as my previous post on <a href="http://zachstednick.name/blog/2016/01/31/weekend-at-chucks/">beer sales analysis</a> at <a href="http://chucks85th.com/">Chuck&rsquo;s Hop Shop</a>, I wanted to make a similar analysis but this time focus on Super Bowl Sunday sales. Similar to last time, I made a few assumptions:</p>

<ul>
<li>A keg is on tap until it is empty.</li>
<li>Each keg only serves pints of beer.</li>
<li>A pint is the only unit served (ie no 8 oz. pours).</li>
</ul>


<p>Anyways, here is a brief summary of beers on tap for the shortest amount
of
time on Sunday:</p>

<table>
<thead>
<tr>
<th style="text-align:left;">Brewery      </th>
<th style="text-align:left;">Beer     </th>
<th style="text-align:left;">Hours on tap </th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">Cloudburst    </td>
<td style="text-align:left;">Psycho Hose Beast&hellip;    </td>
<td style="text-align:left;">  0.25</td>
</tr>
<tr>
<td style="text-align:left;">Iron Fist     </td>
<td style="text-align:left;">Mint Chocolate Im&hellip;    </td>
<td style="text-align:left;">  1.00</td>
</tr>
<tr>
<td style="text-align:left;">Ballast Point </td>
<td style="text-align:left;">Watermelon Dorado&hellip;    </td>
<td style="text-align:left;">  1.00</td>
</tr>
<tr>
<td style="text-align:left;">Wander        </td>
<td style="text-align:left;">Wanderale, Belgia&hellip;    </td>
<td style="text-align:left;">  1.25</td>
</tr>
<tr>
<td style="text-align:left;">Sound         </td>
<td style="text-align:left;">Humonkulous IIIPA       </td>
<td style="text-align:left;">  1.25</td>
</tr>
<tr>
<td style="text-align:left;">Cloudburst    </td>
<td style="text-align:left;">Saison W/Grapefru&hellip;    </td>
<td style="text-align:left;">  1.25</td>
</tr>
<tr>
<td style="text-align:left;">Victory       </td>
<td style="text-align:left;">Prima Pilsner           </td>
<td style="text-align:left;">  1.75</td>
</tr>
<tr>
<td style="text-align:left;">Commons       </td>
<td style="text-align:left;">Holden Saison           </td>
<td style="text-align:left;">  2.00</td>
</tr>
<tr>
<td style="text-align:left;">Seattle Cider </td>
<td style="text-align:left;">Semi-Sweet Cider        </td>
<td style="text-align:left;">  3.00</td>
</tr>
<tr>
<td style="text-align:left;">Deschutes     </td>
<td style="text-align:left;">Abyss &lsquo;15  &frac12; Pint     </td>
<td style="text-align:left;">  3.00</td>
</tr>
</tbody>
</table>


<h1>Time on tap vs. ABV</h1>

<p>Did beer with higher ABV sell faster?</p>

<p><img src="http://zachstednick.com/SB_abv_count.png"></p>

<h1>Time on tap vs. cost</h1>

<p>Did more expensive beer sell faster?</p>

<p><img src="http://zachstednick.com/SB_cost_count.png"></p>

<h1>What does the relation between cost per pint and ABV look like?</h1>

<p><img src="http://zachstednick.com/SB_labeled.png"></p>

<p>Once again, all code lives <a href="https://github.com/stedy/chucks-beer-db">here</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Thoughts on Pronto]]></title>
    <link href="http://stedy.github.io/blog/2016/02/09/thoughts-on-pronto/"/>
    <updated>2016-02-09T17:21:05-08:00</updated>
    <id>http://stedy.github.io/blog/2016/02/09/thoughts-on-pronto</id>
    <content type="html"><![CDATA[<p><a href="http://www.prontocycleshare.com/">Pronto bikeshare</a> is in serious
financial trouble and may not make it
until the end of <a href="http://www.seattlebikeblog.com/2016/02/04/pronto-needs-city-buyout-before-end-of-march-how-did-we-get-here/">March of this
year</a>.
There has been a lot of talk about the future of Pronto and funding but for now I
just wanted to mention a few of the things I like
about Pronto.</p>

<ul>
<li><p>It is an excellent great for connecting mass transit and your final destination.
Waiting for a bus transfer and then taking said transfer sometimes
feels like it takes forever and
hopping on a Pronto bike can make the trip dramatically faster.</p></li>
<li><p>It provides an ideal solution for just going somewhere and not worrying about your bike.
Worried about locking up your bike in a certain area at a certain
time? If there is a Pronto station nearby that problem can be easily
fixed.</p></li>
<li><p>It is fun, full stop. The bikes are very sturdy and while they can feel
a bit slow I never feel like they will have physical problems or break
down. Yeah, I
realize I look like a dork while riding it but then again I run
my own blog, who am I to judge?</p></li>
<li><p>Finally, cars treat you as if you have never been on a bike before and
give you significant leeway. On my commuter bike I often get buzzed by
cars but on a Pronto I get treated like some tourist who has no
clue what they are doing. From a safety standopint, thats pretty tough
to beat.</p></li>
</ul>


<p>I think that Pronto had a terrible rollout (starting a bikeshare program
in October?) and as multiple people have
shown, it does not have the best station placement compared to a
similarly sized metropolitan area.</p>

<blockquote class="twitter-tweet" data-lang="en"><p lang="en"
dir="ltr">Seattle&#39;s <a
href="https://twitter.com/CyclePronto">@CyclePronto</a> and DC&#39;s <a
href="https://twitter.com/bikeshare">@bikeshare</a> at the same scale.
Maybe hills &amp; helmets aren&#39;t the problem? <a
href="https://twitter.com/hashtag/SEABikes?src=hash">#SEABikes</a> <a
href="https://t.co/at9HjJkDxX">pic.twitter.com/at9HjJkDxX</a></p>&mdash;
Jake Vanderplas (@jakevdp) <a
href="https://twitter.com/jakevdp/status/695393881710419970">February 4,
2016</a></blockquote>


<script async src="//platform.twitter.com/widgets.js"
charset="utf-8"></script>


<p> I have had multiple
problems with bike docks and the helmet locker can sometimes be unresponsive. But, even after all that, I
am long on Pronto and am constantly telling people about it. I had an
<a href="http://prontostories.com">entry</a> in the <a href="http://www.prontocycleshare.com/datachallenge">Pronto Data
Challenge</a>. I was even
planning on doing both the <a href="http://www.cascade.org/rides-major-rides/emerald-city-bike-ride">Emerald
City Bike
Ride</a>
and
<a href="http://obliteride.org/">Obliteride</a> on a Pronto bike just because I
thought it would be fun.</p>

<p>Seattle is growing,
really fast in fact, and giving people as many transit options as
possible will only make it that much easier for people to move around. I
really hope that Pronto lasts longer than the end of March and is
eventually able
to expand to cover more areas. As to whether the city should step in to
save it or not, we&rsquo;ll leave that
exercise to the reader.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Weekend at Chuck's]]></title>
    <link href="http://stedy.github.io/blog/2016/01/31/weekend-at-chucks/"/>
    <updated>2016-01-31T22:52:49-08:00</updated>
    <id>http://stedy.github.io/blog/2016/01/31/weekend-at-chucks</id>
    <content type="html"><![CDATA[<p><a href="http://chucks85th.com">Chuck&rsquo;s Hop Shop on 85th</a> is a beer store with
40 beers on tap and hundreds if not thousands of bottled beers for sale.
I have always been curious about what types of beers they go though
fastest and what are some of the more popular breweries.
Fortunately, they post their current tap list on their website which
allowed me to look at their data over the course of the weekend.</p>

<p>I scraped their website every 15 minutes from opening Friday,
January 29 - closing Sunday, January 30th. Their website also lists cost
per pint,
cost per growler, and <a href="https://en.wikipedia.org/wiki/ABV">ABV</a>.</p>

<p>For the purposes of this analysis, I made a few assumptions:</p>

<ul>
<li>A keg is on tap until it is empty.</li>
<li>Each keg only serves pints of beer.</li>
<li>A pint is the only unit served (ie no 8 oz. pours).</li>
</ul>


<p>With these assumptions in mind, my first question was which beer goes
fastest? The below table shows beers that were on tap for five hours or
less:</p>

<table>
<thead>
<tr>
<th>Brewery         </th>
<th>Beer                 </th>
<th style="text-align:right;">    Minutes on tap </th>
</tr>
</thead>
<tbody>
<tr>
<td> Bale Breaker    </td>
<td> Field 41 Pale        </td>
<td style="text-align:right;">  74.98333</td>
</tr>
<tr>
<td> Boneyard        </td>
<td> Hop Venom IIPA       </td>
<td style="text-align:right;">  74.98333</td>
</tr>
<tr>
<td> Almanac         </td>
<td> Elephant Heart de&hellip; </td>
<td style="text-align:right;"> 134.98333</td>
</tr>
<tr>
<td> Firestone       </td>
<td> Wookey Jack CDA      </td>
<td style="text-align:right;"> 224.98333</td>
</tr>
<tr>
<td> pFriem          </td>
<td> Imperial IPA         </td>
<td style="text-align:right;"> 240.00833</td>
</tr>
<tr>
<td> Sound           </td>
<td> Dubbel Entendre      </td>
<td style="text-align:right;"> 270.00000</td>
</tr>
<tr>
<td> Bale Breaker    </td>
<td> Top Cutter IPA       </td>
<td style="text-align:right;"> 277.48333</td>
</tr>
<tr>
<td> Kulshan         </td>
<td> Bastard Kat IPA      </td>
<td style="text-align:right;"> 284.98333</td>
</tr>
<tr>
<td> Roslyn          </td>
<td> Brookside Pale Lager </td>
<td style="text-align:right;"> 284.98333</td>
</tr>
<tr>
<td> Breakside       </td>
<td> Vienna Coffee OG &hellip; </td>
<td style="text-align:right;"> 299.86667</td>
</tr>
<tr>
<td> Bale Breaker    </td>
<td> High Camp Winter &hellip; </td>
<td style="text-align:right;"> 300.00833</td>
</tr>
</tbody>
</table>


<p>In a first place tie for a duration of a stunning <em>1 hour fifteen</em> minutes were
Bale Breaker Field 41 Pale and Hop Venom IIPA. There may be
another explanation but with respective BeerAdvocate scores of
<a href="http://www.beeradvocate.com/beer/profile/31389/93176/?ba=beertunes">89</a>
and <a href="http://www.beeradvocate.com/beer/profile/23066/72750/">97</a>, its
easy to see why they are so popular.</p>

<h1>Price of a pint vs. ABV</h1>

<p>Is there a correlation between the price of a pint of beer and the ABV?</p>

<p><img src="http://zachstednick.com/cost_vs_abv.png"></p>

<h1>Drinking based on ABV</h1>

<p>Do beers with higher ABV get ordered faster?</p>

<p><img src="http://zachstednick.com/min_on_tap_vs_abv.png"></p>

<h1>Are beers at Chuck&rsquo;s veblen goods?</h1>

<p>Do pricier beer move faster?</p>

<p><img src="http://zachstednick.com/min_on_tap_vs_price.png"></p>

<p>Obviously there are many more types of analysis one could look at with
this data. I do think this analysis was sorely lacking in first person
research, something I intend to fix in the next analysis. Full code on
<a href="https://github.com/stedy/chucks-beer-db">github</a>.</p>
]]></content>
  </entry>
  
</feed>
