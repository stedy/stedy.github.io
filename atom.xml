<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Zach Stednick]]></title>
  <link href="http://stedy.github.io/atom.xml" rel="self"/>
  <link href="http://stedy.github.io/"/>
  <updated>2016-06-06T22:36:04-07:00</updated>
  <id>http://stedy.github.io/</id>
  <author>
    <name><![CDATA[Zach Stednick]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Changes in NPS visits over time with D3]]></title>
    <link href="http://stedy.github.io/blog/2016/06/05/changes-in-nps-visits-over-time-with-d3/"/>
    <updated>2016-06-05T11:24:26-07:00</updated>
    <id>http://stedy.github.io/blog/2016/06/05/changes-in-nps-visits-over-time-with-d3</id>
    <content type="html"><![CDATA[<p>This year is the 100th anniversary of the National Park Service and I
was curious about how park attendance had changed recently. Andrew
Flowers of FiveThirtyEight had a nice
<a href="http://fivethirtyeight.com/features/the-national-parks-have-never-been-more-popular/">overview</a>
using official <a href="https://irma.nps.gov/Stats/Reports/National">NPS data</a>.
The article was interesting but contrary to most of FiveThirtyEight&rsquo;s
pieces there was a stunning lack of interactivity in what I felt was the
main figure of the piece</p>

<p><img src="http://i2.wp.com/espnfivethirtyeight.files.wordpress.com/2016/05/flowers-national-parks-4.png?quality=90&strip=all&w=575&ssl=1"></p>

<p>I remade the figure using that same data but just focusong on 2006 until
2015 and basically the popular parks stay popular and vice versa:</p>

<iframe src="http://zachstednick.com/rankNP.html" marginwidth="0" marginheight="0" style="height:500px; width:1060px;" scrolling="no"></iframe>


<p>Basically, Great Smoky Mountains NP has dominated attendance since its
creation but what other parks have recently become popular? I used
<a href="https://d3js.org/">D3</a> to make a simple line plot that allowed for
interactively exploring park attendance based on year over year change
from the mean attendance over the last ten years:</p>

<iframe src="http://zachstednick.com/deltaNP.html" marginwidth="0" marginheight="0" style="height:500px; width:1060px;" scrolling="no"></iframe>


<p>Clearly a spike in recent years - maybe due to lower gas prices or
increased popularity of social media?</p>

<p>Also, I was curious about what the trend looked like for the National
Monuments.</p>

<iframe src="http://zachstednick.com/deltaNM.html" marginwidth="0" marginheight="0" style="height:500px; width:1060px;" scrolling="no"></iframe>


<p>The most immediate trend that jumps out is the effect of the
<a href="https://en.wikipedia.org/wiki/Statue_of_Liberty#Closures_and_reopening_.282001.E2.80.93present.29">closing</a> of the <a href="https://www.nps.gov/stli/index.htm">Statue of Liberty</a> in 2011 which indirectly caused a slowdown in visits to <a href="https://www.nps.gov/cacl/index.htm">Castle Clinton</a>.</p>

<p>I aim to visit some new parks and monuments this summer and hopefully
looking at the data like this will help me avoid the crowds.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A National Parks tour with feather]]></title>
    <link href="http://stedy.github.io/blog/2016/04/16/a-national-parks-tour-with-feather/"/>
    <updated>2016-04-16T12:35:08-07:00</updated>
    <id>http://stedy.github.io/blog/2016/04/16/a-national-parks-tour-with-feather</id>
    <content type="html"><![CDATA[<p>This year is the 100th Anniversary of the National Park system and the
National Park Service is kicking things off with <a href="https://www.nps.gov/findapark/national-park-week.htm">National Park
Week</a> where every
National Park will be open for free. With all the savings, my next
thought was what would be the best way to visit all of them in a single
trip?</p>

<p>To calculate this, I used a variant of the <a href="https://en.wikipedia.org/wiki/Concorde_TSP_Solver">Concorde
algorithm</a> which is
an algorithm for solving the <a href="https://en.wikipedia.org/wiki/Traveling_salesman_problem">Traveling Salesman
Problem</a> or
what is the shortest route I can use to visit every National Park?</p>

<p>This also gave me an excellent opportunity to use
<a href="https://blog.rstudio.org/2016/03/29/feather/">Feather</a> a way of writing
dataframes to disk for interchanging between R and Python. I wanted to
use R largely due to Michael Hahsler&rsquo;s <a href="https://github.com/mhahsler/TSP">TSP
library</a> and I wanted to use python
because of the ease of use of the Google Maps API client. Finally, I
wanted to make a static map to show the route and I decided return to R
and use the <a href="https://github.com/dkahle/ggmap">ggmap library</a>.</p>

<p><img src="http://zachstednick.com/NP_tour.png"></p>

<p>I realize there are many ways to call R from Python and vice versa but I
wanted to try feather. As a first attempt, I was
pretty impressed with feather&rsquo;s ease of use. I did not have too large of
a dataset so I was unable to comment on the speed but simple reading and
writing in R and Python is made to feel very simple. The one issue I did run into was
more of a user issue and that it was challenging to rapidly flip back and forth
between the two languages as I iterated this code. The 0-index of Python versus
the 1-index of R is all handled by feather which is nice not to have to think
about.</p>

<p>As a whole, I highly recommend checking out feather and as for me, well
its time to hit the road and start visiting some National Parks and
according to Google Maps I only have 14832.8 miles to go.</p>

<p>My code for this lives
<a href="https://github.com/stedy/blog-supplemental">here</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Super Bowl Sunday at Chuck's]]></title>
    <link href="http://stedy.github.io/blog/2016/02/13/super-bowl-sunday-at-chucks/"/>
    <updated>2016-02-13T20:58:15-08:00</updated>
    <id>http://stedy.github.io/blog/2016/02/13/super-bowl-sunday-at-chucks</id>
    <content type="html"><![CDATA[<p>In the same vein as my previous post on <a href="http://zachstednick.name/blog/2016/01/31/weekend-at-chucks/">beer sales analysis</a> at <a href="http://chucks85th.com/">Chuck&rsquo;s Hop Shop</a>, I wanted to make a similar analysis but this time focus on Super Bowl Sunday sales. Similar to last time, I made a few assumptions:</p>

<ul>
<li>A keg is on tap until it is empty.</li>
<li>Each keg only serves pints of beer.</li>
<li>A pint is the only unit served (ie no 8 oz. pours).</li>
</ul>


<p>Anyways, here is a brief summary of beers on tap for the shortest amount
of
time on Sunday:</p>

<table>
<thead>
<tr>
<th style="text-align:left;">Brewery      </th>
<th style="text-align:left;">Beer     </th>
<th style="text-align:left;">Hours on tap </th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">Cloudburst    </td>
<td style="text-align:left;">Psycho Hose Beast&hellip;    </td>
<td style="text-align:left;">  0.25</td>
</tr>
<tr>
<td style="text-align:left;">Iron Fist     </td>
<td style="text-align:left;">Mint Chocolate Im&hellip;    </td>
<td style="text-align:left;">  1.00</td>
</tr>
<tr>
<td style="text-align:left;">Ballast Point </td>
<td style="text-align:left;">Watermelon Dorado&hellip;    </td>
<td style="text-align:left;">  1.00</td>
</tr>
<tr>
<td style="text-align:left;">Wander        </td>
<td style="text-align:left;">Wanderale, Belgia&hellip;    </td>
<td style="text-align:left;">  1.25</td>
</tr>
<tr>
<td style="text-align:left;">Sound         </td>
<td style="text-align:left;">Humonkulous IIIPA       </td>
<td style="text-align:left;">  1.25</td>
</tr>
<tr>
<td style="text-align:left;">Cloudburst    </td>
<td style="text-align:left;">Saison W/Grapefru&hellip;    </td>
<td style="text-align:left;">  1.25</td>
</tr>
<tr>
<td style="text-align:left;">Victory       </td>
<td style="text-align:left;">Prima Pilsner           </td>
<td style="text-align:left;">  1.75</td>
</tr>
<tr>
<td style="text-align:left;">Commons       </td>
<td style="text-align:left;">Holden Saison           </td>
<td style="text-align:left;">  2.00</td>
</tr>
<tr>
<td style="text-align:left;">Seattle Cider </td>
<td style="text-align:left;">Semi-Sweet Cider        </td>
<td style="text-align:left;">  3.00</td>
</tr>
<tr>
<td style="text-align:left;">Deschutes     </td>
<td style="text-align:left;">Abyss &lsquo;15  &frac12; Pint     </td>
<td style="text-align:left;">  3.00</td>
</tr>
</tbody>
</table>


<h1>Time on tap vs. ABV</h1>

<p>Did beer with higher ABV sell faster?</p>

<p><img src="http://zachstednick.com/SB_abv_count.png"></p>

<h1>Time on tap vs. cost</h1>

<p>Did more expensive beer sell faster?</p>

<p><img src="http://zachstednick.com/SB_cost_count.png"></p>

<h1>What does the relation between cost per pint and ABV look like?</h1>

<p><img src="http://zachstednick.com/SB_labeled.png"></p>

<p>Once again, all code lives <a href="https://github.com/stedy/chucks-beer-db">here</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Thoughts on Pronto]]></title>
    <link href="http://stedy.github.io/blog/2016/02/09/thoughts-on-pronto/"/>
    <updated>2016-02-09T17:21:05-08:00</updated>
    <id>http://stedy.github.io/blog/2016/02/09/thoughts-on-pronto</id>
    <content type="html"><![CDATA[<p><a href="http://www.prontocycleshare.com/">Pronto bikeshare</a> is in serious
financial trouble and may not make it
until the end of <a href="http://www.seattlebikeblog.com/2016/02/04/pronto-needs-city-buyout-before-end-of-march-how-did-we-get-here/">March of this
year</a>.
There has been a lot of talk about the future of Pronto and funding but for now I
just wanted to mention a few of the things I like
about Pronto.</p>

<ul>
<li><p>It is an excellent great for connecting mass transit and your final destination.
Waiting for a bus transfer and then taking said transfer sometimes
feels like it takes forever and
hopping on a Pronto bike can make the trip dramatically faster.</p></li>
<li><p>It provides an ideal solution for just going somewhere and not worrying about your bike.
Worried about locking up your bike in a certain area at a certain
time? If there is a Pronto station nearby that problem can be easily
fixed.</p></li>
<li><p>It is fun, full stop. The bikes are very sturdy and while they can feel
a bit slow I never feel like they will have physical problems or break
down. Yeah, I
realize I look like a dork while riding it but then again I run
my own blog, who am I to judge?</p></li>
<li><p>Finally, cars treat you as if you have never been on a bike before and
give you significant leeway. On my commuter bike I often get buzzed by
cars but on a Pronto I get treated like some tourist who has no
clue what they are doing. From a safety standopint, thats pretty tough
to beat.</p></li>
</ul>


<p>I think that Pronto had a terrible rollout (starting a bikeshare program
in October?) and as multiple people have
shown, it does not have the best station placement compared to a
similarly sized metropolitan area.</p>

<blockquote class="twitter-tweet" data-lang="en"><p lang="en"
dir="ltr">Seattle&#39;s <a
href="https://twitter.com/CyclePronto">@CyclePronto</a> and DC&#39;s <a
href="https://twitter.com/bikeshare">@bikeshare</a> at the same scale.
Maybe hills &amp; helmets aren&#39;t the problem? <a
href="https://twitter.com/hashtag/SEABikes?src=hash">#SEABikes</a> <a
href="https://t.co/at9HjJkDxX">pic.twitter.com/at9HjJkDxX</a></p>&mdash;
Jake Vanderplas (@jakevdp) <a
href="https://twitter.com/jakevdp/status/695393881710419970">February 4,
2016</a></blockquote>


<script async src="//platform.twitter.com/widgets.js"
charset="utf-8"></script>


<p> I have had multiple
problems with bike docks and the helmet locker can sometimes be unresponsive. But, even after all that, I
am long on Pronto and am constantly telling people about it. I had an
<a href="http://prontostories.com">entry</a> in the <a href="http://www.prontocycleshare.com/datachallenge">Pronto Data
Challenge</a>. I was even
planning on doing both the <a href="http://www.cascade.org/rides-major-rides/emerald-city-bike-ride">Emerald
City Bike
Ride</a>
and
<a href="http://obliteride.org/">Obliteride</a> on a Pronto bike just because I
thought it would be fun.</p>

<p>Seattle is growing,
really fast in fact, and giving people as many transit options as
possible will only make it that much easier for people to move around. I
really hope that Pronto lasts longer than the end of March and is
eventually able
to expand to cover more areas. As to whether the city should step in to
save it or not, we&rsquo;ll leave that
exercise to the reader.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Weekend at Chuck's]]></title>
    <link href="http://stedy.github.io/blog/2016/01/31/weekend-at-chucks/"/>
    <updated>2016-01-31T22:52:49-08:00</updated>
    <id>http://stedy.github.io/blog/2016/01/31/weekend-at-chucks</id>
    <content type="html"><![CDATA[<p><a href="http://chucks85th.com">Chuck&rsquo;s Hop Shop on 85th</a> is a beer store with
40 beers on tap and hundreds if not thousands of bottled beers for sale.
I have always been curious about what types of beers they go though
fastest and what are some of the more popular breweries.
Fortunately, they post their current tap list on their website which
allowed me to look at their data over the course of the weekend.</p>

<p>I scraped their website every 15 minutes from opening Friday,
January 29 - closing Sunday, January 30th. Their website also lists cost
per pint,
cost per growler, and <a href="https://en.wikipedia.org/wiki/ABV">ABV</a>.</p>

<p>For the purposes of this analysis, I made a few assumptions:</p>

<ul>
<li>A keg is on tap until it is empty.</li>
<li>Each keg only serves pints of beer.</li>
<li>A pint is the only unit served (ie no 8 oz. pours).</li>
</ul>


<p>With these assumptions in mind, my first question was which beer goes
fastest? The below table shows beers that were on tap for five hours or
less:</p>

<table>
<thead>
<tr>
<th>Brewery         </th>
<th>Beer                 </th>
<th style="text-align:right;">    Minutes on tap </th>
</tr>
</thead>
<tbody>
<tr>
<td> Bale Breaker    </td>
<td> Field 41 Pale        </td>
<td style="text-align:right;">  74.98333</td>
</tr>
<tr>
<td> Boneyard        </td>
<td> Hop Venom IIPA       </td>
<td style="text-align:right;">  74.98333</td>
</tr>
<tr>
<td> Almanac         </td>
<td> Elephant Heart de&hellip; </td>
<td style="text-align:right;"> 134.98333</td>
</tr>
<tr>
<td> Firestone       </td>
<td> Wookey Jack CDA      </td>
<td style="text-align:right;"> 224.98333</td>
</tr>
<tr>
<td> pFriem          </td>
<td> Imperial IPA         </td>
<td style="text-align:right;"> 240.00833</td>
</tr>
<tr>
<td> Sound           </td>
<td> Dubbel Entendre      </td>
<td style="text-align:right;"> 270.00000</td>
</tr>
<tr>
<td> Bale Breaker    </td>
<td> Top Cutter IPA       </td>
<td style="text-align:right;"> 277.48333</td>
</tr>
<tr>
<td> Kulshan         </td>
<td> Bastard Kat IPA      </td>
<td style="text-align:right;"> 284.98333</td>
</tr>
<tr>
<td> Roslyn          </td>
<td> Brookside Pale Lager </td>
<td style="text-align:right;"> 284.98333</td>
</tr>
<tr>
<td> Breakside       </td>
<td> Vienna Coffee OG &hellip; </td>
<td style="text-align:right;"> 299.86667</td>
</tr>
<tr>
<td> Bale Breaker    </td>
<td> High Camp Winter &hellip; </td>
<td style="text-align:right;"> 300.00833</td>
</tr>
</tbody>
</table>


<p>In a first place tie for a duration of a stunning <em>1 hour fifteen</em> minutes were
Bale Breaker Field 41 Pale and Hop Venom IIPA. There may be
another explanation but with respective BeerAdvocate scores of
<a href="http://www.beeradvocate.com/beer/profile/31389/93176/?ba=beertunes">89</a>
and <a href="http://www.beeradvocate.com/beer/profile/23066/72750/">97</a>, its
easy to see why they are so popular.</p>

<h1>Price of a pint vs. ABV</h1>

<p>Is there a correlation between the price of a pint of beer and the ABV?</p>

<p><img src="http://zachstednick.com/cost_vs_abv.png"></p>

<h1>Drinking based on ABV</h1>

<p>Do beers with higher ABV get ordered faster?</p>

<p><img src="http://zachstednick.com/min_on_tap_vs_abv.png"></p>

<h1>Are beers at Chuck&rsquo;s veblen goods?</h1>

<p>Do pricier beer move faster?</p>

<p><img src="http://zachstednick.com/min_on_tap_vs_price.png"></p>

<p>Obviously there are many more types of analysis one could look at with
this data. I do think this analysis was sorely lacking in first person
research, something I intend to fix in the next analysis. Full code on
<a href="https://github.com/stedy/chucks-beer-db">github</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[2016 Africa Reading Challenge]]></title>
    <link href="http://stedy.github.io/blog/2016/01/13/2016-africa-reading-challenge/"/>
    <updated>2016-01-13T17:10:46-08:00</updated>
    <id>http://stedy.github.io/blog/2016/01/13/2016-africa-reading-challenge</id>
    <content type="html"><![CDATA[<p>A blog I occasionally read issues a challenge every year that is simply to read <a href="http://kinnareads.com/2016/01/06/2016-africa-reading-challenge/">five books from Africa</a>. Since it has been ten years since I moved from Africa back to the States, I figured this would be a good time to finally take on this challenge. I will likely be writing up shor reviews of these on my LibraryThing <a href="https://www.librarything.com/profile/pbirch01">page</a>. My current list includes:</p>

<ul>
<li>Tram 83 by Fiston Mwanza Mujila (<a href="https://www.librarything.com/work/15037664">LT page</a>)</li>
<li>The Fishermen by Chigozie Obioma (<a href="https://www.librarything.com/work/15400542">LT page</a>)</li>
<li>Waiting for the Barbarians by J.M. Coetzee (Really liked everything I
have read by him, not sure why I have never read this one. <a href="https://www.librarything.com/work/3619">LT
page</a>)</li>
<li>Wizard of the Crow by Ngugi wa&#8217;Thiong&#8217;o (Second book set in a
fictional country, perhaps a subtheme for the year? <a href="https://www.librarything.com/work/1026301">LT
page</a>)</li>
<li>The Second Coming of Mavala Shikongo by Peter Orner (Okay, not an
African writer but it is set where I used to live so I think well
worth bending the rules a bit for. <a href="https://www.librarything.com/work/853774">LT page</a></li>
</ul>


<p>Looking forward to this challenge and you should join in if you feel so
inclined.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Summarizing the Seattle restaurant scene in 2015]]></title>
    <link href="http://stedy.github.io/blog/2015/12/28/summarizing-the-seattle-restaurant-scene-in-2015/"/>
    <updated>2015-12-28T12:50:02-08:00</updated>
    <id>http://stedy.github.io/blog/2015/12/28/summarizing-the-seattle-restaurant-scene-in-2015</id>
    <content type="html"><![CDATA[<p>Seattle is experiencing sustained economic growth which manifests
itself in a variety of ways, one of the most visible being changes in
the restaurant
industry. However, tracking the exact changes can be difficult due
to a variety of factors such the size of the city and the difficulty in
aggregating reliable data. There are several options for attempting to
track changes such as scraping data from a review site such as Yelp or
Zomato, systematically
checking the Food section of the local paper, or going straight to the city
records and attempt to
determine what restaurant permits are being issued over time.</p>

<p>For the purposes of this analysis, I decided to compare official city permit
records with the Food column from a local paper, specifically <a href="http://www.thestranger.com/">The
Stranger</a>.</p>

<p>Earlier this year, I started scraping the <a href="http://www.seattle.gov/licenses/find-a-business">City of Seattle Business
License
database</a> which
tracks both permit issuance and revocations. The City of Seattle site
classifies permits based on <a href="https://en.wikipedia.org/wiki/North_American_Industry_Classification_System">NAICS
codes</a>.
Because I am only interested in restaurant related permits, I focused on
the following five NAICS codes: Breweries, Drinking Places
(Alcoholic Beverages), Mobile Food Services, Limited-Service
Restaurants, and Full-Service Restaurants. I tracked changes in permits
by
week which when looked at for the entire year produces the following
figure:</p>

<p><img src="http://zachstednick.com/City_DB_NAICS_2015.png"></p>

<p>Clearly a good time to be in the food truck business. Also I am not sure
why so many Limited-Service Restaurants are closing and so many
Full-Service
Restaurants are opening. It could be simply that the restaurant decided
to expand its service or attempt to cater to a different clientele.
According to the official
<a href="http://www.census.gov/cgi-bin/sssd/naics/naicsrch?code=722511&amp;search=2012">definition</a>,
the NAICS code for Full-Service Restaurant
is that &ldquo;Providing food services to patrons who order and are served
while seated and pay after eating&rdquo;. For more details
on these changes, I have made an <a href="http://seattlerestaurantchanges.com">interactive
site</a> that allows you to further
investigate by neighborhood and restaurant type.</p>

<p>For comparison, I looked at every column I could
find by <a href="http://www.thestranger.com/authors/31833/angela-garbes">Angela
Garbes</a> who
seems to be the main restaurant reporter at The Stranger. I looked at
every restaurant opening and closing she mentioned in her
column. I then
<a href="https://docs.google.com/spreadsheets/d/1HFS4H8E-4RiM47pQzbauLzjfKSoEzv3IZ6Xl2PbHmIg/edit?usp=sharing">classified</a>
all the restaurants she mentioned
based on the same NAICS code to generate a similar figure:</p>

<p><img src="http://zachstednick.com/Stranger_NAICS_2015.png"></p>

<p>Obviously, she has a lot less data and her column is focused on more of
the narrative of big name restaurants that
have opened and less likely to focus on something like a new Taco Time.
She focuses a lot on local food events such as popup restaurants as well
as restaurant personnel which is not something that shows up in the
official city
statistics. Regardless, our figures look very similar and both of the
datasets
show that Full-Service Restaurants have had sustained growth over the
entire year, which makes for great storytelling either with data or a
narrative and
benefits all of us with increased diversity of restaurant options.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Mapping Seattle Traffic Circles]]></title>
    <link href="http://stedy.github.io/blog/2015/09/21/mapping-seattle-traffic-circles/"/>
    <updated>2015-09-21T22:03:39-07:00</updated>
    <id>http://stedy.github.io/blog/2015/09/21/mapping-seattle-traffic-circles</id>
    <content type="html"><![CDATA[<p>Recently, there was a post on Priceonomics about traffic circles with
the argument that roundabouts are <a href="http://priceonomics.com/the-case-for-more-traffic-roundabouts/">safer, help improve traffic flow, and
reduce emissions</a>.
The post mentions that there are 3700 roundabouts in the United States,
which made me wonder - how many of those are in Seattle?</p>

<p>Fortunately, the <a href="https://data.seattle.gov/">City of Seattle data site</a>
has GIS data for all streets as well as GIS data for all traffic
circles. The traffic circles dataset had 1042 total entries or about a
third of the total number of roundabouts in the United States. I&rsquo;m not
sure how accurate that is but I think focusing on traffic circles within
the city limits is more interesting.</p>

<p>I used the <a href="https://cran.r-project.org/web/packages/sp/">sp library</a> in
R to read in both sets of shapefiles and quickly determine the streets
with the highest number of roundabouts:</p>

<table>
<thead>
<tr>
<th> street </th>
<th style="text-align:right;"> count </th>
</tr>
</thead>
<tbody>
<tr>
<td> FREMONT AVE N </td>
<td style="text-align:right;"> 27</td>
</tr>
<tr>
<td> 1ST AVE NW </td>
<td style="text-align:right;"> 24</td>
</tr>
<tr>
<td> 8TH AVE NE </td>
<td style="text-align:right;"> 23</td>
</tr>
<tr>
<td> DAYTON AVE N </td>
<td style="text-align:right;"> 23</td>
</tr>
<tr>
<td> 6TH AVE NW </td>
<td style="text-align:right;"> 21</td>
</tr>
<tr>
<td> 12TH AVE NE </td>
<td style="text-align:right;"> 18</td>
</tr>
</tbody>
</table>


<p>Fremont Ave N. has a whopping 27 traffic circles which seems excessive
until you realize that most of these are North of the zoo and not in the
denser southern part of the street.</p>

<p>I thought quite a bit about how to best represent these values and
ultimately
settled on mapping streets colored to the number of traffic
circles on that street. I used R and <a href="http://colorbrewer2.org/">Color Brewer</a> to make a color attribute for the map based on traffic circle count that could be read by Leaflet. I wrote out my GeoJSON files using
<code>rgdal::writeOGR()</code> and then found that was way too slow to reasonably
run in a browser so I converted it to
<a href="https://github.com/mbostock/topojson">topojson</a> using <code>topojson -o colored_traffic_circles2.topojson -p color colored_traffic_circles.geojson</code>. Even this ran too slowly so I had to
reduce to streets with more than two traffic circles.</p>

<p><img src="http://zachstednick.com/seattle_traffic_circles.png"></p>

<p>Here the darker the color, the more traffic circles present on that
street. For me the most interesting feature is how many traffic circles
in a N-S
direction are right next to State Highway 99 or near I-5. Is this an
attempt to mitigate traffic of people trying to use surface streets
instead of arterials? Possibly, though difficult to determine.
Regardless, 1042 is an impressive amount of traffic circles for a
metropolitan area this large.</p>

<h2>Notes</h2>

<ul>
<li>The City dataset has dates on traffic circle installations with the
oldest traffic circle being 18TH AVE E AND E HARRISON ST installed on
1/5/1976</li>
<li>Full size version of this map available
<a href="http://zachstednick.com/seattle_traffic_circles.html">here</a></li>
<li>All code available <a href="https://github.com/stedy/blog-supplemental">here</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[subsetting shapefiles with R]]></title>
    <link href="http://stedy.github.io/blog/2015/09/04/subsetting-shapefiles-with-r/"/>
    <updated>2015-09-04T13:05:49-07:00</updated>
    <id>http://stedy.github.io/blog/2015/09/04/subsetting-shapefiles-with-r</id>
    <content type="html"><![CDATA[<p>I have been trying to improve my GIS skills lately and have been trying
to use R for as much of this process as I can. One of the tasks I
frequently perform is taking a shapefile, subsetting it, and then
converting to a GeoJSON. The npm module
<a href="https://www.npmjs.com/package/ogr2ogr">ogr2ogr</a> is excellent for
converting from a shapefile to GeoJSON, however I frequntly find myself
needing to select only certain areas of a shapefile. I have been using
two libraries in R to achieve this, specifically
<a href="https://cran.r-project.org/web/packages/rgdal/index.html">rgdal</a> and
<a href="https://cran.r-project.org/web/packages/sp/">sp</a>.</p>

<p>For example, lets use the Congressional District 2012 shapefiles from
the <a href="http://www.ofm.wa.gov/pop/geographic/tiger.asp">Washington State Office of Financial
Management</a>. Downloading
the file, unzipping, and then loading into R with</p>

<p><script
src="https://gist.github.com/stedy/b05e2e11e309b66eb28e.js"></script></p>

<p>We want to select only the districts that cover Seattle, 7 and 9 which
is as simple as subsetting</p>

<p><code>seattle.only &lt;- subset(wa.cd, CD113FP %in% c('07', '09'))</code></p>

<p>One of the nice features about GitHub gists is that you can overlay a
GeoJSON file on a Google map for a quick QC check. While R accepts a
variety of
projection formats, Github does not and I occasionally I find I have to
convert to the WGS84 datum which are easily done with</p>

<p><code>seattle.only.wgs &lt;- spTransform(seattle.only, CRS("+proj=longlat
+ellps=WGS84"))</code></p>

<p>And written out as a GeoJSON file with</p>

<p><code>writeOGR(seattle.only.wgs, dsn="seattle.only.wgs.geojson", layer="cd2012", driver="GeoJSON", check_exists = FALSE)</code></p>

<p>Occasionally I get an error about the file I am about to create not
being found. This Stack Overflow
<a href="http://stackoverflow.com/a/29743969/163809">answer</a> was very helpful
and now I add the <code>check_exists = FALSE</code> parameter every time I write
out with <code>writeOGR()</code>.</p>

<p><script
src="https://gist.github.com/stedy/3c4954baa0b1c96fc9e8.js"></script></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Fremont bridge opening times]]></title>
    <link href="http://stedy.github.io/blog/2015/09/02/fremont-bridge-opening-times/"/>
    <updated>2015-09-02T09:54:03-07:00</updated>
    <id>http://stedy.github.io/blog/2015/09/02/fremont-bridge-opening-times</id>
    <content type="html"><![CDATA[<p>I bike across the Fremont Bridge twice a day which Wikipedia
<a href="https://en.wikipedia.org/wiki/Fremont_Bridge_%28Seattle%29">claims</a> is
the most frequently opened bridge in the United States.
This claim is uncited and while it may be true, due to <a href="http://www.ecfr.gov/cgi-bin/text-idx?SID=cfc62d83403b960e08f5038e67b9ecd1&amp;node=33:1.0.1.10.61.1.65.3&amp;rgn=div8">Federal Maritime
Law</a>
boats get precedence for bridge opening with the exceptions of rush
hours which in Seattle are M-F 7-9 AM and 4-6 PM. I often get to the
bridge on my bike around 9 AM in the morning and 6 PM in the evening and
it
always felt like the bridge opens for a boat right at 9 and 6 PM on the
dot. I wanted to verify this and figured the only way to do so would be
to manually time the bridge openings but that seemed like too much
effort.</p>

<p>Recently, a <a href="https://twitter.com/brotherslogsdon">friend</a> notified me about the twitter account of <a href="https://twitter.com/sdotbridges">Seattle DOT
bridges</a> which is basically a bot that
posts bridge openings and closings such as:</p>

<blockquote class="twitter-tweet" lang="en"><p lang="en" dir="ltr">The
Fremont Bridge has closed to traffic - 9:43:03 AM</p>&mdash;
seattleDOTbridges (@SDOTbridges) <a
href="https://twitter.com/SDOTbridges/status/639116352200970240">September
2, 2015</a></blockquote>


<script async src="//platform.twitter.com/widgets.js"
charset="utf-8"></script>


<p>I used the excellent <a href="https://github.com/geoffjentry/twitteR">twitteR</a>
to scrape tweets from Seattle DOT bridges for the past month to test how
accurate my assumption was. From this, I pulled
the first crossing post morning rush hour and evening rush hour for
weekdays only.</p>

<p><img src="http://zachstednick.com/Fremont_bridge_openings.png"></p>

<p>The mean opening time post-morning rush was 9:28 AM and the mean opening
time post-evening rush was 9:25 which means that my assumptions were
pretty off and I should not feel so stressed to arrive at the bridge
before 9 AM and 6 PM.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Batch collection of park boundaries with Open Street Map]]></title>
    <link href="http://stedy.github.io/blog/2015/08/16/batch-collection-of-park-boundaries-with-open-street-map/"/>
    <updated>2015-08-16T12:16:39-07:00</updated>
    <id>http://stedy.github.io/blog/2015/08/16/batch-collection-of-park-boundaries-with-open-street-map</id>
    <content type="html"><![CDATA[<p>Open Street Map (OSM) is, simply put, a freely available and editable
map of the <a href="http://www.openstreetmap.org">world</a>. I have been interested in
improving the availability of <a href="https://github.com/openseattle/seattle-boundaries">boundaries in Seattle</a> and wanted
to add park boundaries to this list as well. It was easy to look up
boundaries on OSM, for example <a href="http://www.openstreetmap.org/way/53600070">Salmon Bay Park</a> shows the various nodes that
make up its boundaries. But I had struggled with how to automate this
search since at last count Seattle had over <a href="http://www.seattle.gov/parks/listall.asp">400
parks</a>. After months of
struggling with the OSM API, I fortuitously stumbed
across the following tweet:</p>

<blockquote class="twitter-tweet" lang="en"><p lang="en" dir="ltr">Two
map data posts:&#10;&#10;Historic map warping for <a
href="https://twitter.com/somethingmodern">@somethingmodern</a> <a
href="http://t.co/7POw8PNBXI">http://t.co/7POw8PNBXI</a>&#10;&#10;Data
from OSM for <a href="https://twitter.com/alignedleft">@alignedleft</a>
<a href="http://t.co/DqxAuyPunh">http://t.co/DqxAuyPunh</a></p>&mdash;
Michal Migurski (@michalmigurski) <a
href="https://twitter.com/michalmigurski/status/632613151721193472">August
15, 2015</a></blockquote>


<script async src="//platform.twitter.com/widgets.js"
charset="utf-8"></script>


<p>This tweet lead me to Mapzen which provides a service called <a href="https://mapzen.com/data/metro-extracts/">Metro
Extracts</a> which provides
datasets from OSM on a weekly basis. I downloaded the OSM2PGSQL GeoJSON
file for Seattle which provided me files for Line, Point, and
Polygon geometries. I then used
<a href="https://www.npmjs.com/package/ogr2ogr">ogr2ogr</a> to filter for parks
only with the command</p>

<p><code>ogr2ogr select 'osm_id, name, geometry' where "leisure = 'park'"</code></p>

<p>This produced a GeoJSON file that looked like this:</p>

<script src="https://gist.github.com/stedy/d11ab757400fd5375796.js"></script>


<p>Obviously, more filtering needed to be done. Since many of these parks
were not in Seattle, I used the Nominatim API to search for each park
based on the OSM ID number. For example, the above mentioned park Salmon
Bay Park returns a nicely formatted <a href="http://nominatim.openstreetmap.org/lookup?osm_ids=W53600070">XML
file</a> which
I just filtered based on city.</p>

<p>Even after this there were still parks that were wrongly labelled as
being in Seattle. I loaded the file into R and subset based on OSM ID
and then used
<a href="https://cran.r-project.org/web/packages/rgdal/">rgdal</a> to write the
final result out as a GeoJSON file.</p>

<script src="https://gist.github.com/stedy/1ef0a40bc8c29f7412a0.js"></script>


<p>The take home lesson for me is that OSM is an excellent service but as
with any publically annotated dataset be prepared to invest some time
into cleaning and validating the data.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Update on restaurant changes]]></title>
    <link href="http://stedy.github.io/blog/2015/07/23/update-on-restaurant-changes/"/>
    <updated>2015-07-23T15:31:31-07:00</updated>
    <id>http://stedy.github.io/blog/2015/07/23/update-on-restaurant-changes</id>
    <content type="html"><![CDATA[<p>I have been tracking restaurant openings via the <a href="http://www.seattle.gov/licenses/find-a-business">City of Seattle
Business Finder</a> since
the beginning of this year and am reporting those changes at <a href="http://seattlerestaurantchanges.com/">Seattle
Restaurant Changes</a>. Recently I
put up a <a href="http://seattlerestaurantchanges.com/heatmap.html">heatmap</a>
showing changes by neighborhood. This heatmap shows a current snaphot of
the changes which made me curious about changes by restaurant type over the course of
the year.</p>

<iframe width="980" height="520" src="http://zachstednick.com/net_restaurant_changes.html"></iframe>


<p>A few notes:</p>

<ul>
<li><p>The City of Seattle uses North American Industry Classification System
codes <a href="http://www.census.gov/eos/www/naics/">(NAICS)</a> to track
restaurants. I then use the date of permit issuance as a proxy for a
restaurant opening and date of permit revocation as a proxy for closing.</p></li>
<li><p>A Full Service Restaurant as defined by NAICS is &ldquo;establishments primarily engaged in providing food services to patrons who order and are served while seated (i.e., waiter/waitress service) and pay after eating&rdquo;</p></li>
<li><p>I realized that not that many breweries would be opening up but who
doesn&rsquo;t want more breweries in town?</p></li>
<li><p>I was not expecting Full Service Restaurants to take off as much as
they did, especially since Limited Service Restaurants seem to be
declining.</p></li>
</ul>


<p>I will try to post another update on this in December, that is unless I decide to open up a food truck of my own.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The Felix Factor]]></title>
    <link href="http://stedy.github.io/blog/2015/05/30/the-felix-factor/"/>
    <updated>2015-05-30T16:12:37-07:00</updated>
    <id>http://stedy.github.io/blog/2015/05/30/the-felix-factor</id>
    <content type="html"><![CDATA[<p>I was listening to the Jonah Keri
<a href="http://espn.go.com/espnradio/grantland/player?id=12957030">podcast</a> and
he and Ben Gibbard were talking about the Mariners, specifically <a href="http://m.mlb.com/player/433587/felix-hernandez/2015/career/R/pitching/MLB">Felix
Hernandez</a>.
One of the points Gibbard made was that Hernandez is so outstanding that
he will be remembered and that people should try to see him pitch in
person. This made me wonder, did Felix Hernandez have an impact on home
ticket sales for the Mariners in 2014?</p>

<p>I was able to get all the data from some of the nicely formatted box
score data that <a href="http://gd2.mlb.com/components/game/mlb/year_2014/">MLB provides</a>. I initially tried to look at the data over the course of the year but attendance was so variable (which made for an extremely confusing plot) that I just ended up making a box and whiskers plot and ignored the date element:</p>

<p><img src="http://zachstednick.com/felix_factor.png"></p>

<p>Conclusion: Hernandez was not that strong of a driver of ticket sales
which is great news if you are hoping to see him pitching in person.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Seattle Restaurant Changes]]></title>
    <link href="http://stedy.github.io/blog/2015/04/17/seattle-restaurant-changes/"/>
    <updated>2015-04-17T10:51:18-07:00</updated>
    <id>http://stedy.github.io/blog/2015/04/17/seattle-restaurant-changes</id>
    <content type="html"><![CDATA[<p>Seattle construction is currently booming and I was interested in how
that
reflected in the local restaurant scene. There are many food blogs and
local news sites that cover openings and closings, but I found it too
difficult to parse these in a regular manner. Fortunately I was able to
use data from the <a href="http://www.seattle.gov/licenses/find-a-business">City of Seattle business
finder</a> and used the
restaurant classification or
<a href="https://en.wikipedia.org/wiki/NAICS">NAICS</a> code as a proxy. Using the
data in this manner makes an assumption that a restaurant will no longer
have a business licence after it closes.
I&rsquo;m not sure how accurate this is but I figured it was as accurate as I
could get short of hiring people on Mechanical Turk to phone every
restaurant every week and ask if the restaurant is still open. To map each
restaurant to a particular neighborhood, I
used geolocation to map license address returned by the City of Seattle business
finder. Obviously that does not work as well for Mobile Food Services
(i.e. food trucks) but it still allows for an interesting comparison.
This data is plotted at <a href="http://seattlerestaurantchanges.com">Seattle Restaurant
Changes</a>.</p>

<p>I initially attempted to scrape
data from <a href="https://www.thestranger.com/food-and-drink">The Stranger</a> but
after finding the City of Seattle site I just used
<a href="http://www.crummy.com/software/BeautifulSoup/">BeautifulSoup</a> for the
scraping. I would not have been able to do much more beyond that state
if it had not been for Nathan Yau&rsquo;s excellent tutorial on <a href="http://flowingdata.com/2015/02/19/make-an-interactive-map-with-category-filters/">making maps
with category
filters</a>.
I was able to get a state level shapefile for Washington state from
<a href="http://www.zillow.com/blog/7000-neighborhood-boundary-files-in-shapefile-format-4653/">Zillow</a>
and then reduce that to just Seattle neighborhoods using R&rsquo;s
<a href="http://cran.r-project.org/web/packages/sp/">sp</a> package. Full code
posted on <a href="https://github.com/stedy/seattle-restaurants">github</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[First year on Fitbit]]></title>
    <link href="http://stedy.github.io/blog/2015/02/25/first-year-on-fitbit/"/>
    <updated>2015-02-25T14:15:11-08:00</updated>
    <id>http://stedy.github.io/blog/2015/02/25/first-year-on-fitbit</id>
    <content type="html"><![CDATA[<p>After a year on <a href="https://www.fitbit.com/user/2F49C2">Fitbit</a>, I figured it might be time to take a look at the data that I have been generating. Unfortunately, Fitbit makes you sign up for <a href="https://www.fitbit.com/premium/export">Premium</a> which charges you $50 per year to export your data. Fortunately, Cory Nissen has created an excellent <a href="https://github.com/corynissen/fitbitScraper">R package</a> for doing just this. The package simply uses a POST request handled by Hadley&rsquo;s <a href="https://github.com/hadley/httr">httr</a> library to generate a cookie and then parses the returned JSP results to return a nice <code>data.frame</code>.</p>

<p>Anyways, onto the data.</p>

<p>The first command I tried was the <code>get_15_min_data()</code> for parsing step
data in 15 minute increments. I figured that looking at yesterday&rsquo;s data
would be granular enough to get a good feel for the data.</p>

<p><img src="http://zachstednick.com/fifteen_min_fitbit.png"></p>

<p>I then plotted number of steps taken per day, with a smoothing function
overlaid:</p>

<p><img src="http://zachstednick.com/one_year_fitbit_steps.png"></p>

<p>I had a mean step count of 13935 for the past year. This data is more
interesting to look at as more of an overall trend. There definitely a
seasonal trend in the summer which makes sense. I can also see the
signatures of when I went on a four day backpacking trip in August and
when I broke two ribs and was confined to the couch for four days in
mid-March.</p>

<p>Since I have a Fitbit one, I can also measure floors climbed.</p>

<p><img src="http://zachstednick.com/fitbit_one_year_floors.png"></p>

<p>My mean number of floors climbed is 69.62 which seems absurdly high. My
desk is on the fourth floor of my building and I usually take the stairs
but not sure that is enough to fully explain why these counts are so
high.</p>

<p>Still, it is pretty interesting to look at this data outside of the
Fitbit interface and I would highly recommend checking out Cory&rsquo;s github
<a href="https://github.com/corynissen/fitbitScraper">repo</a></p>

<p>Also, speaking of github, for those of you who regularly follow this
blog (hi, Mom!) I have moved away from making a new gist every time to
simply having a standalone
<a href="https://github.com/stedy/blog-supplemental">repo</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Offsetting beer by running]]></title>
    <link href="http://stedy.github.io/blog/2015/01/10/offsetting-beer-by-running/"/>
    <updated>2015-01-10T14:24:32-08:00</updated>
    <id>http://stedy.github.io/blog/2015/01/10/offsetting-beer-by-running</id>
    <content type="html"><![CDATA[<p>Last year, among other personal data, I tracked every bar I went to and
every mile I
<a href="http://zachstednick.name/blog/2014/02/06/foursquare-without-a-smartphone/">ran</a>.
Naturally my first question is do I run enough to offset the amount of
beer I am drinking (at bars)?</p>

<p>First we define some units. According to this Runner&rsquo;s World
<a href="http://www.runnersworld.com/tools/calories-burned-calculator">calculator</a>, at 8:45 minute/mile for my
weight I am burning 145 calories. Google says the amount of calories in
a <a href="https://www.google.com/?gws_rd=ssl#q=calories+in+a+pint+of+beer">pint of
beer</a> is about 180. Since I usually average about two beers each time I go to a bar, that simplifies the calculations. Over the course of the year, how often was I above or below the residual? To answer this, I used R and finally got around to trying <a href="https://github.com/hadley/tidyr">tidyr</a> which is pretty slick.</p>

<p><img src="http://zachstednick.com/running_vs_beers.png"></p>

<p>I thougth a lot about how to determine the residual but eventually
settled on calories out - calories in because I felt this method made the best
visualization. As you can see around week 30, I started to run more and
did a better job at offsetting my beer consumption. Obviously this is an
overly simplistic view of my caloric expenditure but shows some of the
interesting insights that can be gained from personal data.</p>

<p>As always, all code and data is in this github
<a href="https://gist.github.com/stedy/9f2ffa58e25cc52dbe2e">gist</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Summarizing books read over time]]></title>
    <link href="http://stedy.github.io/blog/2015/01/04/summarizing-books-read-over-time/"/>
    <updated>2015-01-04T13:54:43-08:00</updated>
    <id>http://stedy.github.io/blog/2015/01/04/summarizing-books-read-over-time</id>
    <content type="html"><![CDATA[<p>I recently read an interesting blog post where the author examined their books rated on Goodreads and summarizing <a href="http://citizen-statistician.org/2014/12/31/annual-review-of-reading/">interesting
trends</a>.
I decided to do a similar analysis even though I use
<a href="https://www.librarything.com/profile/pbirch01">LibraryThing</a>
instead.</p>

<p>LibraryThing has a nice option to allow to to export your data in a
variety of <a href="https://www.librarything.com/export.php">formats</a>. Since I
write R code to parse CSV files everyday I thought I would do something
different and parse a JSON file with python.</p>

<p>I have been on LibraryThing since 2007 and the first question I was
interested in was have my average ratings changed over time? I
calculated the mean for each book by year:</p>

<table>
<thead>
<tr>
<th> Year </th>
<th style="text-align:right;"> Average Rating </th>
</tr>
</thead>
<tbody>
<tr>
<td> 2007 </td>
<td style="text-align:right;"> 3.446809</td>
</tr>
<tr>
<td> 2008 </td>
<td style="text-align:right;"> 3.480000</td>
</tr>
<tr>
<td> 2009 </td>
<td style="text-align:right;"> 3.485294</td>
</tr>
<tr>
<td> 2010 </td>
<td style="text-align:right;"> 3.641509</td>
</tr>
<tr>
<td> 2011 </td>
<td style="text-align:right;"> 3.456522</td>
</tr>
<tr>
<td> 2012 </td>
<td style="text-align:right;"> 3.529412</td>
</tr>
<tr>
<td> 2013 </td>
<td style="text-align:right;"> 3.321429</td>
</tr>
<tr>
<td> 2014 </td>
<td style="text-align:right;"> 3.614583</td>
</tr>
</tbody>
</table>


<p>While uninteresting, this makes a lot of sense - if I am reading a book
that I do not enjoy, I will usually bail on it which tends to bias my
ratings upward. Over time, there have been a few notable
<a href="https://www.librarything.com/work/11691727/reviews/92168929">exceptions</a>.</p>

<p>One of the other interesting analyses in the blog post was examining how the
reviewer&rsquo;s ratings have changed based on the month of the year. I wanted to
make a similar plot using R&rsquo;s
<a href="http://docs.ggplot2.org/current/index.html">ggplot2</a> however since I
was writing this in python I was largely limited to matplotlib.
Fortunately, many people have struggled with this issue and the fine
folks at yhat have ported ggplot2 over to
<a href="https://pypi.python.org/pypi/ggplot">python</a>. With this library I was
able to use <code>geom_smooth</code> to produce the following plot showing rating
trends by week.</p>

<p><img src="http://zachstednick.com/all_years.png"></p>

<p>I tried to figure out why my legend never showed up but I figured that
since most of the trend lines were pretty much the same anyways that the
plot was fine without a legend. It appears that I get in most of my good
reviews early in the year and am harsher later in the year.</p>

<p>The last figure in the blog post compares the writer&rsquo;s review scores to
the Goodreads consensus score. I attempted to
replicate this but ran into more trouble than it was worth to extract
that data from LibraryThing so I abandoned that analysis.</p>

<p>If interested, I put my python code in a GitHub
<a href="https://gist.github.com/stedy/2cb4fb1f332508f39ff2">gist</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Simple webstats with R]]></title>
    <link href="http://stedy.github.io/blog/2014/11/19/simple-webstats-with-r/"/>
    <updated>2014-11-19T17:01:34-08:00</updated>
    <id>http://stedy.github.io/blog/2014/11/19/simple-webstats-with-r</id>
    <content type="html"><![CDATA[<p>As someone who puts out writings out publically, I am naturally curious who (if anyone) is actually reading what I write. To answer this I developed a simple webstat calculator using R. I realize there are many options out there for tracking visits but to paraphrase my friend Andy, <a href="http://wingolog.org/archives/2014/11/14/on-yakshave-on-color-on-cosines-on-glitchen">when has using standard libraries lead to anything cool?</a>.</p>

<p>My main interests in this project is to answer two questions:</p>

<ol>
<li><p>Are people visting this site?</p></li>
<li><p>Where are they visting from?</p></li>
</ol>


<p>I don&rsquo;t really care about things like <a href="https://en.wikipedia.org/wiki/Bounce_rate">bounce rate</a> or type of device used to access the site. Not having to worry about either of these issues helps cut down on the complexity. I run this site on an Apache server and use a standard log output to write my logfile:
<code>LogFormat "%v:%p %h %l %u %t \"%r\" %&gt;s %O \"%{Referer}i\" \"%{User-Agent}i\"</code></p>

<p>I made a small R script that uses <a href="http://cran.r-project.org/web/packages/knitr/">knitr</a> to output plots to HTML for ease in viewing. I wrote a shell script that uses the excellent <a href="http://dirk.eddelbuettel.com/code/littler.html">little r</a> to perform the commands. I run the shell script daily as a cron job and only look back at the past week&rsquo;s worth of data. Since this blog is served on github pages, it can be difficult to see page views so I use images loaded as a proxy.</p>

<p>Here are some example plots of recent visitors:</p>

<p><img src="http://zachstednick.com/visitor.png"></p>

<p>And then another plot of visitor locations:</p>

<p><img src="http://zachstednick.com/referrer_location.png"></p>

<p>That outlier from Brazil is likely a Google bot crawling the site; better detection and removal of bot traffic from the final output is on the TODO list. All of the code (minus the shell script) lives on <a href="https://github.com/stedy/simple-webstats">github</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[(206)419-PARKS]]></title>
    <link href="http://stedy.github.io/blog/2014/11/05/206-419-parks/"/>
    <updated>2014-11-05T15:27:00-08:00</updated>
    <id>http://stedy.github.io/blog/2014/11/05/206-419-parks</id>
    <content type="html"><![CDATA[<p>I recently became aware of the efforts of Linnea Westerlind who made a
goal to visit every park in Seattle and documented her efforts
<a href="http://www.yearofseattleparks.com/">here</a>. I thought this was pretty
neat so I looked up the list of <a href="http://www.seattle.gov/parks/listall.asp">City of Seattle
Parks</a> which currently lists
419 parks. The definitions for a park are hard to determine and this
list ends up with some oddball parks such as <a href="https://www.google.com/maps/place/Crescent+Place/@47.6842435,-122.3330848,19z/data=!4m2!3m1!1s0x54901413dd56bf9f:0xa19ed07208659a24">Crescent
Place</a>.
Still I think it is an interesting way to learn more about where you live
wherever that may be. I have currently visited 118/419  parkswhich is
about 28%, not bad. Not
sure if I will be able to reach them faster than the 4 years it took
Westerlind but maybe I should just focus on the journey instead.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[LEGO price estimates over time]]></title>
    <link href="http://stedy.github.io/blog/2014/08/30/lego-price-estimates-over-time/"/>
    <updated>2014-08-30T16:22:00-07:00</updated>
    <id>http://stedy.github.io/blog/2014/08/30/lego-price-estimates-over-time</id>
    <content type="html"><![CDATA[<p>LEGO recently introduced a new LEGO set called <a href="http://shop.lego.com/en-US/Research-Institute-21110">Research
Institute</a> which
featured three female scientists. Since my wife is also a female
scientist, I tried to order one from the LEGO website only to learn that
they had sold out in less than a day. I then wrote an email complaining
about this to LEGO who responded by sending me an apology note and a
catalog.</p>

<p>I grew up playing with LEGO sets, hard to avoid when you were named Zach
and commercials like
<a href="https://www.youtube.com/watch?v=pDH3AoOQzE0">this</a> dominated the
airwaves. Anyways, when I was a kid my dad once mentioned to me that a good rule of thumb
for determining the price of a LEGO set was to estimate each brick
costing about 10 cents. This new catalog made me wonder if this was still
true. I copied all the model numbers as well as the number of pieces and
the prices. I was also curious in how true this trend was when adjusted
for inflation so I used the CPI Inflation calculator from <a href="http://data.bls.gov/cgi-bin/cpicalc.pl?cost1=.10&amp;year1=1989&amp;year2=2014">US Bureau of
Labor
Statistics</a> which showed
that $0.10 in 1989 had the same buying power as $0.19 in 2014. Ideally I
could have found a catalog from 1989 but I don&rsquo;t remember any back then
and I probably would have cut it up to put pictures in my locker or
something like that. I used R
to plot both of these trends and it appears that my dad&rsquo;s estimate still
holds true for 2014.</p>

<p><img src="http://zachstednick.com/lego_by_year.png"></p>

<p>A correlation calculation for all sets gives a value of 0.91 which means my dad had a
pretty good estimate back in the day.</p>

<p>I also looked at the average price for each collection and found that
almost all collections retained a high correlation between the estimated
price and the actual price.</p>

<table>
<thead>
<tr>
<th> Collection </th>
<th> Collection Mean Price </th>
<th style="text-align:right;"> Collection Correlation </th>
</tr>
</thead>
<tbody>
<tr>
<td> Basics </td>
<td> 29.99 </td>
<td style="text-align:right;"> NA</td>
</tr>
<tr>
<td> Chima </td>
<td> 38.99 </td>
<td style="text-align:right;"> 0.977</td>
</tr>
<tr>
<td> City </td>
<td> 54.365 </td>
<td style="text-align:right;"> 0.896</td>
</tr>
<tr>
<td> Creator </td>
<td> 100.375 </td>
<td style="text-align:right;"> 0.955</td>
</tr>
<tr>
<td> DC Superheroes </td>
<td> 76.657 </td>
<td style="text-align:right;"> 1</td>
</tr>
<tr>
<td> Disney Princess </td>
<td> 27.657 </td>
<td style="text-align:right;"> 0.963</td>
</tr>
<tr>
<td> Exclusive </td>
<td> 149.99 </td>
<td style="text-align:right;"> NA</td>
</tr>
<tr>
<td> Friends </td>
<td> 23.354 </td>
<td style="text-align:right;"> 0.992</td>
</tr>
<tr>
<td> Ideas </td>
<td> 49.99 </td>
<td style="text-align:right;"> NA</td>
</tr>
<tr>
<td> Juniors </td>
<td> 27.49 </td>
<td style="text-align:right;"> 0.901</td>
</tr>
<tr>
<td> LEGO Movie </td>
<td> 63.99 </td>
<td style="text-align:right;"> 0.997</td>
</tr>
<tr>
<td> Marvel Superheroes </td>
<td> 40.99 </td>
<td style="text-align:right;"> 0.934</td>
</tr>
<tr>
<td> Mindstorms </td>
<td> 349.99 </td>
<td style="text-align:right;"> NA</td>
</tr>
<tr>
<td> Minecraft </td>
<td> 139.96 </td>
<td style="text-align:right;"> NA</td>
</tr>
<tr>
<td> Mixels </td>
<td> 4.99 </td>
<td style="text-align:right;"> NA</td>
</tr>
<tr>
<td> Ninjago </td>
<td> 47.434 </td>
<td style="text-align:right;"> 0.976</td>
</tr>
<tr>
<td> Simpsons </td>
<td> 199.99 </td>
<td style="text-align:right;"> NA</td>
</tr>
<tr>
<td> Star Wars </td>
<td> 133.365 </td>
<td style="text-align:right;"> 0.979</td>
</tr>
<tr>
<td> Technic </td>
<td> 81.99 </td>
<td style="text-align:right;"> 0.989</td>
</tr>
<tr>
<td> Ultra Agents </td>
<td> 45.323 </td>
<td style="text-align:right;"> 0.983</td>
</tr>
</tbody>
</table>


<p>Raw data and code for this lives at this
<a href="https://gist.github.com/stedy/92b949ba44effd66c855">gist</a></p>
]]></content>
  </entry>
  
</feed>
