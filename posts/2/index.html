
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>My Octopress Blog</title>
  <meta name="author" content="Your Name">

  
  <meta name="description" content="The Listserve is an email lottery, you sign up and once a day someone
gets a chance to send the entire list an email. My previous
post
covered how I &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://stedy.github.io/posts/2">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="My Octopress Blog" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="/javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  

</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">My Octopress Blog</a></h1>
  
    <h2>A blogging framework for hackers.</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="https://www.google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="sitesearch" value="stedy.github.io">
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2013/04/14/analysis-of-the-listserve-emails/">Analysis of the Listserve Emails</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2013-04-14T20:23:00-07:00'><span class='date'><span class='date-month'>Apr</span> <span class='date-day'>14</span><span class='date-suffix'>th</span>, <span class='date-year'>2013</span></span> <span class='time'>8:23 pm</span></time>
        
      </p>
    
  </header>


  <div class="entry-content"><p><a href="http://www.thelistserve.com">The Listserve</a> is an email lottery, you sign up and once a day someone
gets a chance to send the entire list an email.  My previous
<a href="http://zachstednick.name/blog/2013/04/14/text-mining-the-listserve-emails/">post</a>
covered how I fetched these emails, this post will discuss the actual
statistics obtained from The Listserve emails.</p>

<h1>To:</h1>

<p>The Listserve website mentions the countries of subscribers but thats
about it.  As of today, there are currently 21,402 subscribers.  I
fetched all the archival data I could from Internet Archive and looked
at enrollment over time which has stayed consistent around 20,000.  I
also plotted
enrollment over <a href="http://zachstednick.com/enrollment.html">time</a>.</p>

<h1>From:</h1>

<p>The Listserve allows you to use any name you want as the sender of the email, here are the ones that occurred more than once:</p>

<table>
<thead>
<tr>
<th> Name </th>
<th style="text-align:right;"> Occurrence </th>
</tr>
</thead>
<tbody>
<tr>
<td> Anonymous </td>
<td style="text-align:right;"> 12</td>
</tr>
<tr>
<td> Laura </td>
<td style="text-align:right;"> 3</td>
</tr>
<tr>
<td> The Listserve </td>
<td style="text-align:right;"> 2</td>
</tr>
<tr>
<td> Ben </td>
<td style="text-align:right;"> 2</td>
</tr>
<tr>
<td> Beth </td>
<td style="text-align:right;"> 2</td>
</tr>
<tr>
<td> David </td>
<td style="text-align:right;"> 2</td>
</tr>
<tr>
<td> Sam </td>
<td style="text-align:right;"> 2</td>
</tr>
<tr>
<td> Michelle Huang </td>
<td style="text-align:right;"> 2</td>
</tr>
<tr>
<td> T. </td>
<td style="text-align:right;"> 2</td>
</tr>
</tbody>
</table>


<p>Interesting that Michelle Huang had two entries, what happens if we look at first name only?</p>

<table>
<thead>
<tr>
<th> Name </th>
<th style="text-align:right;"> Occurrence </th>
</tr>
</thead>
<tbody>
<tr>
<td> Anonymous </td>
<td style="text-align:right;"> 12</td>
</tr>
<tr>
<td> Chris </td>
<td style="text-align:right;"> 8</td>
</tr>
<tr>
<td> David </td>
<td style="text-align:right;"> 7</td>
</tr>
<tr>
<td> Jordan </td>
<td style="text-align:right;"> 4</td>
</tr>
<tr>
<td> Michelle </td>
<td style="text-align:right;"> 4</td>
</tr>
<tr>
<td> Alex </td>
<td style="text-align:right;"> 3</td>
</tr>
<tr>
<td> Andy </td>
<td style="text-align:right;"> 3</td>
</tr>
<tr>
<td> Ben </td>
<td style="text-align:right;"> 3</td>
</tr>
<tr>
<td> Brian </td>
<td style="text-align:right;"> 3</td>
</tr>
<tr>
<td> Daniel </td>
<td style="text-align:right;"> 3</td>
</tr>
<tr>
<td> James </td>
<td style="text-align:right;"> 3</td>
</tr>
<tr>
<td> Laura </td>
<td style="text-align:right;"> 3</td>
</tr>
</tbody>
</table>


<p>What about time of day sent?</p>

<p>I took all the timestamps from the emails and plotted when they were
sent based on GMT.  This was more due to personal curiosity but
interesting nonetheless.  The red line in the plot is the mean time
which ended up being 17:19:15 GMT.  Those large drops are likely due to
some nuances in email dates.  For example, I got two emails on 23 June
2012 and none on 22 June.</p>

<p><img src="http://zachstednick.com/arrivaltimes.png"></p>

<h1>Subject:</h1>

<p>I took all the subject lines and created a word frequency table on how
often that word occurred:</p>

<table>
<thead>
<tr>
<th> Word </th>
<th style="text-align:right;"> Occurrence </th>
</tr>
</thead>
<tbody>
<tr>
<td> life </td>
<td style="text-align:right;"> 9</td>
</tr>
<tr>
<td> world </td>
<td style="text-align:right;"> 9</td>
</tr>
<tr>
<td> day </td>
<td style="text-align:right;"> 8</td>
</tr>
<tr>
<td> little </td>
<td style="text-align:right;"> 8</td>
</tr>
<tr>
<td> love </td>
<td style="text-align:right;"> 7</td>
</tr>
<tr>
<td> story </td>
<td style="text-align:right;"> 7</td>
</tr>
<tr>
<td> advice </td>
<td style="text-align:right;"> 6</td>
</tr>
<tr>
<td> time </td>
<td style="text-align:right;"> 6</td>
</tr>
</tbody>
</table>


<h1>Body:</h1>

<p>For the body of the email I created a <a href="http://en.wikipedia.org/wiki/Document-term_matrix">Term-Document Matrix</a> which is a matrix that describes the frequency of words and how often they occur together.  This allows themes and trends of the body of work or corpus, which in this case happens to be The Listserve emails.  I took all the emails and removed punctuation and stop words such as &ldquo;and&rdquo; or &ldquo;but&rdquo; and made a matrix based on how often the most common words occured together.  I then created a dendrogram of all the words and how they clustered with each other.</p>

<p><img src="http://zachstednick.com/dendrogram.png"></p>

<p>The majority of the words are pretty evenly clustered and its difficult
to determine any trends.  However there is a cluster on the far left
side of the tree which I zoomed in on:</p>

<p><img src="http://zachstednick.com/zoomin.png"></p>

<p>This cluster includes word pairs such as &ldquo;email&rdquo; and &ldquo;listserve&rdquo;, &ldquo;love&rdquo; and &ldquo;time&rdquo;, and &ldquo;life&rdquo; and &ldquo;people&rdquo;.  While its not surprising to see these words occurring so often together, it is interesting to see that a majority of people use this email to dispense wisdom or advice to the masses.</p>

<p>I have not yet been selected for The Listserve but I am sure these
findings here will strongly influence what I write.  In the meantime,
I want to learn more about text processing since I found it pretty interesting.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2013/04/14/text-mining-the-listserve-emails/">Text Mining the Listserve Emails</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2013-04-14T18:21:00-07:00'><span class='date'><span class='date-month'>Apr</span> <span class='date-day'>14</span><span class='date-suffix'>th</span>, <span class='date-year'>2013</span></span> <span class='time'>6:21 pm</span></time>
        
      </p>
    
  </header>


  <div class="entry-content"><p><a href="http://www.thelistserve.com">The Listserve</a> is an email list where
people sign up for a chance to send an email out to the entire list to
discuss whatever they want.  Currently the number of people enrolled is
about 20,000 and there has been one email per day since 16 April 2012.
I thought that since this project has been running for about a year, it
would be a nice opportunity to learn a little more about text mining.</p>

<p>In this first part I will discuss how I fetched all those emails and
parsed them and in a second blog post I will talk about what I found.</p>

<p>The first issue was how to get the emails off the server and after
trying a few solutions I finally ended up using the Python
<a href="http://docs.python.org/2/library/imaplib.html">imaplib</a> which is a
Python library for connecting with an IMAP4 email server which is used
by all the major providers such as Yahoo and Google.  After connecting I
used the Python <a href="http://docs.python.org/2/library/email.html">email</a> library which helped facilitate selecting certain parts of the email.  I relied
heavily on the function <code>email.message_from_string()</code> to fetch email
attributes such as Message-ID or Sender.  I took all these emails and
dumped them into a SQLite database to later parse with
<a href="http://cran.r-project.org">R</a>.</p>

<p>I use R almost daily for work so it was nice to tackle this part of the
project with tools I knew pretty well.  I used <code>sapply()</code> and
<code>strsplit()</code> mostly to parse out parts of various email attributes and
then used the <a href="http://cran.r-project.org/web/packages/tm/index.html">tm</a>
package to handle all of the text processing.  The tm package makes it
easier to get all the emails into a term document matrix which is much
easier to work with a large corpus of text such as this.  I used an
English dictonary with the tm package to remove stop words and for
stemming (reducing the word to its base form).  There have been two
emails so far in Portuguese but the rest are all in English.</p>

<p>Initially I thought I could track all the emails by date but this proved
to be a difficult task due to the nuances of email and when they
actually got sent off the server.  Instead I ended up using the
Message-ID for making sure that I did not duplicate emails in the
analysis.</p>

<p>I put up all the source code on a github
<a href="https://github.com/stedy/thelistserve-stats">repo</a>.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2013/02/27/insert-cat-picture/">Insert Cat Picture</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2013-02-27T10:26:00-08:00'><span class='date'><span class='date-month'>Feb</span> <span class='date-day'>27</span><span class='date-suffix'>th</span>, <span class='date-year'>2013</span></span> <span class='time'>10:26 am</span></time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>My wife and sometimes other people ask me to send them research papers
that are not publicly available which I am more than happy to do.
However, why should I not have some fun with the final document I send
her.  I use the excellent, although sadly deprecated
<a href="http://pybrary.net/pyPdf/">PyPdf</a>.  I have not checked out pyPdf2 but
it does look promising.  Here is the
<a href="https://gist.github.com/stedy/5050018">gist</a> for how I randomly add an
image (usually of a cat) to the pdf document and then rename it since
most research sites name their documents similar to the DOI for the
paper.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2013/02/21/css-stopwatch/">CSS Stopwatch</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2013-02-21T17:49:00-08:00'><span class='date'><span class='date-month'>Feb</span> <span class='date-day'>21</span><span class='date-suffix'>st</span>, <span class='date-year'>2013</span></span> <span class='time'>5:49 pm</span></time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>My wife kept complaining about being bored in long meetings so I decided
to try and help her cut down on the monotony.
<a href="http://thecodeplayer.com/walkthrough/make-a-stopwatch-using-css3-without-images-or-javascript">There</a> is a great demo for
making a stopwatch with pure CSS.  I extended it to provide two
stopwatches that can run independently.  My
<a href="https://gist.github.com/stedy/4728805">gist</a> and the actual
<a href="http://www.zachstednick.com/stopwatch.html">stopwatches</a>.  With more
time I would go back in and add a javascript popup window to populate
the names.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2013/01/30/knapsack-problem-irl/">Knapsack Problem IRL</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2013-01-30T22:49:00-08:00'><span class='date'><span class='date-month'>Jan</span> <span class='date-day'>30</span><span class='date-suffix'>th</span>, <span class='date-year'>2013</span></span> <span class='time'>10:49 pm</span></time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Orders for placing Girl Scout cookies are starting to hit the workplace
email lists which is always a unique buying experience.  The cookies are
available for a limited time so you have to order when you can
especailly since I always seem to run into girls selling them when I
don&rsquo;t have any cash on me (time for a Square charitable collaboration?).</p>

<p>This year I decided to apply the <a href="https://en.wikipedia.org/wiki/Knapsack_problem">knapsack
algorithm</a> to find the
best possible approach to buying cookies.  I used the code from the
wikipedia page, and edited it for GS cookies.  Since I bike most days I
limited the weight to 80 ounces.  I wanted to maximize for the cookies I
liked as well as maximize the number of cookies I would get.</p>

<p>Looks like the Savannah smiles really threw off my preferences:<br>
<code>4 of: do-si-does</code><br>
<code>3 of: samoas</code><br>
<code>1 of: savannah smiles</code><br></p>

<p>Regardless, I am looking forward to getting my order!</p>

<p>Try for yourself with this <a href="https://gist.github.com/4680832">gist</a></p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2013/01/16/red-e-and-me/">Red E and Me</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2013-01-16T18:02:00-08:00'><span class='date'><span class='date-month'>Jan</span> <span class='date-day'>16</span><span class='date-suffix'>th</span>, <span class='date-year'>2013</span></span> <span class='time'>6:02 pm</span></time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Sometime around the 30th of December 2012, I noticed a huge spike in
traffic on my server.  I looked at the server logs and noticed that many
of the requests were coming from <a href="http://theredecafe.com">The Red e
cafe</a>, a cafe in Portland.  I had never been
to this cafe so I looked up their website only to find my own
<a href="http://www.zachstednick.com">website</a>
staring back at me.</p>

<p>For approximately two days my index page and the associated CSS were
replacing the main site of this cafe.  It was truly bizzare and am not
sure why someone would hack a commercial site and put another site such
as an individual website in
its place.  For the lulz I guess.</p>

<p>Still, the weirdest part of this to me was that not a single person
conteacted me on twitter or any other medium to ask what I was doing on
the mainpage of their cafe.  Someday I will have to visit Red e cafe to
ask in person if this happens often to them.</p>

<p>One other interesting thing I noticed was how often my site gets hit by
a Baidu spider.  Prior to this one came by maybe once a week now one
visits about every 20 minutes.</p>

<p><a href="http://theredecafe.com"><img
src="http://red-e-cafe.morsel.cc/system/logos/1/medium/red_e_logo_large.png"></a></p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/12/31/frugal-mondays/">Frugal Mondays</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2012-12-31T18:26:00-08:00'><span class='date'><span class='date-month'>Dec</span> <span class='date-day'>31</span><span class='date-suffix'>st</span>, <span class='date-year'>2012</span></span> <span class='time'>6:26 pm</span></time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>My 2012 resolution was to attempt to avoid spending any money on
Mondays. <strong>tldr:</strong> I came nowhere close to that goal but it was
interesting.</p>

<h2>Motivation</h2>

<p>In 2010, I was slightly annoyed by Amazon deciding to stop hosting
WikiLeaks
<a href="http://www.reuters.com/article/2010/12/03/us-wikileaks-amazon-idUSTRE6B05EK20101203">website</a>
so I made it a challenge to avoid buying anything off Amazon in 2011.
It was difficult especially for all those little things such as odd
batteries or MP3 albums from obscure music groups.  This year I decided
to step up the challenge, instead of trying to avoid spending any money
on Amazon - I would try to avoid spending any money at all on Mondays.</p>

<h2>Rules</h2>

<p>My only rule was no spending physical money in the form of cash or card
and no spending money online.  I allowed Monday holidays and Mondays I was on a
work trip or vacation day to be exempted.  The hours were from 12 am
Monday morning until midnight Monday night.</p>

<h2>Results</h2>

<p>I would estimate that I was successful in about &frac34; weeks per month on
average and probably 40 Mondays for the entire year.</p>

<p>The two biggest things I noticed:</p>

<ul>
<li><p>This did not really affect my spending on larger items.  I noticed
that if it was something that I wanted that was more substantial than
just coffee I would have no qualms waiting a day to purchase it.  Most
of the purchases I did not make were for things I could spend less on
such as trips to the coffee shop.</p></li>
<li><p>I felt more focused on Mondays.  With no option to take a break and
walk to the coffee shop or wonder about where to eat lunch, I was able to keep my
head down and get some good work done.</p></li>
</ul>


<p>Looking ahead, I will continue to avoid spending money on Mondays
because I find it to be an interesting challenge and anything that can
help me stay focused is highly welcome in my life.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/11/30/running-flask-on-centos/">Running Flask on CentOS</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2012-11-30T10:01:00-08:00'><span class='date'><span class='date-month'>Nov</span> <span class='date-day'>30</span><span class='date-suffix'>th</span>, <span class='date-year'>2012</span></span> <span class='time'>10:01 am</span></time>
        
      </p>
    
  </header>


  <div class="entry-content"><h1>Notes on installing my flask app on CentOS</h1>

<p>I developed a small <a href="http://flask.pocoo.org/">flask</a> application at work and put it on a server
running CentOS which I ended up spending a few days struggling with
getting it up and running.  Hopefully this helps someone else
avoid some of the pain I had.  Jumping right in, the first thing I did
was install flask, since CentOS 6.3 comes with python 2.6.6 it was easy to
install pip-python and use that to get flask.  Since the server was
running Apache 2.2, I put my webapp in /var/www/html. Next step was to
install httpd_devel and mod_wsgi both of which easily installed from
source.  Everything seemed fine but I kept getting an HTTP 500 error
message, here is what I did:</p>

<p>There was a lot of editing of /etc/httpd/conf/httpd.conf to allow for vitrual hosts.<br/>
This was also the first time I had worked on a subdomain but my DNS had supposedly been setup correctly.</p>

<h2>Is mod_wsgi in the right place?</h2>

<p>Check that mod_wsgi.so is in /usr/lib/httpd/modules a</p>

<p><code>-rwxr-xr-x. 1 root root 315812 Sep  4 10:20 mod_wsgi.so</code></p>

<p>The <code>.</code> at the end of the permissions field is the SELinux (in this case
CentOS) ACL.</p>

<h2>Does the application have SE Linux permissions?</h2>

<p><code>bash: semanage: comand not found</code></p>

<p>Since semanage is not found I am assume that we are running CentOS
without any additional security.</p>

<h2>Are we loading mod_wsgi correctly?</h2>

<p>Add a line to httpd.conf</p>

<p><code>LoadModule wsgi_module modules/mod_wsgi.so</code></p>

<h2>Is the socket file set up correctly?</h2>

<p>Socket file not able to write to directory, set socket file to write to
/tmp using WSGISocketPrefix to allow www-data worker process to connect.</p>

<h2>Server still giving HTTP 500</h2>

<p>This is where I started trying everything I could thinkof.  I set
LogLevel to debug and later set LogLevel to info and still could not
figure out what was going on. I even asked on <a href="http://stackoverflow.com/questions/12416532/mod-wsgi-test-wsgi-runs-fine-application-gives-http-500">stack
overflow</a>
with little luck.</p>

<h2>Solved!</h2>

<p>Basically, I was not explicit enough in my file path names which caused the majority of these headaches.  I assumed that CentOS would be like
Debian in how it parsed filepaths, however this was wrong and this
caused more headaches than needed.  Since this was a python application,
you would think that I would recall from the Zen of Python that
&ldquo;Explicit is better than implicit&rdquo; but in this case you would be wrong.
remember the Zen of Python</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/11/20/kalalau-trail/">Kalalau Trail</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2012-11-20T23:07:00-08:00'><span class='date'><span class='date-month'>Nov</span> <span class='date-day'>20</span><span class='date-suffix'>th</span>, <span class='date-year'>2012</span></span> <span class='time'>11:07 pm</span></time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>My wife and I just did the Kalalau trail in Na Pali State park and I wanted to put out some of my general thoughts to help make it easier for someone else who may be interested.  Prior to departure I had a hard time determining what was important and what to trust based on what I read online.  Hopefully this is helpful, although your milage may vary.</p>

<p>The Hawaii State Parks office is in what might be the tallest building in
Lihue.  They will give you an informational brochure with a map and try
to sell you a topo map of the entire island.  We thoguht this would be
a good informational stop but I think best to skip stopping here and use the map in your guidebook.</p>

<p>The trail itself is difficult, but not quite as dramatic as most of the literature makes it out to be.  The first half to Hanakoa Valley is the most up and down and felt like the hardest part in both directions.  The second half feels much easier.  Hanakoa Valley is a great place to stay if you are not feeling up to doing the trek in one day.  Plus, staying here gives you a full afternoon at the beach as opposed to getting in late in the day.  There is a pretty steep and exposed part around mile 8 or 9 that I was glad to walk across when I was properly rested.</p>

<p>We brought packs and a tent and rented a stove and bought fuel at <a href="http://www.pedalnpaddle.com">Pedal
n&#8217; Paddle</a> in Hanalei which also sold us some of the other things we forget (such as a cooking pot).  There is a grocery store next door that is pretty well stocked.  You may be able to save a few bucks by planning in advance and shopping elsewhere.</p>

<p>We found the mosquitos were a nuisance, but nothing intolerable and the seemed much smaller and less vigorous than those we have on the mainland.  Bug spray was nice but not essential.</p>

<p>We left our car at Ha&#8217;ena State Park.  There is not a lot of parking at the end of the road and we saw some cars that were ticketed for being illegally parked.  The earlier you can get on the trail the better, but you might get lucky.  The guidebooks talk about break-ins at Ke&#8217;e Beach and while we did not see this, Ha&#8217;ena has better visibility.</p>

<p>We filtered all our water.  The guidebooks warn about Leptospirosis and while I am not sure how prevalent this is better to be on the safe side.</p>

<p>There are a lot of long term campers there but there are also plenty of spots.</p>

<p>Otherwise, the hike was a lot of fun and I totally recommend it!</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/08/28/testing-setup/">Testing Setup</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2012-08-28T19:57:00-07:00'><span class='date'><span class='date-month'>Aug</span> <span class='date-day'>28</span><span class='date-suffix'>th</span>, <span class='date-year'>2012</span></span> <span class='time'>7:57 pm</span></time>
        
      </p>
    
  </header>


  <div class="entry-content"><h2>This is the first post of this blog</h2>

<p>testing formatting, etc.</p>
</div>
  
  


    </article>
  
  <div class="pagination">
    
    <a href="/blog/archives">Blog Archives</a>
    
    <a class="next" href="/index.html">Newer &rarr;</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2015/01/04/summarizing-books-read-over-time/">Summarizing Books Read Over Time</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/11/19/simple-webstats-with-r/">Simple Webstats With R</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/11/05/206-419-parks/">(206)419-PARKS</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/08/30/lego-price-estimates-over-time/">LEGO Price Estimates Over Time</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/08/19/datasets-for-machine-learning-with-r-by-lantz/">Datasets for Machine Learning With R by Lantz</a>
      </li>
    
  </ul>
</section>





  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2015 - Your Name -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = '//platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
