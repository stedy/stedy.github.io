
<!DOCTYPE HTML>
<html>
<head>
	<meta charset="utf-8">
	<title>Zach Stednick</title>
	<meta name="author" content="Zach Stednick">

	
	<meta name="description" content="I have been tracking restaurant openings via the City of Seattle
Business Finder since
the beginning of this year and am reporting those changes at &hellip;">
	
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

	<link href="/atom.xml" rel="alternate" title="Zach Stednick" type="application/atom+xml">
	<link rel="canonical" href="">
	<link href="/favicon.png" rel="shortcut icon">
	<link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
	<!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
	<script async="true" src="//ajax.googleapis.com/ajax/libs/jquery/1.7.2/jquery.min.js"></script>
	
</head>


<body>
	<header id="header" class="inner"><h1><a href="/">Zach Stednick</a></h1>
<nav id="main-nav"><ul class="main">
	<li><a href="/">Blog</a></li>
	<li><a href="/blog/archives">Archives</a></li>
</ul>
</nav>
<nav id="mobile-nav">
	<div class="alignleft menu">
		<a class="button">Menu</a>
		<div class="container"><ul class="main">
	<li><a href="/">Blog</a></li>
	<li><a href="/blog/archives">Archives</a></li>
</ul>
</div>
	</div>
	<div class="alignright search">
		<a class="button"></a>
		<div class="container">
			<form action="https://www.google.com/search" method="get">
				<input type="text" name="q" results="0">
				<input type="hidden" name="q" value="site:stedy.github.io">
			</form>
		</div>
	</div>
</nav>
<nav id="sub-nav" class="alignright">
	<div class="social">
		
		
		
		<a class="twitter" href="http://twitter.com/zachstednick" title="Twitter">Twitter</a>
		
		
		<a class="github" href="https://github.com/stedy" title="GitHub">GitHub</a>
		
    
		
		
		
		
		
		<a class="rss" href="/atom.xml" title="RSS">RSS</a>
		
    
	</div>
	<form class="search" action="https://www.google.com/search" method="get">
		<input class="alignright" type="text" name="q" results="0">
		<input type="hidden" name="q" value="site:stedy.github.io">
	</form>
</nav>

</header>
	
		
	
	<div id="content" class="inner">


    <article class="post">
	<h2 class="title">
		
		<a href="/blog/2015/07/23/update-on-restaurant-changes/">
		
			Update on Restaurant Changes</a>
	</h2>
	<div class="entry-content">
		<p>I have been tracking restaurant openings via the <a href="http://www.seattle.gov/licenses/find-a-business">City of Seattle
Business Finder</a> since
the beginning of this year and am reporting those changes at <a href="http://seattlerestaurantchanges.com/">Seattle
Restaurant Changes</a>. Recently I
put up a <a href="http://seattlerestaurantchanges.com/heatmap.html">heatmap</a>
showing changes by neighborhood. This heatmap shows a current snaphot of
the changes which made me curious about changes by restaurant type over the course of
the year.</p>

<iframe width="980" height="520" src="http://zachstednick.com/net_restaurant_changes.html"></iframe>


<p>A few notes:</p>

<ul>
<li><p>The City of Seattle uses North American Industry Classification System
codes <a href="http://www.census.gov/eos/www/naics/">(NAICS)</a> to track
restaurants. I then use the date of permit issuance as a proxy for a
restaurant opening and date of permit revocation as a proxy for closing.</p></li>
<li><p>A Full Service Restaurant as defined by NAICS is &ldquo;establishments primarily engaged in providing food services to patrons who order and are served while seated (i.e., waiter/waitress service) and pay after eating&rdquo;</p></li>
<li><p>I realized that not that many breweries would be opening up but who
doesn&rsquo;t want more breweries in town?</p></li>
<li><p>I was not expecting Full Service Restaurants to take off as much as
they did, especially since Limited Service Restaurants seem to be
declining.</p></li>
</ul>


<p>I will try to post another update on this in December, that is unless I decide to open up a food truck of my own.</p>

		
		
	</div>


<div class="meta">
	<div class="date">








  


<time datetime="2015-07-23T15:31:31-07:00" pubdate data-updated="true"></time></div>
	<div class="tags">


	<a class='category' href='/blog/categories/d3/'>d3,</a>, <a class='category' href='/blog/categories/restaurants/'>restaurants,</a>, <a class='category' href='/blog/categories/seattle/'>seattle</a>


</div>
	
</div>
</article>


    <article class="post">
	<h2 class="title">
		
		<a href="/blog/2015/05/30/the-felix-factor/">
		
			The Felix Factor</a>
	</h2>
	<div class="entry-content">
		<p>I was listening to the Jonah Keri
<a href="http://espn.go.com/espnradio/grantland/player?id=12957030">podcast</a> and
he and Ben Gibbard were talking about the Mariners, specifically <a href="http://m.mlb.com/player/433587/felix-hernandez/2015/career/R/pitching/MLB">Felix
Hernandez</a>.
One of the points Gibbard made was that Hernandez is so outstanding that
he will be remembered and that people should try to see him pitch in
person. This made me wonder, did Felix Hernandez have an impact on home
ticket sales for the Mariners in 2014?</p>

<p>I was able to get all the data from some of the nicely formatted box
score data that <a href="http://gd2.mlb.com/components/game/mlb/year_2014/">MLB provides</a>. I initially tried to look at the data over the course of the year but attendance was so variable (which made for an extremely confusing plot) that I just ended up making a box and whiskers plot and ignored the date element:</p>

<p><img src="http://zachstednick.com/felix_factor.png"></p>

<p>Conclusion: Hernandez was not that strong of a driver of ticket sales
which is great news if you are hoping to see him pitching in person.</p>

		
		
	</div>


<div class="meta">
	<div class="date">








  


<time datetime="2015-05-30T16:12:37-07:00" pubdate data-updated="true"></time></div>
	<div class="tags">


	<a class='category' href='/blog/categories/baseball/'>baseball,</a>, <a class='category' href='/blog/categories/r/'>r</a>


</div>
	
</div>
</article>


    <article class="post">
	<h2 class="title">
		
		<a href="/blog/2015/04/17/seattle-restaurant-changes/">
		
			Seattle Restaurant Changes</a>
	</h2>
	<div class="entry-content">
		<p>Seattle construction is currently booming and I was interested in how
that
reflected in the local restaurant scene. There are many food blogs and
local news sites that cover openings and closings, but I found it too
difficult to parse these in a regular manner. Fortunately I was able to
use data from the <a href="http://www.seattle.gov/licenses/find-a-business">City of Seattle business
finder</a> and used the
restaurant classification or
<a href="https://en.wikipedia.org/wiki/NAICS">NAICS</a> code as a proxy. Using the
data in this manner makes an assumption that a restaurant will no longer
have a business licence after it closes.
I&rsquo;m not sure how accurate this is but I figured it was as accurate as I
could get short of hiring people on Mechanical Turk to phone every
restaurant every week and ask if the restaurant is still open. To map each
restaurant to a particular neighborhood, I
used geolocation to map license address returned by the City of Seattle business
finder. Obviously that does not work as well for Mobile Food Services
(i.e. food trucks) but it still allows for an interesting comparison.
This data is plotted at <a href="http://seattlerestaurantchanges.com">Seattle Restaurant
Changes</a>.</p>

<p>I initially attempted to scrape
data from <a href="https://www.thestranger.com/food-and-drink">The Stranger</a> but
after finding the City of Seattle site I just used
<a href="http://www.crummy.com/software/BeautifulSoup/">BeautifulSoup</a> for the
scraping. I would not have been able to do much more beyond that state
if it had not been for Nathan Yau&rsquo;s excellent tutorial on <a href="http://flowingdata.com/2015/02/19/make-an-interactive-map-with-category-filters/">making maps
with category
filters</a>.
I was able to get a state level shapefile for Washington state from
<a href="http://www.zillow.com/blog/7000-neighborhood-boundary-files-in-shapefile-format-4653/">Zillow</a>
and then reduce that to just Seattle neighborhoods using R&rsquo;s
<a href="http://cran.r-project.org/web/packages/sp/">sp</a> package. Full code
posted on <a href="https://github.com/stedy/seattle-restaurants">github</a></p>

		
		
	</div>


<div class="meta">
	<div class="date">








  


<time datetime="2015-04-17T10:51:18-07:00" pubdate data-updated="true"></time></div>
	<div class="tags">


	<a class='category' href='/blog/categories/d3/'>d3</a>, <a class='category' href='/blog/categories/restaurants/'>restaurants,</a>


</div>
	
</div>
</article>


    <article class="post">
	<h2 class="title">
		
		<a href="/blog/2015/02/25/first-year-on-fitbit/">
		
			First Year on Fitbit</a>
	</h2>
	<div class="entry-content">
		<p>After a year on <a href="https://www.fitbit.com/user/2F49C2">Fitbit</a>, I figured it might be time to take a look at the data that I have been generating. Unfortunately, Fitbit makes you sign up for <a href="https://www.fitbit.com/premium/export">Premium</a> which charges you $50 per year to export your data. Fortunately, Cory Nissen has created an excellent <a href="https://github.com/corynissen/fitbitScraper">R package</a> for doing just this. The package simply uses a POST request handled by Hadley&rsquo;s <a href="https://github.com/hadley/httr">httr</a> library to generate a cookie and then parses the returned JSP results to return a nice <code>data.frame</code>.</p>

<p>Anyways, onto the data.</p>

<p>The first command I tried was the <code>get_15_min_data()</code> for parsing step
data in 15 minute increments. I figured that looking at yesterday&rsquo;s data
would be granular enough to get a good feel for the data.</p>

<p><img src="http://zachstednick.com/fifteen_min_fitbit.png"></p>

<p>I then plotted number of steps taken per day, with a smoothing function
overlaid:</p>

<p><img src="http://zachstednick.com/one_year_fitbit_steps.png"></p>

<p>I had a mean step count of 13935 for the past year. This data is more
interesting to look at as more of an overall trend. There definitely a
seasonal trend in the summer which makes sense. I can also see the
signatures of when I went on a four day backpacking trip in August and
when I broke two ribs and was confined to the couch for four days in
mid-March.</p>

<p>Since I have a Fitbit one, I can also measure floors climbed.</p>

<p><img src="http://zachstednick.com/fitbit_one_year_floors.png"></p>

<p>My mean number of floors climbed is 69.62 which seems absurdly high. My
desk is on the fourth floor of my building and I usually take the stairs
but not sure that is enough to fully explain why these counts are so
high.</p>

<p>Still, it is pretty interesting to look at this data outside of the
Fitbit interface and I would highly recommend checking out Cory&rsquo;s github
<a href="https://github.com/corynissen/fitbitScraper">repo</a></p>

<p>Also, speaking of github, for those of you who regularly follow this
blog (hi, Mom!) I have moved away from making a new gist every time to
simply having a standalone
<a href="https://github.com/stedy/blog-supplemental">repo</a></p>

		
		
	</div>


<div class="meta">
	<div class="date">








  


<time datetime="2015-02-25T14:15:11-08:00" pubdate data-updated="true"></time></div>
	<div class="tags">


	<a class='category' href='/blog/categories/r/'>r,</a>, <a class='category' href='/blog/categories/running/'>running</a>


</div>
	
</div>
</article>


    <article class="post">
	<h2 class="title">
		
		<a href="/blog/2015/01/10/offsetting-beer-by-running/">
		
			Offsetting Beer by Running</a>
	</h2>
	<div class="entry-content">
		<p>Last year, among other personal data, I tracked every bar I went to and
every mile I
<a href="http://zachstednick.name/blog/2014/02/06/foursquare-without-a-smartphone/">ran</a>.
Naturally my first question is do I run enough to offset the amount of
beer I am drinking (at bars)?</p>

<p>First we define some units. According to this Runner&rsquo;s World
<a href="http://www.runnersworld.com/tools/calories-burned-calculator">calculator</a>, at 8:45 minute/mile for my
weight I am burning 145 calories. Google says the amount of calories in
a <a href="https://www.google.com/?gws_rd=ssl#q=calories+in+a+pint+of+beer">pint of
beer</a> is about 180. Since I usually average about two beers each time I go to a bar, that simplifies the calculations. Over the course of the year, how often was I above or below the residual? To answer this, I used R and finally got around to trying <a href="https://github.com/hadley/tidyr">tidyr</a> which is pretty slick.</p>

<p><img src="http://zachstednick.com/running_vs_beers.png"></p>

<p>I thougth a lot about how to determine the residual but eventually
settled on calories out - calories in because I felt this method made the best
visualization. As you can see around week 30, I started to run more and
did a better job at offsetting my beer consumption. Obviously this is an
overly simplistic view of my caloric expenditure but shows some of the
interesting insights that can be gained from personal data.</p>

<p>As always, all code and data is in this github
<a href="https://gist.github.com/stedy/9f2ffa58e25cc52dbe2e">gist</a></p>

		
		
	</div>


<div class="meta">
	<div class="date">








  


<time datetime="2015-01-10T14:24:32-08:00" pubdate data-updated="true"></time></div>
	<div class="tags">


	<a class='category' href='/blog/categories/r-running/'>r,running</a>


</div>
	
</div>
</article>


    <article class="post">
	<h2 class="title">
		
		<a href="/blog/2015/01/04/summarizing-books-read-over-time/">
		
			Summarizing Books Read Over Time</a>
	</h2>
	<div class="entry-content">
		<p>I recently read an interesting blog post where the author examined their books rated on Goodreads and summarizing <a href="http://citizen-statistician.org/2014/12/31/annual-review-of-reading/">interesting
trends</a>.
I decided to do a similar analysis even though I use
<a href="https://www.librarything.com/profile/pbirch01">LibraryThing</a>
instead.</p>

<p>LibraryThing has a nice option to allow to to export your data in a
variety of <a href="https://www.librarything.com/export.php">formats</a>. Since I
write R code to parse CSV files everyday I thought I would do something
different and parse a JSON file with python.</p>

<p>I have been on LibraryThing since 2007 and the first question I was
interested in was have my average ratings changed over time? I
calculated the mean for each book by year:</p>

<table>
<thead>
<tr>
<th> Year </th>
<th style="text-align:right;"> Average Rating </th>
</tr>
</thead>
<tbody>
<tr>
<td> 2007 </td>
<td style="text-align:right;"> 3.446809</td>
</tr>
<tr>
<td> 2008 </td>
<td style="text-align:right;"> 3.480000</td>
</tr>
<tr>
<td> 2009 </td>
<td style="text-align:right;"> 3.485294</td>
</tr>
<tr>
<td> 2010 </td>
<td style="text-align:right;"> 3.641509</td>
</tr>
<tr>
<td> 2011 </td>
<td style="text-align:right;"> 3.456522</td>
</tr>
<tr>
<td> 2012 </td>
<td style="text-align:right;"> 3.529412</td>
</tr>
<tr>
<td> 2013 </td>
<td style="text-align:right;"> 3.321429</td>
</tr>
<tr>
<td> 2014 </td>
<td style="text-align:right;"> 3.614583</td>
</tr>
</tbody>
</table>


<p>While uninteresting, this makes a lot of sense - if I am reading a book
that I do not enjoy, I will usually bail on it which tends to bias my
ratings upward. Over time, there have been a few notable
<a href="https://www.librarything.com/work/11691727/reviews/92168929">exceptions</a>.</p>

<p>One of the other interesting analyses in the blog post was examining how the
reviewer&rsquo;s ratings have changed based on the month of the year. I wanted to
make a similar plot using R&rsquo;s
<a href="http://docs.ggplot2.org/current/index.html">ggplot2</a> however since I
was writing this in python I was largely limited to matplotlib.
Fortunately, many people have struggled with this issue and the fine
folks at yhat have ported ggplot2 over to
<a href="https://pypi.python.org/pypi/ggplot">python</a>. With this library I was
able to use <code>geom_smooth</code> to produce the following plot showing rating
trends by week.</p>

<p><img src="http://zachstednick.com/all_years.png"></p>

<p>I tried to figure out why my legend never showed up but I figured that
since most of the trend lines were pretty much the same anyways that the
plot was fine without a legend. It appears that I get in most of my good
reviews early in the year and am harsher later in the year.</p>

<p>The last figure in the blog post compares the writer&rsquo;s review scores to
the Goodreads consensus score. I attempted to
replicate this but ran into more trouble than it was worth to extract
that data from LibraryThing so I abandoned that analysis.</p>

<p>If interested, I put my python code in a GitHub
<a href="https://gist.github.com/stedy/2cb4fb1f332508f39ff2">gist</a>.</p>

		
		
	</div>


<div class="meta">
	<div class="date">








  


<time datetime="2015-01-04T13:54:43-08:00" pubdate data-updated="true"></time></div>
	<div class="tags">


	<a class='category' href='/blog/categories/librarything/'>librarything</a>, <a class='category' href='/blog/categories/r/'>r,</a>


</div>
	
</div>
</article>


    <article class="post">
	<h2 class="title">
		
		<a href="/blog/2014/11/19/simple-webstats-with-r/">
		
			Simple Webstats With R</a>
	</h2>
	<div class="entry-content">
		<p>As someone who puts out writings out publically, I am naturally curious who (if anyone) is actually reading what I write. To answer this I developed a simple webstat calculator using R. I realize there are many options out there for tracking visits but to paraphrase my friend Andy, <a href="http://wingolog.org/archives/2014/11/14/on-yakshave-on-color-on-cosines-on-glitchen">when has using standard libraries lead to anything cool?</a>.</p>

<p>My main interests in this project is to answer two questions:</p>

<ol>
<li><p>Are people visting this site?</p></li>
<li><p>Where are they visting from?</p></li>
</ol>


<p>I don&rsquo;t really care about things like <a href="https://en.wikipedia.org/wiki/Bounce_rate">bounce rate</a> or type of device used to access the site. Not having to worry about either of these issues helps cut down on the complexity. I run this site on an Apache server and use a standard log output to write my logfile:
<code>LogFormat "%v:%p %h %l %u %t \"%r\" %&gt;s %O \"%{Referer}i\" \"%{User-Agent}i\"</code></p>

<p>I made a small R script that uses <a href="http://cran.r-project.org/web/packages/knitr/">knitr</a> to output plots to HTML for ease in viewing. I wrote a shell script that uses the excellent <a href="http://dirk.eddelbuettel.com/code/littler.html">little r</a> to perform the commands. I run the shell script daily as a cron job and only look back at the past week&rsquo;s worth of data. Since this blog is served on github pages, it can be difficult to see page views so I use images loaded as a proxy.</p>

<p>Here are some example plots of recent visitors:</p>

<p><img src="http://zachstednick.com/visitor.png"></p>

<p>And then another plot of visitor locations:</p>

<p><img src="http://zachstednick.com/referrer_location.png"></p>

<p>That outlier from Brazil is likely a Google bot crawling the site; better detection and removal of bot traffic from the final output is on the TODO list. All of the code (minus the shell script) lives on <a href="https://github.com/stedy/simple-webstats">github</a></p>

		
		
	</div>


<div class="meta">
	<div class="date">








  


<time datetime="2014-11-19T17:01:34-08:00" pubdate data-updated="true"></time></div>
	<div class="tags">


	<a class='category' href='/blog/categories/github/'>github</a>, <a class='category' href='/blog/categories/r/'>r,</a>, <a class='category' href='/blog/categories/web/'>web,</a>


</div>
	
</div>
</article>


    <article class="post">
	<h2 class="title">
		
		<a href="/blog/2014/11/05/206-419-parks/">
		
			(206)419-PARKS</a>
	</h2>
	<div class="entry-content">
		<p>I recently became aware of the efforts of Linnea Westerlind who made a
goal to visit every park in Seattle and documented her efforts
<a href="http://www.yearofseattleparks.com/">here</a>. I thought this was pretty
neat so I looked up the list of <a href="http://www.seattle.gov/parks/listall.asp">City of Seattle
Parks</a> which currently lists
419 parks. The definitions for a park are hard to determine and this
list ends up with some oddball parks such as <a href="https://www.google.com/maps/place/Crescent+Place/@47.6842435,-122.3330848,19z/data=!4m2!3m1!1s0x54901413dd56bf9f:0xa19ed07208659a24">Crescent
Place</a>.
Still I think it is an interesting way to learn more about where you live
wherever that may be. I have currently visited 118/419  parkswhich is
about 28%, not bad. Not
sure if I will be able to reach them faster than the 4 years it took
Westerlind but maybe I should just focus on the journey instead.</p>

		
		
	</div>


<div class="meta">
	<div class="date">








  


<time datetime="2014-11-05T15:27:00-08:00" pubdate data-updated="true"></time></div>
	<div class="tags">

</div>
	
</div>
</article>


    <article class="post">
	<h2 class="title">
		
		<a href="/blog/2014/08/30/lego-price-estimates-over-time/">
		
			LEGO Price Estimates Over Time</a>
	</h2>
	<div class="entry-content">
		<p>LEGO recently introduced a new LEGO set called <a href="http://shop.lego.com/en-US/Research-Institute-21110">Research
Institute</a> which
featured three female scientists. Since my wife is also a female
scientist, I tried to order one from the LEGO website only to learn that
they had sold out in less than a day. I then wrote an email complaining
about this to LEGO who responded by sending me an apology note and a
catalog.</p>

<p>I grew up playing with LEGO sets, hard to avoid when you were named Zach
and commercials like
<a href="https://www.youtube.com/watch?v=pDH3AoOQzE0">this</a> dominated the
airwaves. Anyways, when I was a kid my dad once mentioned to me that a good rule of thumb
for determining the price of a LEGO set was to estimate each brick
costing about 10 cents. This new catalog made me wonder if this was still
true. I copied all the model numbers as well as the number of pieces and
the prices. I was also curious in how true this trend was when adjusted
for inflation so I used the CPI Inflation calculator from <a href="http://data.bls.gov/cgi-bin/cpicalc.pl?cost1=.10&amp;year1=1989&amp;year2=2014">US Bureau of
Labor
Statistics</a> which showed
that $0.10 in 1989 had the same buying power as $0.19 in 2014. Ideally I
could have found a catalog from 1989 but I don&rsquo;t remember any back then
and I probably would have cut it up to put pictures in my locker or
something like that. I used R
to plot both of these trends and it appears that my dad&rsquo;s estimate still
holds true for 2014.</p>

<p><img src="http://zachstednick.com/lego_by_year.png"></p>

<p>A correlation calculation for all sets gives a value of 0.91 which means my dad had a
pretty good estimate back in the day.</p>

<p>I also looked at the average price for each collection and found that
almost all collections retained a high correlation between the estimated
price and the actual price.</p>

<table>
<thead>
<tr>
<th> Collection </th>
<th> Collection Mean Price </th>
<th style="text-align:right;"> Collection Correlation </th>
</tr>
</thead>
<tbody>
<tr>
<td> Basics </td>
<td> 29.99 </td>
<td style="text-align:right;"> NA</td>
</tr>
<tr>
<td> Chima </td>
<td> 38.99 </td>
<td style="text-align:right;"> 0.977</td>
</tr>
<tr>
<td> City </td>
<td> 54.365 </td>
<td style="text-align:right;"> 0.896</td>
</tr>
<tr>
<td> Creator </td>
<td> 100.375 </td>
<td style="text-align:right;"> 0.955</td>
</tr>
<tr>
<td> DC Superheroes </td>
<td> 76.657 </td>
<td style="text-align:right;"> 1</td>
</tr>
<tr>
<td> Disney Princess </td>
<td> 27.657 </td>
<td style="text-align:right;"> 0.963</td>
</tr>
<tr>
<td> Exclusive </td>
<td> 149.99 </td>
<td style="text-align:right;"> NA</td>
</tr>
<tr>
<td> Friends </td>
<td> 23.354 </td>
<td style="text-align:right;"> 0.992</td>
</tr>
<tr>
<td> Ideas </td>
<td> 49.99 </td>
<td style="text-align:right;"> NA</td>
</tr>
<tr>
<td> Juniors </td>
<td> 27.49 </td>
<td style="text-align:right;"> 0.901</td>
</tr>
<tr>
<td> LEGO Movie </td>
<td> 63.99 </td>
<td style="text-align:right;"> 0.997</td>
</tr>
<tr>
<td> Marvel Superheroes </td>
<td> 40.99 </td>
<td style="text-align:right;"> 0.934</td>
</tr>
<tr>
<td> Mindstorms </td>
<td> 349.99 </td>
<td style="text-align:right;"> NA</td>
</tr>
<tr>
<td> Minecraft </td>
<td> 139.96 </td>
<td style="text-align:right;"> NA</td>
</tr>
<tr>
<td> Mixels </td>
<td> 4.99 </td>
<td style="text-align:right;"> NA</td>
</tr>
<tr>
<td> Ninjago </td>
<td> 47.434 </td>
<td style="text-align:right;"> 0.976</td>
</tr>
<tr>
<td> Simpsons </td>
<td> 199.99 </td>
<td style="text-align:right;"> NA</td>
</tr>
<tr>
<td> Star Wars </td>
<td> 133.365 </td>
<td style="text-align:right;"> 0.979</td>
</tr>
<tr>
<td> Technic </td>
<td> 81.99 </td>
<td style="text-align:right;"> 0.989</td>
</tr>
<tr>
<td> Ultra Agents </td>
<td> 45.323 </td>
<td style="text-align:right;"> 0.983</td>
</tr>
</tbody>
</table>


<p>Raw data and code for this lives at this
<a href="https://gist.github.com/stedy/92b949ba44effd66c855">gist</a></p>

		
		
	</div>


<div class="meta">
	<div class="date">








  


<time datetime="2014-08-30T16:22:00-07:00" pubdate data-updated="true"></time></div>
	<div class="tags">


	<a class='category' href='/blog/categories/lego/'>lego</a>, <a class='category' href='/blog/categories/r/'>r,</a>


</div>
	
</div>
</article>


    <article class="post">
	<h2 class="title">
		
		<a href="/blog/2014/08/19/datasets-for-machine-learning-with-r-by-lantz/">
		
			Datasets for Machine Learning With R by Lantz</a>
	</h2>
	<div class="entry-content">
		<p>I recently read <a href="https://www.packtpub.com/big-data-and-business-intelligence/machine-learning-r">Machine Learning with
R</a>
by Brett Lantz. This is a book that provides an introduction to machine
learning using R. I really enjoyed the book and thought Lantz did an
excellent job explaining the content as well as providing many good
references and examples, which is what lead to my problem with the book.
As far as I can tell, Packt Publishing does not make its datasets
available online unless you buy the book and create a <a href="https://www.packtpub.com/books/content/support">user
account</a> which can be a
problem if you are checking the book out from the library or borrowing
the book from a friend. All of these datasets
are in the public domain but simply needed some cleaning up and
recoding to match the format in the book so I went ahead and made a
<a href="https://github.com/stedy/Machine-Learning-with-R-datasets">github repo</a>
to host them.</p>

		
		
	</div>


<div class="meta">
	<div class="date">








  


<time datetime="2014-08-19T15:31:00-07:00" pubdate data-updated="true"></time></div>
	<div class="tags">


	<a class='category' href='/blog/categories/r/'>r</a>


</div>
	
</div>
</article>

<nav id="pagenavi">
    
        
            <a href="/posts/4" class="prev">Prev</a>
        
    
    
        <a href="/posts/6" class="next">Next</a>
    
    <div class="center"><a href="/blog/archives">Blog Archives</a></div>
</nav>
</div>
	<footer id="footer" class="inner">Copyright &copy; 2021

    Zach Stednick

</footer>
	<script src="/javascripts/slash.js"></script>
<script src="/javascripts/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
	$('.fancybox').fancybox();
})(jQuery);
</script> <!-- Delete or comment this line to disable Fancybox -->






</body>
</html>